{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437c0b6d-58ee-4726-8008-93e223ad5cb7",
   "metadata": {},
   "source": [
    "# RAG demo with OpenVINO Model Server and langchain\n",
    "This demo shows how to use Retrieval Augmented Generation with langchain and gen AI endpoint from OpenVINO Model Server.\n",
    "\n",
    "It employs the `chat/completion` and `embeddings` and `rerank` endpoints.\n",
    "\n",
    "It assumes the model server is already deployed on the same machine on port 8000 with:\n",
    "\n",
    "OpenVINO models:\n",
    " `OpenVINO/Qwen3-8B-int4-ov` for `chat/completions` and `OpenVINO/bge-base-en-v1.5-fp16-ov` for `embeddings` and `OpenVINO/bge-reranker-base-fp16-ov` for `rerank` endpoint.\n",
    "\n",
    "or\n",
    "Converted models:\n",
    " `meta-llama/Meta-Llama-3-8B-Instruct` for `chat/completions` and `Alibaba-NLP/gte-large-en-v1.5` for `embeddings` and `BAAI/bge-reranker-large` for `rerank` endpoint. \n",
    "\n",
    "Check https://github.com/openvinotoolkit/model_server/tree/main/demos/continuous_batching/rag/README.md to see how they can be deployed.\n",
    "LLM model, embeddings and rerank can be on hosted on the same model server instance or separately as needed.\n",
    "openai_api_base , base_url parameters with the target url and model names in the commands might need to be adjusted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a498b22-fb7f-4fa1-a4d8-a0d983eb4565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7212515f-b59b-498c-a66a-f6c59de8fcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97b31c1ba61476fa8d43eb48812691c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Radio Selector:', options=('OpenVINO models', 'Converted models'), value='OpenVINO mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7b21f697bf4063a90a985e26b1b3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='OpenVINO models', disabled=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets, link\n",
    "from IPython.display import display\n",
    "options = [\"OpenVINO models\", \"Converted models\"]\n",
    "\n",
    "# Create the radio buttons and a text box for output\n",
    "radio_button = widgets.RadioButtons(options=options, description='Radio Selector:')\n",
    "output_text = widgets.Text(disabled=True)\n",
    "\n",
    "# Link the value of the radio buttons to the text box\n",
    "link((radio_button, 'value'), (output_text, 'value'))\n",
    "\n",
    "# Display both widgets\n",
    "display(radio_button, output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b085cd3f-5473-474e-b35c-a1a548d50f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted models\n"
     ]
    }
   ],
   "source": [
    "print(output_text.value)\n",
    "if output_text.value == \"OpenVINO models\":\n",
    "    embeddings_model = \"OpenVINO/bge-base-en-v1.5-fp16-ov\"\n",
    "    rerank_model = \"OpenVINO/bge-reranker-base-fp16-ov\"\n",
    "    chat_model = \"OpenVINO/Qwen3-8B-int4-ov\"\n",
    "else:\n",
    "    embeddings_model = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "    rerank_model = \"BAAI/bge-reranker-large\"\n",
    "    chat_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5158e553-3355-46af-879b-e7ef09058aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Document Splitter\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader, )\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b52e35-ea8a-4c84-b98c-24f774b03721",
   "metadata": {},
   "source": [
    "The documents to scan with knowledge context are to be placed in ./docs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25410751-e4a5-4348-8376-938dc4ffd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FOLDER = \"./docs/\"\n",
    "\n",
    "TEXT_SPLITERS = {\n",
    "    \"Character\": CharacterTextSplitter,\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,\n",
    "    \"Markdown\": MarkdownTextSplitter,\n",
    "}\n",
    "\n",
    "LOADERS = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b57421-d418-4c4d-bd98-21d67e7fd31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  747k    0  747k    0     0   925k      0 --:--:-- --:--:-- --:--:--  926k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  773k    0  773k    0     0   888k      0 --:--:-- --:--:-- --:--:--  889k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  750k    0  750k    0     0  1092k      0 --:--:-- --:--:-- --:--:-- 1095k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  769k    0  769k    0     0  1816k      0 --:--:-- --:--:-- --:--:-- 1823k\n"
     ]
    }
   ],
   "source": [
    "!curl https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html --create-dirs -o ./docs/ovms_what_is_openvino_model_server.html\n",
    "!curl https://docs.openvino.ai/2025/model-server/ovms_docs_metrics.html -o ./docs/ovms_docs_metrics.html\n",
    "!curl https://docs.openvino.ai/2025/model-server/ovms_docs_streaming_endpoints.html -o ./docs/ovms_docs_streaming_endpoints.html\n",
    "!curl https://docs.openvino.ai/2025/model-server/ovms_docs_target_devices.html -o ./docs/ovms_docs_target_devices.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed9afff-df0f-42f2-8aa7-6b5fff9f794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    helper for loading a single document\n",
    "\n",
    "    Params:\n",
    "      file_path: document path\n",
    "    Returns:\n",
    "      documents loaded\n",
    "\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADERS:\n",
    "        loader_class, loader_args = LOADERS[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"File does not exist '{ext}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75374e4c-3af8-44fa-ad8d-f905766cc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=embeddings_model,\n",
    "    api_key=\"unused\",\n",
    "    tiktoken_enabled=False,\n",
    "    base_url=\"http://localhost:8000/v3\",\n",
    "    embedding_ctx_length=8190,  # 8190 is the model max context length subtracted by 2 special tokens \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12abca7-3074-4c87-8ba1-eb27e4332860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading document ./docs/ovms_docs_metrics.html...\n",
      "Reading document ./docs/ovms_docs_streaming_endpoints.html...\n",
      "Reading document ./docs/ovms_docs_target_devices.html...\n",
      "Reading document ./docs/ovms_what_is_openvino_model_server.html...\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for file_path in os.listdir(TARGET_FOLDER):\n",
    "    if not file_path.endswith('.html'):\n",
    "        continue\n",
    "    abs_path = os.path.join(TARGET_FOLDER, file_path)\n",
    "    print(f\"Reading document {abs_path}...\", flush=True)\n",
    "    documents.extend(load_single_document(abs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173aee0c-9cfc-4ad6-bdb5-68df52b797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter_name = \"RecursiveCharacter\"  # PARAM\n",
    "chunk_size=1000  # PARAM\n",
    "chunk_overlap=200  # PARAM\n",
    "text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281c64d8-e188-4712-8aaa-c2d2eb61c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db.delete_collection()\n",
    "except:\n",
    "    pass\n",
    "db = FAISS.from_documents(texts, embeddings)  # This command populates vector store with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f316f93a-becd-48c6-a40b-dad72fc44c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2d944-cba1-44d5-8b9a-fccc7860bb4e",
   "metadata": {},
   "source": [
    "The commands below can be used to test the retriever. It can report the content for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc958cd3-72e6-4311-a77f-44eaa147b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Metrics#\n",
      "\n",
      "Introduction#\n",
      "\n",
      "This document describes how to use metrics endpoint in the OpenVINO Model Server. They can be applied for:\n",
      "\n",
      "Providing performance and utilization statistics for monitoring and benchmarking purposes\n",
      "\n",
      "Auto scaling of the model server instances in Kubernetes and OpenShift based on application related metrics\n",
      "\n",
      "Built-in metrics allow tracking the performance without any extra logic on the client side or using network traffic monitoring tools like load balancers or reverse-proxies.\n",
      "\n",
      "It also exposes metrics which are not related to the network traffic.\n",
      "\n",
      "For example, statistics of the inference execution queue, model runtime parameters etc. They can also track the usage based on model version, API type or requested endpoint methods.\n",
      "\n",
      "OpenVINO Model Server metrics are compatible with Prometheus standard\n",
      "\n",
      "They are exposed on the /metrics endpoint.\n",
      "\n",
      "Available metrics families#\n",
      "\n",
      "Metrics from default list are enabled with the metrics_enable flag or json configuration.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "MediaPipe graphs serving\n",
      "\n",
      "Model management - including model versioning and model updates in runtime\n",
      "\n",
      "Dynamic model inputs\n",
      "\n",
      "Directed Acyclic Graph Scheduler along with custom nodes in DAG pipelines\n",
      "\n",
      "Metrics - metrics compatible with Prometheus standard\n",
      "\n",
      "Support for multiple frameworks, such as TensorFlow, PaddlePaddle and ONNX\n",
      "\n",
      "Support for AI accelerators\n",
      "\n",
      "Additional Resources#\n",
      "\n",
      "ADVANCING GENAI WITH CPU OPTIMIZATION\n",
      "\n",
      "Manage deep learning models with OpenVINO Model Server\n",
      "\n",
      "RAG building blocks made easy and affordable with OpenVINO Model Server\n",
      "\n",
      "Simple deployment with KServe API\n",
      "\n",
      "Benchmarking results\n",
      "\n",
      "On this page\n",
      "\n",
      ".pdf .zip\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Configuration file with all metrics enabled#\n",
      "\n",
      "echo '{\n",
      " \"model_config_list\": [\n",
      "     {\n",
      "        \"config\": {\n",
      "             \"name\": \"resnet\",\n",
      "             \"base_path\": \"/workspace/models/resnet50\"\n",
      "        }\n",
      "     }\n",
      " ],\n",
      " \"monitoring\":\n",
      "     {\n",
      "         \"metrics\":\n",
      "         {\n",
      "             \"enable\" : true,\n",
      "             \"metrics_list\": \n",
      "                 [ \"ovms_requests_success\",\n",
      "                 \"ovms_requests_fail\",\n",
      "                 \"ovms_requests_accepted\",\n",
      "                 \"ovms_requests_rejected\",\n",
      "                 \"ovms_responses\",\n",
      "                 \"ovms_inference_time_us\",\n",
      "                 \"ovms_wait_for_infer_req_time_us\",\n",
      "                 \"ovms_request_time_us\",\n",
      "                 \"ovms_current_requests\",\n",
      "                 \"ovms_current_graphs\",\n",
      "                 \"ovms_infer_req_active\",\n",
      "                 \"ovms_streams\",\n",
      "                 \"ovms_infer_req_queue_size\"]\n",
      "         }\n",
      "     }\n",
      "}' > workspace/config.json\n",
      "\n",
      "Start with the configuration file above#\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Check how to write the client applications using generative endpoints.\n",
      "\n",
      "OVMS picture\n",
      "\n",
      "The models used by the server need to be stored locally or hosted remotely by object storage services. For more details, refer to Preparing Model Repository documentation. Model server works inside Docker containers, on Bare Metal, and in Kubernetes environment. Start using OpenVINO Model Server with a fast-forward serving example from the QuickStart guide or LLM QuickStart guide.\n",
      "\n",
      "Key features:#\n",
      "\n",
      "[NEW] Support for AI agents\n",
      "\n",
      "[NEW] Image generation and editing\n",
      "\n",
      "[NEW] Native Windows support. Check updated deployment guide\n",
      "\n",
      "[NEW] Embeddings endpoint compatible with OpenAI API\n",
      "\n",
      "[NEW] Reranking compatible with Cohere API\n",
      "\n",
      "[NEW] Efficient Text Generation with OpenAI API\n",
      "\n",
      "Python code execution\n",
      "\n",
      "gRPC streaming\n",
      "\n",
      "MediaPipe graphs serving\n",
      "\n",
      "Model management - including model versioning and model updates in runtime\n",
      "\n",
      "Dynamic model inputs\n",
      "\n",
      "Directed Acyclic Graph Scheduler along with custom nodes in DAG pipelines\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "You can enable from one up to all the metrics available at once.\n",
      "\n",
      "To enable specific set of metrics you need to specify the metrics_list flag or json setting:\n",
      "\n",
      "Option 1: CLI#\n",
      "\n",
      "wget -N https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/2/resnet50-binary-0001/FP32-INT1/resnet50-binary-0001.{xml,bin} -P models/resnet50/1\n",
      "docker run -d -u $(id -u) -v $(pwd)/models:/models -p 9000:9000 -p 8000:8000 openvino/model_server:latest \\\n",
      "      --model_name resnet --model_path /models/resnet50  --port 9000 \\\n",
      "      --rest_port 8000 \\\n",
      "      --metrics_enable \\\n",
      "      --metrics_list ovms_requests_success,ovms_infer_req_queue_size\n",
      "\n",
      "Option 2: Configuration file#\n"
     ]
    }
   ],
   "source": [
    "vector_search_top_k = 5\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": vector_search_top_k})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Which metrics are supported in the model server? Give examples.\")\n",
    "pretty_print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875703ab-19c7-439f-af1e-8ffd239d183a",
   "metadata": {},
   "source": [
    "\n",
    "Below the document compressor is used to filter the documents to the most relevant for the given query. It employs rerank endpoint in the model server and cohere client.\n",
    "In the response is reported a list of documents limited to top_n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe8590f-fa84-469e-9093-5c2440c5a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Metrics#\n",
      "\n",
      "Introduction#\n",
      "\n",
      "This document describes how to use metrics endpoint in the OpenVINO Model Server. They can be applied for:\n",
      "\n",
      "Providing performance and utilization statistics for monitoring and benchmarking purposes\n",
      "\n",
      "Auto scaling of the model server instances in Kubernetes and OpenShift based on application related metrics\n",
      "\n",
      "Built-in metrics allow tracking the performance without any extra logic on the client side or using network traffic monitoring tools like load balancers or reverse-proxies.\n",
      "\n",
      "It also exposes metrics which are not related to the network traffic.\n",
      "\n",
      "For example, statistics of the inference execution queue, model runtime parameters etc. They can also track the usage based on model version, API type or requested endpoint methods.\n",
      "\n",
      "OpenVINO Model Server metrics are compatible with Prometheus standard\n",
      "\n",
      "They are exposed on the /metrics endpoint.\n",
      "\n",
      "Available metrics families#\n",
      "\n",
      "Metrics from default list are enabled with the metrics_enable flag or json configuration.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "import cohere\n",
    "co = cohere.ClientV2(\n",
    "    api_key=\"no_used\",\n",
    "    base_url=\"http://localhost:8000/v3/\",\n",
    ")\n",
    "compressor = CohereRerank(model=rerank_model, client=co, top_n=1)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Which metrics are supported in the model server? Give examples.\",\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29941c55-e781-42a0-b578-2e463e1b879d",
   "metadata": {},
   "source": [
    "Finally, LLM component needs to be configured. Here will be used chat/completions endpoint from the model server.\n",
    "Change the base url and model name depending on the model server deployment and configuration. It is important to use /v3/ part which is specific for the OpenVINO Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d6a7cc-de79-4255-81b3-80ed44f2ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=\"http://localhost:8000/v3\",\n",
    "    model_name=chat_model,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b37bd5-cf76-42f6-bb8a-cb71944a0032",
   "metadata": {},
   "source": [
    "With all the building blocks defined, the RAG chain is established to link all the components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ba4bc8d-abab-4374-9776-2f531ba62f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt input_variables=['context', 'question'] input_types={} partial_variables={} template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=PromptTemplate(input_variables=['context', 'question'], \n",
    "                      template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\")\n",
    "\n",
    "print(\"prompt\", prompt)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5109c9-a43d-4f33-be41-8f982ead321c",
   "metadata": {},
   "source": [
    "Below you can start the RAG chain using your own query. It will call the embedding model first, retrieve the relevant context and pass it to the LLM endpoint in a single request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5b1d12-0b36-4818-a6e5-d94868c8c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking which metrics are supported in the model server and wants examples. Let me check the provided context.\n",
      "\n",
      "The context mentions that the OpenVINO Model Server has metrics for performance and utilization stats, used for monitoring and benchmarking. It also talks about auto-scaling in Kubernetes/OpenShift based on application metrics. The built-in metrics track things without needing client-side logic or network tools. Examples given include inference execution queue stats, model runtime parameters, and usage based on model version, API type, or endpoint methods. Also, metrics not related to network traffic are exposed. The metrics are compatible with Prometheus and available via the /metrics endpoint.\n",
      "\n",
      "So, the answer should list the types of metrics and give examples. The available metrics families are mentioned as being enabled via metrics_enable or JSON config, but the specific examples from the context are the inference queue stats, model runtime parameters, usage tracking (model version, API type, endpoint methods), and others not related to network traffic. I need to present these as examples of supported metrics.\n",
      "</think>\n",
      "\n",
      "The OpenVINO Model Server supports various metrics for monitoring and benchmarking, including:  \n",
      "- **Performance and utilization statistics** (e.g., inference execution queue statistics, model runtime parameters).  \n",
      "- **Usage tracking** based on factors like model version, API type, or endpoint methods.  \n",
      "- **Non-network-related metrics** (e.g., application-specific statistics).  \n",
      "\n",
      "Examples of supported metrics include:  \n",
      "1. **Inference execution queue statistics** (e.g., latency, throughput).  \n",
      "2. **Model runtime parameters** (e.g., GPU/CPU utilization, memory usage).  \n",
      "3. **Request-specific metrics** (e.g., count of requests per model version or API type).  \n",
      "\n",
      "These metrics are exposed via the `/metrics` endpoint and are compatible with Prometheus."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"Which metrics are supported in the model server? Give examples.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e021ada-d63a-43d2-ad0d-20664f2867a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
