{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437c0b6d-58ee-4726-8008-93e223ad5cb7",
   "metadata": {},
   "source": [
    "# RAG demo with OpenVINO Model Server and langchain\n",
    "This demo shows how to use Retrieval Augmented Generation with langchain and OpenAI API endpoint.\n",
    "\n",
    "It assumes the model server is already deployed on the same machine on port 8000 with model meta-llama/Meta-Llama-3-8B-Instruct.\n",
    "\n",
    "Check https://github.com/openvinotoolkit/model_server/tree/main/demos/continuous_batching to see how it can be deployed.\n",
    "\n",
    "Target host, port and model and all the commands can be adjusted to play with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a498b22-fb7f-4fa1-a4d8-a0d983eb4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5158e553-3355-46af-879b-e7ef09058aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Document Splitter\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader, )\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b52e35-ea8a-4c84-b98c-24f774b03721",
   "metadata": {},
   "source": [
    "The documents to scan with knowledge context are to be placed in ./docs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25410751-e4a5-4348-8376-938dc4ffd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FOLDER = \"./docs/\"\n",
    "\n",
    "TEXT_SPLITERS = {\n",
    "    \"Character\": CharacterTextSplitter,\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,\n",
    "    \"Markdown\": MarkdownTextSplitter,\n",
    "}\n",
    "\n",
    "LOADERS = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b57421-d418-4c4d-bd98-21d67e7fd31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  697k    0  697k    0     0  1206k      0 --:--:-- --:--:-- --:--:-- 1204k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  719k    0  719k    0     0  1263k      0 --:--:-- --:--:-- --:--:-- 1266k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  700k    0  700k    0     0  2103k      0 --:--:-- --:--:-- --:--:-- 2098k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  717k    0  717k    0     0  1662k      0 --:--:-- --:--:-- --:--:-- 1665k\n"
     ]
    }
   ],
   "source": [
    "!curl https://docs.openvino.ai/2024/ovms_what_is_openvino_model_server.html --create-dirs -o ./docs/ovms_what_is_openvino_model_server.html\n",
    "!curl https://docs.openvino.ai/2024/ovms_docs_metrics.html -o ./docs/ovms_docs_metrics.html\n",
    "!curl https://docs.openvino.ai/2024/ovms_docs_streaming_endpoints.html -o ./docs/ovms_docs_streaming_endpoints.html\n",
    "!curl https://docs.openvino.ai/2024/ovms_docs_target_devices.html -o ./docs/ovms_docs_target_devices.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed9afff-df0f-42f2-8aa7-6b5fff9f794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    helper for loading a single document\n",
    "\n",
    "    Params:\n",
    "      file_path: document path\n",
    "    Returns:\n",
    "      documents loaded\n",
    "\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADERS:\n",
    "        loader_class, loader_args = LOADERS[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"File does not exist '{ext}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75374e4c-3af8-44fa-ad8d-f905766cc407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtrawins/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings=HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={\"device\":\"cpu\"},\n",
    "            show_progress=True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d12abca7-3074-4c87-8ba1-eb27e4332860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading document ./docs/ovms_docs_streaming_endpoints.html...\n",
      "Reading document ./docs/ovms_docs_metrics.html...\n",
      "Reading document ./docs/ovms_what_is_openvino_model_server.html...\n",
      "Reading document ./docs/ovms_docs_target_devices.html...\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for file_path in os.listdir(TARGET_FOLDER):\n",
    "    if not file_path.endswith('.html'):\n",
    "        continue\n",
    "    abs_path = os.path.join(TARGET_FOLDER, file_path)\n",
    "    print(f\"Reading document {abs_path}...\", flush=True)\n",
    "    documents.extend(load_single_document(abs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173aee0c-9cfc-4ad6-bdb5-68df52b797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter_name = \"RecursiveCharacter\"  # PARAM\n",
    "chunk_size=1000  # PARAM\n",
    "chunk_overlap=200  # PARAM\n",
    "text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281c64d8-e188-4712-8aaa-c2d2eb61c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db.delete_collection()\n",
    "except:\n",
    "    pass\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2d944-cba1-44d5-8b9a-fccc7860bb4e",
   "metadata": {},
   "source": [
    "The commands below can be used to test the retriever. It can report the content for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc958cd3-72e6-4311-a77f-44eaa147b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Metrics¶\\n\\nIntroduction¶\\n\\nThis document describes how to use metrics endpoint in the OpenVINO Model Server. They can be applied for:\\n\\nProviding performance and utilization statistics for monitoring and benchmarking purposes\\n\\nAuto scaling of the model server instances in Kubernetes and OpenShift based on application related metrics\\n\\nBuilt-in metrics allow tracking the performance without any extra logic on the client side or using network traffic monitoring tools like load balancers or reverse-proxies.\\n\\nIt also exposes metrics which are not related to the network traffic.\\n\\nFor example, statistics of the inference execution queue, model runtime parameters etc. They can also track the usage based on model version, API type or requested endpoint methods.\\n\\nOpenVINO Model Server metrics are compatible with Prometheus standard\\n\\nThey are exposed on the /metrics endpoint.\\n\\nAvailable metrics families¶\\n\\nMetrics from default list are enabled with the metrics_enable flag or json configuration.' metadata={'source': './docs/ovms_docs_metrics.html'}\n",
      "page_content='They are exposed on the /metrics endpoint.\\n\\nAvailable metrics families¶\\n\\nMetrics from default list are enabled with the metrics_enable flag or json configuration.\\n\\nHowever, you can enable also additional metrics by listing all the metrics you want to enable in the metric_list flag or json configuration.\\n\\nDefault metrics' metadata={'source': './docs/ovms_docs_metrics.html'}\n",
      "page_content='Labels description\\n\\nName Values Description api KServe, TensorFlowServing Name of the serving API. interface REST, gRPC Name of the serving interface. method ModelMetadata, ModelReady, ModelInfer, Predict, GetModelStatus, GetModelMetadata Interface methods. version 1, 2, …, n Model version. Note that GetModelStatus and ModelReady do not have the version label. name As defined in model server config Model name or DAG name.\\n\\nEnable metrics¶\\n\\nBy default, the metrics feature is disabled.\\n\\nMetrics endpoint is using the same port as the REST interface for running the model queries.\\n\\nIt is required to enable REST in the model server by setting the parameter –rest_port.\\n\\nTo enable default metrics set you need to specify the metrics_enable flag or json setting:\\n\\nOption 1: CLI¶\\n\\nwget\\n\\nN\\n\\nhttps://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/2/resnet50-binary-0001/FP32-INT1/resnet50-binary-0001.\\n\\n{xml,bin\\n\\nP\\n\\nmodels/resnet50/1\\ndocker\\n\\nrun\\n\\nd\\n\\nu\\n\\n$(id\\n\\nu\\n\\nv\\n\\n$(\\n\\npwd' metadata={'source': './docs/ovms_docs_metrics.html'}\n",
      "page_content='However, you can enable also additional metrics by listing all the metrics you want to enable in the metric_list flag or json configuration.\\n\\nDefault metrics\\n\\nType Name Labels Description gauge ovms_streams name,version Number of OpenVINO execution streams gauge ovms_current_requests name,version Number of requests being currently processed by the model server counter ovms_requests_success api,interface,method,name,version Number of successful requests to a model or a DAG. counter ovms_requests_fail api,interface,method,name,version Number of failed requests to a model or a DAG. histogram ovms_request_time_us interface,name,version Processing time of requests to a model or a DAG. histogram ovms_inference_time_us name,version Inference execution time in the OpenVINO backend. histogram ovms_wait_for_infer_req_time_us name,version Request waiting time in the scheduling queue. Indicates how long the request has to wait before required resources are assigned to it.\\n\\nOptional metrics' metadata={'source': './docs/ovms_docs_metrics.html'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_search_top_k = 4\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": vector_search_top_k})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What metrics are supported in the model server?\")\n",
    "print(retrieved_docs[0])\n",
    "print(retrieved_docs[1])\n",
    "print(retrieved_docs[2])\n",
    "print(retrieved_docs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29941c55-e781-42a0-b578-2e463e1b879d",
   "metadata": {},
   "source": [
    "Change the base url and model name depending on the model server deployment and configuration. It is important to use /v3/ part which is specific for the OpenVINO Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d6a7cc-de79-4255-81b3-80ed44f2ff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtrawins/.local/lib/python3.10/site-packages/langchain_core/utils/utils.py:161: UserWarning: WARNING! seed is not default parameter.\n",
      "                seed was transferred to model_kwargs.\n",
      "                Please confirm that seed is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=\"http://localhost:8000/v3\",\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    temperature=0.0,\n",
    "    seed=5,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba4bc8d-abab-4374-9776-2f531ba62f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt input_variables=['context', 'question'] template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=PromptTemplate(input_variables=['context', 'question'], \n",
    "                      template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\")\n",
    "\n",
    "print(\"prompt\", prompt)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5109c9-a43d-4f33-be41-8f982ead321c",
   "metadata": {},
   "source": [
    "Below you can start the RAG chain using your own query. It will call the embedding model first, retreive the relevant context and pass it to the LLM endpoint in a single request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd5b1d12-0b36-4818-a6e5-d94868c8c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 32.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the OpenVINO Model Server supports the following metrics:\n",
      "\n",
      "1. Gauge metrics:\n",
      "\t* `ovms_streams`: Number of OpenVINO execution streams\n",
      "\t* `ovms_current_requests`: Number of requests being currently processed by the model server\n",
      "2. Counter metrics:\n",
      "\t* `ovms_requests_success`: Number of successful requests to a model or a DAG\n",
      "\t* `ovms_requests_fail`: Number of failed requests to a model or a DAG\n",
      "3. Histogram metrics:\n",
      "\t* `ovms_request_time_us`: Processing time of requests to a model or a DAG\n",
      "\t* `ovms_inference_time_us`: Inference execution time in the OpenVINO backend\n",
      "\t* `ovms_wait_for_infer_req_time_us`: Request waiting time in the scheduling queue\n",
      "\n",
      "These metrics are exposed on the `/metrics` endpoint and are compatible with the Prometheus "
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"Which metrics are supported in the model server? Give examples.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58217565-fc81-454c-87c7-9590de2c4fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
