{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437c0b6d-58ee-4726-8008-93e223ad5cb7",
   "metadata": {},
   "source": [
    "# RAG demo with OpenVINO Model Server and langchain\n",
    "This demo shows how to use Retrieval Augmented Generation with langchain and gen AI endpoint from OpenVINO Model Server.\n",
    "\n",
    "It employs the `chat/completion` and `embeddings` and `rerank` endpoints.\n",
    "\n",
    "It assumes the model server is already deployed on the same machine on port 8000 with model `meta-llama/Meta-Llama-3-8B-Instruct` for `chat/completions` and `Alibaba-NLP/gte-large-en-v1.5` for `embeddings` and `BAAI/bge-reranker-large` for `rerank` endpoint.\n",
    "\n",
    "Check https://github.com/openvinotoolkit/model_server/tree/main/demos/continuous_batching/rag/README.md to see how they can be deployed.\n",
    "LLM model, embeddings and rerank can be on hosted on the same model server instance or separately as needed.\n",
    "openai_api_base , base_url parameters with the target url and model names in the commands might need to be adjusted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a498b22-fb7f-4fa1-a4d8-a0d983eb4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5158e553-3355-46af-879b-e7ef09058aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Document Splitter\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader, )\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b52e35-ea8a-4c84-b98c-24f774b03721",
   "metadata": {},
   "source": [
    "The documents to scan with knowledge context are to be placed in ./docs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25410751-e4a5-4348-8376-938dc4ffd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FOLDER = \"./docs/\"\n",
    "\n",
    "TEXT_SPLITERS = {\n",
    "    \"Character\": CharacterTextSplitter,\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,\n",
    "    \"Markdown\": MarkdownTextSplitter,\n",
    "}\n",
    "\n",
    "LOADERS = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b57421-d418-4c4d-bd98-21d67e7fd31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  750k    0  750k    0     0  1097k      0 --:--:-- --:--:-- --:--:-- 1098k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  775k    0  775k    0     0  2879k      0 --:--:-- --:--:-- --:--:-- 2881k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  753k    0  753k    0     0  5232k      0 --:--:-- --:--:-- --:--:-- 5270k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  773k    0  773k    0     0  5507k      0 --:--:-- --:--:-- --:--:-- 5522k\n"
     ]
    }
   ],
   "source": [
    "!curl https://docs.openvino.ai/2024/openvino-workflow/model-server/ovms_what_is_openvino_model_server.html --create-dirs -o ./docs/ovms_what_is_openvino_model_server.html\n",
    "!curl https://docs.openvino.ai/2024/openvino-workflow/model-server/ovms_docs_metrics.html -o ./docs/ovms_docs_metrics.html\n",
    "!curl https://docs.openvino.ai/2024/openvino-workflow/model-server/ovms_docs_streaming_endpoints.html -o ./docs/ovms_docs_streaming_endpoints.html\n",
    "!curl https://docs.openvino.ai/2024/openvino-workflow/model-server/ovms_docs_target_devices.html -o ./docs/ovms_docs_target_devices.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed9afff-df0f-42f2-8aa7-6b5fff9f794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    helper for loading a single document\n",
    "\n",
    "    Params:\n",
    "      file_path: document path\n",
    "    Returns:\n",
    "      documents loaded\n",
    "\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADERS:\n",
    "        loader_class, loader_args = LOADERS[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"File does not exist '{ext}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75374e4c-3af8-44fa-ad8d-f905766cc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "    api_key=\"unused\",\n",
    "    tiktoken_enabled=False,\n",
    "    base_url=\"http://localhost:8000/v3\",\n",
    "    embedding_ctx_length=8190,  # 8190 is the model max context length subtracted by 2 special tokens \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d12abca7-3074-4c87-8ba1-eb27e4332860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading document ./docs/ovms_docs_streaming_endpoints.html...\n",
      "Reading document ./docs/ovms_docs_metrics.html...\n",
      "Reading document ./docs/ovms_what_is_openvino_model_server.html...\n",
      "Reading document ./docs/ovms_docs_target_devices.html...\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for file_path in os.listdir(TARGET_FOLDER):\n",
    "    if not file_path.endswith('.html'):\n",
    "        continue\n",
    "    abs_path = os.path.join(TARGET_FOLDER, file_path)\n",
    "    print(f\"Reading document {abs_path}...\", flush=True)\n",
    "    documents.extend(load_single_document(abs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173aee0c-9cfc-4ad6-bdb5-68df52b797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter_name = \"RecursiveCharacter\"  # PARAM\n",
    "chunk_size=1000  # PARAM\n",
    "chunk_overlap=200  # PARAM\n",
    "text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281c64d8-e188-4712-8aaa-c2d2eb61c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtrawins/model_server/demos/continuous_batching/rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db.delete_collection()\n",
    "except:\n",
    "    pass\n",
    "db = FAISS.from_documents(texts, embeddings)  # This command populates vector store with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f316f93a-becd-48c6-a40b-dad72fc44c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2d944-cba1-44d5-8b9a-fccc7860bb4e",
   "metadata": {},
   "source": [
    "The commands below can be used to test the retriever. It can report the content for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc958cd3-72e6-4311-a77f-44eaa147b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Metrics#\n",
      "\n",
      "Introduction#\n",
      "\n",
      "This document describes how to use metrics endpoint in the OpenVINO Model Server. They can be applied for:\n",
      "\n",
      "Providing performance and utilization statistics for monitoring and benchmarking purposes\n",
      "\n",
      "Auto scaling of the model server instances in Kubernetes and OpenShift based on application related metrics\n",
      "\n",
      "Built-in metrics allow tracking the performance without any extra logic on the client side or using network traffic monitoring tools like load balancers or reverse-proxies.\n",
      "\n",
      "It also exposes metrics which are not related to the network traffic.\n",
      "\n",
      "For example, statistics of the inference execution queue, model runtime parameters etc. They can also track the usage based on model version, API type or requested endpoint methods.\n",
      "\n",
      "OpenVINO Model Server metrics are compatible with Prometheus standard\n",
      "\n",
      "They are exposed on the /metrics endpoint.\n",
      "\n",
      "Available metrics families#\n",
      "\n",
      "Metrics from default list are enabled with the metrics_enable flag or json configuration.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Labels description\n",
      "\n",
      "Name Values Description api KServe, TensorFlowServing, V3 Name of the serving API. interface REST, gRPC Name of the serving interface. method ModelMetadata, ModelReady, ModelInfer, Predict, GetModelStatus, GetModelMetadata, Unary, Stream Interface methods. version 1, 2, …, n Model version. Note that GetModelStatus and ModelReady and all MediaPipe servables do not have the version label. name As defined in model server config Model name, DAG name or MediaPipe graph name.\n",
      "\n",
      "Enable metrics#\n",
      "\n",
      "By default, the metrics feature is disabled.\n",
      "\n",
      "Metrics endpoint is using the same port as the REST interface for running the model queries.\n",
      "\n",
      "It is required to enable REST in the model server by setting the parameter –rest_port.\n",
      "\n",
      "To enable default metrics set you need to specify the metrics_enable flag or json setting:\n",
      "\n",
      "Option 1: CLI#\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "OpenVINO™ Model Server#\n",
      "\n",
      "Model Server hosts models and makes them accessible to software components over standard network protocols: a client sends a request to the model server, which performs model inference and sends a response back to the client. Model Server offers many advantages for efficient model deployment:\n",
      "\n",
      "Remote inference enables using lightweight clients with only the necessary functions to perform API calls to edge or cloud deployments.\n",
      "\n",
      "Applications are independent of the model framework, hardware device, and infrastructure.\n",
      "\n",
      "Client applications in any programming language that supports REST or gRPC calls can be used to run inference remotely on the model server.\n",
      "\n",
      "Clients require fewer updates since client libraries change very rarely.\n",
      "\n",
      "Model topology and weights are not exposed directly to client applications, making it easier to control access to the model.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Start with the configuration file above#\n",
      "\n",
      "docker run -d -u $(id -u) -v ${PWD}/workspace:/workspace -p 9000:9000 -p 8000:8000 openvino/model_server:latest \\\n",
      "   --config_path /workspace/config.json \\\n",
      "   --port 9000 --rest_port 8000\n",
      "\n",
      "Example response from metrics endpoint#\n",
      "\n",
      "To use data from metrics endpoint you can use the curl command:\n",
      "\n",
      "curl http://localhost:8000/metrics\n",
      "\n",
      "Example metrics output\n",
      "\n",
      "Performance considerations#\n",
      "\n",
      "Collecting metrics has negligible performance overhead when used with models of average size and complexity. However when used with very lightweight, fast models which inference time is very short, the metric incrementation can take noticeable proportion of the processing time. Consider it while enabling metrics for such models.\n",
      "\n",
      "Metrics implementation for DAG pipelines#\n",
      "\n",
      "For DAG pipeline execution there are relevant 3 metrics listed below. They track the execution of the whole pipeline, gathering information from all pipeline nodes.\n",
      "\n",
      "DAG metrics\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "The models used by the server need to be stored locally or hosted remotely by object storage services. For more details, refer to Preparing Model Repository documentation. Model server works inside Docker containers, on Bare Metal, and in Kubernetes environment. Start using OpenVINO Model Server with a fast-forward serving example from the Quickstart guide or explore Model Server features.\n",
      "\n",
      "Key features:#\n",
      "\n",
      "[NEW] Efficient Text Generation\n",
      "\n",
      "Python code execution\n",
      "\n",
      "gRPC streaming\n",
      "\n",
      "MediaPipe graphs serving\n",
      "\n",
      "Model management - including model versioning and model updates in runtime\n",
      "\n",
      "Dynamic model inputs\n",
      "\n",
      "Directed Acyclic Graph Scheduler along with custom nodes in DAG pipelines\n",
      "\n",
      "Metrics - metrics compatible with Prometheus standard\n",
      "\n",
      "Support for multiple frameworks, such as TensorFlow, PaddlePaddle and ONNX\n",
      "\n",
      "Support for AI accelerators\n",
      "\n",
      "Additional Resources#\n",
      "\n",
      "Simplified Deployments with OpenVINO™ Model Server and TensorFlow Serving\n"
     ]
    }
   ],
   "source": [
    "vector_search_top_k = 5\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": vector_search_top_k})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Which metrics are supported in the model server? Give examples.\")\n",
    "pretty_print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875703ab-19c7-439f-af1e-8ffd239d183a",
   "metadata": {},
   "source": [
    "\n",
    "Below the document compressor is used to filter the documents to the most relevant for the given query. It employs rerank endpoint in the model server and cohere client.\n",
    "In the response is reported a list of documents limited to top_n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe8590f-fa84-469e-9093-5c2440c5a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Metrics#\n",
      "\n",
      "Introduction#\n",
      "\n",
      "This document describes how to use metrics endpoint in the OpenVINO Model Server. They can be applied for:\n",
      "\n",
      "Providing performance and utilization statistics for monitoring and benchmarking purposes\n",
      "\n",
      "Auto scaling of the model server instances in Kubernetes and OpenShift based on application related metrics\n",
      "\n",
      "Built-in metrics allow tracking the performance without any extra logic on the client side or using network traffic monitoring tools like load balancers or reverse-proxies.\n",
      "\n",
      "It also exposes metrics which are not related to the network traffic.\n",
      "\n",
      "For example, statistics of the inference execution queue, model runtime parameters etc. They can also track the usage based on model version, API type or requested endpoint methods.\n",
      "\n",
      "OpenVINO Model Server metrics are compatible with Prometheus standard\n",
      "\n",
      "They are exposed on the /metrics endpoint.\n",
      "\n",
      "Available metrics families#\n",
      "\n",
      "Metrics from default list are enabled with the metrics_enable flag or json configuration.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "import cohere\n",
    "co = cohere.Client(\n",
    "    api_key=\"no_used\",\n",
    "    base_url=\"http://localhost:8000/v3/\",\n",
    ")\n",
    "compressor = CohereRerank(model=\"BAAI/bge-reranker-large\", client=co, top_n=1)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Which metrics are supported in the model server? Give examples.\",\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29941c55-e781-42a0-b578-2e463e1b879d",
   "metadata": {},
   "source": [
    "Finally, LLM component needs to be configured. Here will be used chat/completions endpoint from the model server.\n",
    "Change the base url and model name depending on the model server deployment and configuration. It is important to use /v3/ part which is specific for the OpenVINO Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d6a7cc-de79-4255-81b3-80ed44f2ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=\"http://localhost:8000/v3\",\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b37bd5-cf76-42f6-bb8a-cb71944a0032",
   "metadata": {},
   "source": [
    "With all the building blocks defined, the RAG chain is established to link all the components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba4bc8d-abab-4374-9776-2f531ba62f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt input_variables=['context', 'question'] input_types={} partial_variables={} template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=PromptTemplate(input_variables=['context', 'question'], \n",
    "                      template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\")\n",
    "\n",
    "print(\"prompt\", prompt)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5109c9-a43d-4f33-be41-8f982ead321c",
   "metadata": {},
   "source": [
    "Below you can start the RAG chain using your own query. It will call the embedding model first, retrieve the relevant context and pass it to the LLM endpoint in a single request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5b1d12-0b36-4818-a6e5-d94868c8c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the OpenVINO Model Server supports metrics that are compatible with the Prometheus standard and are exposed on the /metrics endpoint. These metrics can be used for monitoring and benchmarking purposes, as well as for auto-scaling model server instances.\n",
      "\n",
      "Examples of metrics supported by the model server include:\n",
      "\n",
      "* Statistics of the inference execution queue\n",
      "* Model runtime parameters\n",
      "* Usage based on model version, API type, or requested endpoint methods\n",
      "\n",
      "These metrics can be enabled using the `metrics_enable` flag or a JSON configuration."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"Which metrics are supported in the model server? Give examples.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b978f46-9103-4b57-956a-2e63e944318b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
