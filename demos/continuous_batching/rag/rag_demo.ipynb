{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437c0b6d-58ee-4726-8008-93e223ad5cb7",
   "metadata": {},
   "source": [
    "# RAG demo with OpenVINO Model Server and langchain\n",
    "This demo shows how to use Retrieval Augmented Generation with langchain and gen AI endpoint from OpenVINO Model Server.\n",
    "\n",
    "It employs the `chat/completion` and `embeddings` and `rerank` endpoints.\n",
    "\n",
    "It assumes the model server is already deployed on the same machine on port 8000 with:\n",
    "\n",
    "OpenVINO models:\n",
    " `OpenVINO/Qwen3-8B-int4-ov` for `chat/completions` and `OpenVINO/bge-base-en-v1.5-fp16-ov` for `embeddings` and `OpenVINO/bge-reranker-base-fp16-ov` for `rerank` endpoint.\n",
    "\n",
    "or\n",
    "Converted models:\n",
    " `meta-llama/Meta-Llama-3-8B-Instruct` for `chat/completions` and `Alibaba-NLP/gte-large-en-v1.5` for `embeddings` and `BAAI/bge-reranker-large` for `rerank` endpoint. \n",
    "\n",
    "Check https://github.com/openvinotoolkit/model_server/tree/releases/2025/2/demos/continuous_batching/rag/README.md to see how they can be deployed.\n",
    "LLM model, embeddings and rerank can be on hosted on the same model server instance or separately as needed.\n",
    "openai_api_base , base_url parameters with the target url and model names in the commands might need to be adjusted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a498b22-fb7f-4fa1-a4d8-a0d983eb4565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7212515f-b59b-498c-a66a-f6c59de8fcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7441ba4307bb4841829303d2c288cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(RadioButtons(description='OVMS running with:', options=('OpenVINO models', 'Converted models'),…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import RadioButtons, Text, VBox\n",
    "options = [\"OpenVINO models\", \"Converted models\"]\n",
    "output_text = Text()\n",
    "def on_radio_change(change):\n",
    "    output_text.value = change['new']\n",
    "\n",
    "selection = RadioButtons(options=options, value=\"OpenVINO models\", description=\"OVMS running with:\", disabled=False)\n",
    "selection.observe(on_radio_change, names='value')\n",
    "VBox([selection, output_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b085cd3f-5473-474e-b35c-a1a548d50f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text(value='OpenVINO models')\n"
     ]
    }
   ],
   "source": [
    "print(output_text)\n",
    "if output_text.value == \"OpenVINO models\":\n",
    "    embeddings_model = \"OpenVINO/bge-base-en-v1.5-fp16-ov\"\n",
    "    rerank_model = \"OpenVINO/bge-reranker-base-fp16-ov\"\n",
    "    chat_model = \"OpenVINO/Qwen3-8B-int4-ov\"\n",
    "else:\n",
    "    embeddings_model = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "    rerank_model = \"BAAI/bge-reranker-large\"\n",
    "    chat_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5158e553-3355-46af-879b-e7ef09058aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Document Splitter\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader, )\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b52e35-ea8a-4c84-b98c-24f774b03721",
   "metadata": {},
   "source": [
    "The documents to scan with knowledge context are to be placed in ./docs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25410751-e4a5-4348-8376-938dc4ffd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FOLDER = \"./docs/\"\n",
    "\n",
    "TEXT_SPLITERS = {\n",
    "    \"Character\": CharacterTextSplitter,\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,\n",
    "    \"Markdown\": MarkdownTextSplitter,\n",
    "}\n",
    "\n",
    "LOADERS = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b57421-d418-4c4d-bd98-21d67e7fd31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  742k    0  742k    0     0  1253k      0 --:--:-- --:--:-- --:--:-- 1251k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  768k    0  768k    0     0  1433k      0 --:--:-- --:--:-- --:--:-- 1435k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  745k    0  745k    0     0  1083k      0 --:--:-- --:--:-- --:--:-- 1081k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  764k    0  764k    0     0   641k      0 --:--:--  0:00:01 --:--:--  642k\n"
     ]
    }
   ],
   "source": [
    "!curl https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html --create-dirs -o ./docs/ovms_what_is_openvino_model_server.html\n",
    "!curl https://docs.openvino.ai/2025/model-server/ovms_docs_metrics.html -o ./docs/ovms_docs_metrics.html\n",
    "!curl https://docs.openvino.ai/2025/model-server/ovms_docs_streaming_endpoints.html -o ./docs/ovms_docs_streaming_endpoints.html\n",
    "!curl https://docs.openvino.ai/2025/model-server/ovms_docs_target_devices.html -o ./docs/ovms_docs_target_devices.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed9afff-df0f-42f2-8aa7-6b5fff9f794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    helper for loading a single document\n",
    "\n",
    "    Params:\n",
    "      file_path: document path\n",
    "    Returns:\n",
    "      documents loaded\n",
    "\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADERS:\n",
    "        loader_class, loader_args = LOADERS[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"File does not exist '{ext}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75374e4c-3af8-44fa-ad8d-f905766cc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=embeddings_model,\n",
    "    api_key=\"unused\",\n",
    "    tiktoken_enabled=False,\n",
    "    base_url=\"http://localhost:8000/v3\",\n",
    "    embedding_ctx_length=8190,  # 8190 is the model max context length subtracted by 2 special tokens \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12abca7-3074-4c87-8ba1-eb27e4332860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading document ./docs/ovms_docs_metrics.html...\n",
      "Reading document ./docs/ovms_docs_streaming_endpoints.html...\n",
      "Reading document ./docs/ovms_docs_target_devices.html...\n",
      "Reading document ./docs/ovms_what_is_openvino_model_server.html...\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for file_path in os.listdir(TARGET_FOLDER):\n",
    "    if not file_path.endswith('.html'):\n",
    "        continue\n",
    "    abs_path = os.path.join(TARGET_FOLDER, file_path)\n",
    "    print(f\"Reading document {abs_path}...\", flush=True)\n",
    "    documents.extend(load_single_document(abs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173aee0c-9cfc-4ad6-bdb5-68df52b797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter_name = \"RecursiveCharacter\"  # PARAM\n",
    "chunk_size=1000  # PARAM\n",
    "chunk_overlap=200  # PARAM\n",
    "text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281c64d8-e188-4712-8aaa-c2d2eb61c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db.delete_collection()\n",
    "except:\n",
    "    pass\n",
    "db = FAISS.from_documents(texts, embeddings)  # This command populates vector store with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f316f93a-becd-48c6-a40b-dad72fc44c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2d944-cba1-44d5-8b9a-fccc7860bb4e",
   "metadata": {},
   "source": [
    "The commands below can be used to test the retriever. It can report the content for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc958cd3-72e6-4311-a77f-44eaa147b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Metrics#\n",
      "\n",
      "Introduction#\n",
      "\n",
      "This document describes how to use metrics endpoint in the OpenVINO Model Server. They can be applied for:\n",
      "\n",
      "Providing performance and utilization statistics for monitoring and benchmarking purposes\n",
      "\n",
      "Auto scaling of the model server instances in Kubernetes and OpenShift based on application related metrics\n",
      "\n",
      "Built-in metrics allow tracking the performance without any extra logic on the client side or using network traffic monitoring tools like load balancers or reverse-proxies.\n",
      "\n",
      "It also exposes metrics which are not related to the network traffic.\n",
      "\n",
      "For example, statistics of the inference execution queue, model runtime parameters etc. They can also track the usage based on model version, API type or requested endpoint methods.\n",
      "\n",
      "OpenVINO Model Server metrics are compatible with Prometheus standard\n",
      "\n",
      "They are exposed on the /metrics endpoint.\n",
      "\n",
      "Available metrics families#\n",
      "\n",
      "Metrics from default list are enabled with the metrics_enable flag or json configuration.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "MediaPipe graphs serving\n",
      "\n",
      "Model management - including model versioning and model updates in runtime\n",
      "\n",
      "Dynamic model inputs\n",
      "\n",
      "Directed Acyclic Graph Scheduler along with custom nodes in DAG pipelines\n",
      "\n",
      "Metrics - metrics compatible with Prometheus standard\n",
      "\n",
      "Support for multiple frameworks, such as TensorFlow, PaddlePaddle and ONNX\n",
      "\n",
      "Support for AI accelerators\n",
      "\n",
      "Additional Resources#\n",
      "\n",
      "RAG building blocks made easy and affordable with OpenVINO Model Server\n",
      "\n",
      "Simplified Deployments with OpenVINO™ Model Server and TensorFlow Serving\n",
      "\n",
      "Inference Scaling with OpenVINO™ Model Server in Kubernetes and OpenShift Clusters\n",
      "\n",
      "Benchmarking results\n",
      "\n",
      "Release Notes\n",
      "\n",
      "On this page\n",
      "\n",
      ".pdf .zip\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Configuration file with all metrics enabled#\n",
      "\n",
      "echo '{\n",
      " \"model_config_list\": [\n",
      "     {\n",
      "        \"config\": {\n",
      "             \"name\": \"resnet\",\n",
      "             \"base_path\": \"/workspace/models/resnet50\"\n",
      "        }\n",
      "     }\n",
      " ],\n",
      " \"monitoring\":\n",
      "     {\n",
      "         \"metrics\":\n",
      "         {\n",
      "             \"enable\" : true,\n",
      "             \"metrics_list\": \n",
      "                 [ \"ovms_requests_success\",\n",
      "                 \"ovms_requests_fail\",\n",
      "                 \"ovms_requests_accepted\",\n",
      "                 \"ovms_requests_rejected\",\n",
      "                 \"ovms_responses\",\n",
      "                 \"ovms_inference_time_us\",\n",
      "                 \"ovms_wait_for_infer_req_time_us\",\n",
      "                 \"ovms_request_time_us\",\n",
      "                 \"ovms_current_requests\",\n",
      "                 \"ovms_current_graphs\",\n",
      "                 \"ovms_infer_req_active\",\n",
      "                 \"ovms_streams\",\n",
      "                 \"ovms_infer_req_queue_size\"]\n",
      "         }\n",
      "     }\n",
      "}' > workspace/config.json\n",
      "\n",
      "Start with the configuration file above#\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "You can enable from one up to all the metrics available at once.\n",
      "\n",
      "To enable specific set of metrics you need to specify the metrics_list flag or json setting:\n",
      "\n",
      "Option 1: CLI#\n",
      "\n",
      "wget -N https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/2/resnet50-binary-0001/FP32-INT1/resnet50-binary-0001.{xml,bin} -P models/resnet50/1\n",
      "docker run -d -u $(id -u) -v $(pwd)/models:/models -p 9000:9000 -p 8000:8000 openvino/model_server:latest \\\n",
      "      --model_name resnet --model_path /models/resnet50  --port 9000 \\\n",
      "      --rest_port 8000 \\\n",
      "      --metrics_enable \\\n",
      "      --metrics_list ovms_requests_success,ovms_infer_req_queue_size\n",
      "\n",
      "Option 2: Configuration file#\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "Labels description\n",
      "\n",
      "Name Values Description api KServe, TensorFlowServing, V3 Name of the serving API. interface REST, gRPC Name of the serving interface. method ModelMetadata, ModelReady, ModelInfer, Predict, GetModelStatus, GetModelMetadata, Unary, Stream Interface methods. version 1, 2, …, n Model version. Note that GetModelStatus and ModelReady and all MediaPipe servables do not have the version label. name As defined in model server config Model name, DAG name or MediaPipe graph name.\n",
      "\n",
      "Enable metrics#\n",
      "\n",
      "By default, the metrics feature is disabled.\n",
      "\n",
      "Metrics endpoint is using the same port as the REST interface for running the model queries.\n",
      "\n",
      "It is required to enable REST in the model server by setting the parameter –rest_port.\n",
      "\n",
      "To enable default metrics set you need to specify the metrics_enable flag or json setting:\n",
      "\n",
      "Option 1: CLI#\n"
     ]
    }
   ],
   "source": [
    "vector_search_top_k = 5\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": vector_search_top_k})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Which metrics are supported in the model server? Give examples.\")\n",
    "pretty_print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875703ab-19c7-439f-af1e-8ffd239d183a",
   "metadata": {},
   "source": [
    "\n",
    "Below the document compressor is used to filter the documents to the most relevant for the given query. It employs rerank endpoint in the model server and cohere client.\n",
    "In the response is reported a list of documents limited to top_n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fe8590f-fa84-469e-9093-5c2440c5a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Metrics#\n",
      "\n",
      "Introduction#\n",
      "\n",
      "This document describes how to use metrics endpoint in the OpenVINO Model Server. They can be applied for:\n",
      "\n",
      "Providing performance and utilization statistics for monitoring and benchmarking purposes\n",
      "\n",
      "Auto scaling of the model server instances in Kubernetes and OpenShift based on application related metrics\n",
      "\n",
      "Built-in metrics allow tracking the performance without any extra logic on the client side or using network traffic monitoring tools like load balancers or reverse-proxies.\n",
      "\n",
      "It also exposes metrics which are not related to the network traffic.\n",
      "\n",
      "For example, statistics of the inference execution queue, model runtime parameters etc. They can also track the usage based on model version, API type or requested endpoint methods.\n",
      "\n",
      "OpenVINO Model Server metrics are compatible with Prometheus standard\n",
      "\n",
      "They are exposed on the /metrics endpoint.\n",
      "\n",
      "Available metrics families#\n",
      "\n",
      "Metrics from default list are enabled with the metrics_enable flag or json configuration.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "import cohere\n",
    "co = cohere.ClientV2(\n",
    "    api_key=\"no_used\",\n",
    "    base_url=\"http://localhost:8000/v3/\",\n",
    ")\n",
    "compressor = CohereRerank(model=rerank_model, client=co, top_n=1)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Which metrics are supported in the model server? Give examples.\",\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29941c55-e781-42a0-b578-2e463e1b879d",
   "metadata": {},
   "source": [
    "Finally, LLM component needs to be configured. Here will be used chat/completions endpoint from the model server.\n",
    "Change the base url and model name depending on the model server deployment and configuration. It is important to use /v3/ part which is specific for the OpenVINO Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d6a7cc-de79-4255-81b3-80ed44f2ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=\"http://localhost:8000/v3\",\n",
    "    model_name=chat_model,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b37bd5-cf76-42f6-bb8a-cb71944a0032",
   "metadata": {},
   "source": [
    "With all the building blocks defined, the RAG chain is established to link all the components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ba4bc8d-abab-4374-9776-2f531ba62f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt input_variables=['context', 'question'] input_types={} partial_variables={} template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=PromptTemplate(input_variables=['context', 'question'], \n",
    "                      template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\")\n",
    "\n",
    "print(\"prompt\", prompt)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5109c9-a43d-4f33-be41-8f982ead321c",
   "metadata": {},
   "source": [
    "Below you can start the RAG chain using your own query. It will call the embedding model first, retrieve the relevant context and pass it to the LLM endpoint in a single request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5b1d12-0b36-4818-a6e5-d94868c8c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking which metrics are supported in the model server and wants examples. Let me look at the context provided.\n",
      "\n",
      "The context mentions that the OpenVINO Model Server has a metrics endpoint. It says the metrics can be used for monitoring and benchmarking, auto scaling in Kubernetes/OpenShift, and tracking performance without extra client logic. They also mention that some metrics aren't related to network traffic, like inference execution queue stats and model runtime parameters. Additionally, metrics can track usage based on model version, API type, or endpoint methods. The metrics are compatible with Prometheus and exposed on /metrics. \n",
      "\n",
      "The available metrics families are enabled via metrics_enable flag or JSON config. But the context doesn't list the specific metrics. The example given includes inference execution queue stats and model runtime parameters. The user wants examples, so I should mention those. Also, since the metrics are Prometheus-compatible, maybe list some common Prometheus metrics types, but the context doesn't specify exact names. Wait, the context says \"metrics from default list\" but doesn't list them. So maybe the answer should state that the exact metrics are part of the default list enabled by the flag, and examples include inference queue stats and model runtime parameters. Also, mention that they can track usage based on model version, API type, etc. Since the context doesn't provide a full list, the answer should refer to the examples given and note that the exact metrics are determined by the server's default list. So the answer would include the examples from the context and mention compatibility with Prometheus, but not list all metrics since they aren't provided.\n",
      "</think>\n",
      "\n",
      "The OpenVINO Model Server supports metrics exposed via the `/metrics` endpoint, compatible with Prometheus. These metrics include performance and utilization statistics for monitoring and benchmarking, as well as data for auto-scaling in Kubernetes/OpenShift. Examples of supported metrics include:  \n",
      "- **Inference execution queue statistics** (e.g., queue length, latency).  \n",
      "- **Model runtime parameters** (e.g., inference time, throughput).  \n",
      "- **Usage tracking** based on model version, API type, or endpoint methods.  \n",
      "\n",
      "Specific metrics are enabled via the `metrics_enable` flag or JSON configuration. While the exact list of metrics is determined by the server's default configuration, the examples highlight their utility for monitoring and optimization. For precise metric names, refer to the Prometheus-compatible format exposed by the server."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"Which metrics are supported in the model server? Give examples.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e021ada-d63a-43d2-ad0d-20664f2867a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
