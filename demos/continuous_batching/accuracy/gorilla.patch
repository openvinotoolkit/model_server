diff --git a/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py b/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py
index bb625d2..7204adb 100644
--- a/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py
+++ b/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py
@@ -2153,6 +2153,30 @@ third_party_inference_model_map = {
         is_fc_model=True,
         underscore_to_dot=True,
     ),
+    "ovms-model": ModelConfig(
+        model_name="ovms-model",
+        display_name="ovms-model",
+        url="http://localhost:8000/v3",
+        org="ovms",
+        license="apache-2.0",
+        model_handler=OpenAICompletionsHandler,
+        input_price=None,
+        output_price=None,
+        is_fc_model=True,
+        underscore_to_dot=True,
+    ),
+    "ovms-model-stream": ModelConfig(
+        model_name="ovms-model-stream",
+        display_name="ovms-model-stream",
+        url="http://localhost:8000/v3",
+        org="ovms",
+        license="apache-2.0",
+        model_handler=QwenAPIHandler,
+        input_price=None,
+        output_price=None,
+        is_fc_model=True,
+        underscore_to_dot=True,
+    ),
 }
 
 
diff --git a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py
index 357584f..e45e12c 100644
--- a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py
+++ b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py
@@ -38,10 +38,10 @@ class OpenAICompletionsHandler(BaseHandler):
 
         kwargs = {}
 
-        if api_key := os.getenv("OPENAI_API_KEY"):
+        if api_key := os.getenv("OPENAI_API_KEY","unused"):
             kwargs["api_key"] = api_key
 
-        if base_url := os.getenv("OPENAI_BASE_URL"):
+        if base_url := os.getenv("OPENAI_BASE_URL","http://localhost:8000/v3"):
             kwargs["base_url"] = base_url
 
         if headers_env := os.getenv("OPENAI_DEFAULT_HEADERS"):
@@ -85,6 +85,9 @@ class OpenAICompletionsHandler(BaseHandler):
             "messages": message,
             "model": self.model_name,
             "temperature": self.temperature,
+            "max_completion_tokens": 2048,
+            "tool_choice": os.getenv("TOOL_CHOICE", "auto"),
+            "extra_body": {"chat_template_kwargs": json.loads(os.getenv("CHAT_TEMPLATE_KWARGS", "{}"))},
             "store": False,
         }
 
diff --git a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py
index 10f1a08..50890c7 100644
--- a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py
+++ b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py
@@ -7,6 +7,7 @@ from openai import OpenAI
 from overrides import override
 from qwen_agent.llm import get_chat_model
 import time
+import json
 
 class QwenAPIHandler(OpenAICompletionsHandler):
     """
@@ -28,8 +29,8 @@ class QwenAPIHandler(OpenAICompletionsHandler):
         super().__init__(model_name, temperature, registry_name, is_fc_model, **kwargs)
         self.model_style = ModelStyle.OPENAI_COMPLETIONS
         self.client = OpenAI(
-            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
-            api_key=os.getenv("QWEN_API_KEY"),
+            base_url=os.getenv("OPENAI_BASE_URL", "https://localhost:8000/v3"),
+            api_key=os.getenv("QWEN_API_KEY","unused"),
         )
 
     #### FC methods ####
@@ -45,9 +46,9 @@ class QwenAPIHandler(OpenAICompletionsHandler):
             model=self.model_name.replace("-FC", ""),
             tools=tools,
             parallel_tool_calls=True,
-            extra_body={
-                "enable_thinking": True
-            },
+            max_completion_tokens=2048,
+            tool_choice=os.getenv("TOOL_CHOICE", "auto"),
+            extra_body={"chat_template_kwargs": json.loads(os.getenv("CHAT_TEMPLATE_KWARGS", "{}"))},
             stream=True,
             stream_options={
                 "include_usage": True
@@ -352,4 +353,4 @@ class QwenAgentNoThinkHandler(QwenAgentThinkHandler):
             'timeout': 1000,
             'max_tokens': 16384
         }
-    })
\ No newline at end of file
+    })
diff --git a/berkeley-function-call-leaderboard/bfcl_eval/utils.py b/berkeley-function-call-leaderboard/bfcl_eval/utils.py
index 552a10f..ade0e94 100644
--- a/berkeley-function-call-leaderboard/bfcl_eval/utils.py
+++ b/berkeley-function-call-leaderboard/bfcl_eval/utils.py
@@ -356,7 +356,7 @@ def load_file(file_path, sort_by_id: bool = False, use_lock: bool = True) -> lis
     result = []
 
     def _load_entries(input_path: str) -> None:
-        with open(input_path) as f:
+        with open(input_path, encoding="utf-8") as f:
             file = f.readlines()
             for line in file:
                 content = json.loads(line)
