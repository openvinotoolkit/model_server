diff --git a/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py b/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py
index 73731c0..0d966ed 100644
--- a/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py
+++ b/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py
@@ -2060,6 +2060,30 @@ third_party_inference_model_map = {
         is_fc_model=True,
         underscore_to_dot=False,
     ),
+    "ovms-model": ModelConfig(
+        model_name="ovms-model",
+        display_name="ovms-model",
+        url="http://localhost:8000/v3",
+        org="ovms",
+        license="apache-2.0",
+        model_handler=OpenAICompletionsHandler,
+        input_price=None,
+        output_price=None,
+        is_fc_model=True,
+        underscore_to_dot=True,
+    ),
+    "ovms-model-stream": ModelConfig(
+        model_name="ovms-model-stream",
+        display_name="ovms-model-stream",
+        url="http://localhost:8000/v3",
+        org="ovms",
+        license="apache-2.0",
+        model_handler=QwenAPIHandler,
+        input_price=None,
+        output_price=None,
+        is_fc_model=True,
+        underscore_to_dot=True,
+    ),
 }
 
 
diff --git a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py
index 8665234..9e85e59 100644
--- a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py
+++ b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py
@@ -23,7 +23,7 @@ class OpenAICompletionsHandler(BaseHandler):
     def __init__(self, model_name, temperature) -> None:
         super().__init__(model_name, temperature)
         self.model_style = ModelStyle.OpenAI_Completions
-        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
+        self.client = OpenAI(base_url=os.getenv("OPENAI_BASE_URL"), api_key=os.getenv("OPENAI_API_KEY", "not_used"),timeout=os.getenv("OPENAI_TIMEOUT", 1600))
 
     def decode_ast(self, result, language="Python"):
         if "FC" in self.model_name or self.is_fc_model:
@@ -61,6 +61,8 @@ class OpenAICompletionsHandler(BaseHandler):
             "messages": message,
             "model": self.model_name.replace("-FC", ""),
             "temperature": self.temperature,
+            "tool_choice": os.getenv("TOOL_CHOICE", "auto"),
+            "max_completion_tokens": 2048,
             "store": False,
         }
 
diff --git a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py
index 9ce4e7d..076e706 100644
--- a/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py
+++ b/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/qwen.py
@@ -21,8 +21,8 @@ class QwenAPIHandler(OpenAICompletionsHandler):
         super().__init__(model_name, temperature)
         self.model_style = ModelStyle.OpenAI_Completions
         self.client = OpenAI(
-            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
-            api_key=os.getenv("QWEN_API_KEY"),
+            base_url=os.getenv("OPENAI_BASE_URL", "https://localhost:8000/v3"),
+            api_key=os.getenv("QWEN_API_KEY", "not_used"),
         )
 
     #### FC methods ####
@@ -38,9 +38,10 @@ class QwenAPIHandler(OpenAICompletionsHandler):
             model=self.model_name.replace("-FC", ""),
             tools=tools,
             parallel_tool_calls=True,
-            extra_body={
-                "enable_thinking": True
-            },
+            extra_body={ "chat_template_kwargs": {
+                "enable_thinking": False
+            }},
+            temperature=self.temperature,
             stream=True,
             stream_options={
                 "include_usage": True
