# Long context optimizations {#ovms_demo_long_context}

Using models with very long context and prompts might be particularly challenging. The key goals are to get maximum throughput, minimal latency and reasonable memory consumption.
It is very common for applications using RAG chain, documents summarization, question answering and many more. 
Here is presented optimizations which can significantly boost performance :
- prefix caching
- KV cache compression

Prefix caching:
Prefix caching in large language models (LLMs) is an optimization technique used to improve performance when processing repeated or static parts of input prompts. Instead of recomputing the model's output for the same prefix (e.g., a fixed instruction or context), the results of the prefix are cached after the first computation. 
When the same prefix is encountered again, the cached results are reused, skipping redundant computations. This reduces latency and computational overhead, especially in scenarios like chatbots or applications with repetitive prompts.

KV cache compression:
KV cache stores the intermediate key and value tensors generated by the modelâ€™s attention layers for each token in the input sequence.
This cache allows the model to avoid recomputing attention for previous tokens when generating new tokens, greatly speeding up inference for long contexts.
For very long contexts or high concurrency, the KV cache can consume a large amount of memory (RAM or VRAM).
Compression reduces this memory usage, enabling longer prompts or more parallel requests without running out of memory.

## Deployment

Let's demonstrate all the optimizations combined and test it with the real life scenario of sending multiple various questions in the same context. It will illustrate the gain from the prefix caching on the first token latency, improved second token latency thanks for prompt lookup and moderate memory consumption despite very long prompts and parallel execution.

Export the model Qwen/Qwen2.5-7B-Instruct-1M which has the max context length of 1 million tokens! 
```
```console
curl https://raw.githubusercontent.com/openvinotoolkit/model_server/refs/heads/releases/2025/2/demos/common/export_models/export_model.py -o export_model.py
pip3 install -r https://raw.githubusercontent.com/openvinotoolkit/model_server/refs/heads/releases/2025/2/demos/common/export_models/requirements.txt
mkdir models
python export_model.py text_generation --source_model Qwen/Qwen2.5-7B-Instruct-1M weight-format fp16 --config_file_path models/config.json --model_repository_path models
```

Start OVMS:
```
docker run -it --rm -u $(id -u) -v $(pwd)/models/:/models:rw registry.toolbox.iotg.sclab.intel.com/openvino/model_server:ubuntu24_main_main_OVMSCOps-7024 --rest_port 8000 --source_model Qwen/Qwen2.5-7B-Instruct-1M --model_repository_path /models --task text_generation --enable_prefix_caching true --kv_cache_precision u8 --target_device CPU --rest_port 8000
```

## Dataset for experiments

To test the performance using vllm benchmarking script, let's create a dataset with long shared context and a set of questions in each request.  That way we can create a dataset with identical very long context with different queries related to the context. That is a common scenario for RAG applications which generates response based on a complete knowledge base. To make this experiment similar to real live, the context is not synthetic but build with the content of Don Quixote story with 10 different questions related to the story. Because the context is reused, it is a perfect case for benefitting from prefix caching. 

```
python make_dataset.py --context_tokens 50000
```

It will create a file called `dataset.json`


## Testing performance

Let's check the performance 
```console
git clone --branch v0.6.0 --depth 1 https://github.com/vllm-project/vllm
cd vllm
pip3 install -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu
cd benchmarks
python benchmark_serving.py --host localhost --port 8000 --endpoint /v3/chat/completions --backend openai-chat --model meta-llama/Llama-3.1-8B-Instruct --dataset-path dataset.json --num-prompts 100 --request-rate inf -- extra_body '{"num_assistant_tokens" : 5, "max_ngram_size": 3 }'
```

The results shown above, despite very long context, have much lower TTFT latency with prefix caching. As long as the beginning of the request prompt is reused, KV cache can be also reused to speed up prompt processing.

## Testing accuracy




This test shows that despite model quantization, KV cache compression and tokens eviction mechanism, the model accuracy is not impacted significantly. The memory consumption is on a reasonable level while running this load on OVMS. Moreover, host RAM on the servers is generally much more available than VRAM on GPU cards. That make Xeon server suitable to running the models with long prompt in higher concurrency.


## Intel GPU considerations

Models with long context can be also successfully used on Intel GPUs and take advantage of the same optimizations. Because of the limitations of the VRAM, there are just a couple of recommendations to follow.

Export the model in INT4 precision

Running the load in high concurrency and long context, is recommended when the requests share the beginning of the prompt. That optimizes performance and allows sharing the KV cache

When running the load with completely different long context for each request, reduce the concurrency level.

