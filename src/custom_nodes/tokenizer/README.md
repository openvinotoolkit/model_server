# Custom node for text tokenization and detokenization for Large Language Models

This custom node project compiles into 2 libraries:
- libtokenizer.so
- libdetokenizer.so

It can be used in OVMS DAG to tokenize string input into tokens accepted by NLP model. The other library can be used to create string from tokens generated by NLP model.

![diagram](diagram.svg)

The nodes statically link [BlingFire from Microsoft](https://github.com/microsoft/BlingFire) library and use it for tokenization.

# Supported models
All models which accept the tokens via 2 inputs: `input_ids` of shape `[B, N]` (where `B` is batch and `N` is max length of tokens per batch) and `attention_mask` of the same shape `[B, N]` which declare the padding using `0` and `1` values. The inputs must have `I64` precision.

The model must output logits with 3 dimensions: `[B, N, K]`, where `K` represents vocabulary size. The precision of the logits should be `FP32`.

Known models with such format:
- [GPT-2](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/gpt-2)
- [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)

**NOTE:** Both models work with `gpt2.bin`/`gpt2.i2w` pretrained tokenization models from BlingFire repo. Pretrained tokenization models [can be found here](https://github.com/microsoft/BlingFire/tree/5089d31914cbed7a24589e753bd6cd362a377fbb/ldbsrc/ldb).

# Building custom node library

You can build the shared library of the custom node simply by running the following commands:
```bash
git clone https://github.com/openvinotoolkit/model_server && cd model_server/src/custom_nodes/tokenizer
make
```
It will compile the library inside a docker container and save the results in `lib/<OS>/` folder.

You can also select base OS between UBI 8.6 (redhat) and Ubuntu 20.04 (ubuntu) by setting `BASE_OS` environment variable.
```bash
make BASE_OS=redhat
```

It will compile the libraries:
```bash
ls lib/redhat -A1

libdetokenizer.so
libtokenizer.so
```


# libtokenizer.so inputs

| Input name       | Description           | Shape | Precision |
| ------------- |:-------------:| -----:| -----:|
| texts      | 2D array of bytes, where each row represents single, null terminated sentence, padded with `\0` alignment to longest batch. | `-1,-1` | U8 |

Example 2 strings - `abcd` and `ab`:
```
[
    'a', 'b', 'c', 'd', 0,
    'a', 'b',  0 ,  0 , 0
]
```
OVMS supports automatic conversion from TensorflowServing/KServe proto to 2D U8 data matching the node input format.

# libtokenizer.so outputs
The outputs of this node match inputs of GPT-like models.

| Output name        | Description           | Shape  | Precision |
| ------------- |:-------------:| -----:| -----:|
| input_ids | Returns 2D array of tokenized sentences padded to longest batch of tokens. Padding info required by GPT models are contained in `attention_mask` output | `-1,-1` | I64  |
| attention_mask | Returns padding information, 2D array of `1` and `0` where `0` indicates padding. The same is always identical to `input_ids` output. | `-1,-1` | I64  |

# libdetokenizer.so inputs

| Input name       | Description           | Shape | Precision |
| ------------- |:-------------:| -----:| -----:|
| logits | Output from GPT-like model, the next token prediction tensor | `-1,-1,-1` | FP32 |
| input_ids | Output from tokenizer library required for knowledge of previous tokens (context) | `-1,-1` | I64 |
| attention_mask | Output from tokenizer library required for knowledge of previous tokens (context) | `-1,-1` | I64 |


# libtokenizer.so outputs

| Output name        | Description           | Shape  | Precision |
| ------------- |:-------------:| -----:| -----:|
| texts | Returns 2D array of sentences padded to longest batch of tokens. The sentences contain original content plus the autocompletion provided by prediction of subsequent token | `-1,-1` | U8  |

OVMS supports automatic conversion from `U8 2D` format to TensorflowServing/KServe proto.

# Custom node parameters
Parameters can be defined in pipeline definition in OVMS configuration file. [Read more](https://github.com/openvinotoolkit/model_server/blob/releases/2022/1/docs/custom_node_development.md) about node parameters.

## Parameters for libtokenizer.so
| Parameter        | Description           | Default  | Required |
| ------------- | ------------- | ------------- | ------------ |
| model_path | Local path to [tokenization model](https://github.com/microsoft/BlingFire/tree/5089d31914cbed7a24589e753bd6cd362a377fbb/ldbsrc/ldb) in BlingFire format |  | &check; |
| max_ids_arr_length | Maximum number of tokens to be generated from input sentences. If input string exceeds this amount, the generated tokens are cut. | 1024 | |
| debug  | Defines if debug messages should be displayed | false | |

## Parameters for libdetokenizer.so
| Parameter        | Description           | Default  | Required |
| ------------- | ------------- | ------------- | ------------ |
| model_path | Local path to [detokenization model](https://github.com/microsoft/BlingFire/tree/5089d31914cbed7a24589e753bd6cd362a377fbb/ldbsrc/ldb) in BlingFire format |  | &check; |
| max_buffer_length | Maximum size of text generated by detokenization. This includes context (sentence before autocompletion by GPT-model). If generated text is larger than buffer, it is shrank. This value should generally be larger than `max_ids_arr_length` in tokenization node | 4096 | |
| debug  | Defines if debug messages should be displayed | false | |
