diff --git a/Dockerfile.redhat b/Dockerfile.redhat
index 9e0b46fc..6e0d40c3 100644
--- a/Dockerfile.redhat
+++ b/Dockerfile.redhat
@@ -281,6 +281,15 @@ COPY demos/benchmark/cpp/synthetic_client_async_benchmark.cpp demos/image_classi
 
 COPY src/ /ovms/src/
 
+# Compile tokenizer custom node
+WORKDIR /ovms/src/custom_nodes/tokenizer/build
+RUN cp /ovms/src/custom_node_interface.h /ovms/src/custom_nodes/tokenizer/ && \
+    cp -r /ovms/src/custom_nodes/common /ovms/src/custom_nodes/tokenizer/ && \
+    cmake -DCMAKE_VERBOSE_MAKEFILE=ON -DWITH_TESTS=1 .. && \
+    make -j${JOBS} && \
+    ./test/detokenization_test && \
+    ./test/tokenization_test
+
 # Sample CPU Extension
 WORKDIR /ovms/src/example/SampleCpuExtension/
 RUN make
diff --git a/Dockerfile.ubuntu b/Dockerfile.ubuntu
index 8e214537..1219ed2d 100644
--- a/Dockerfile.ubuntu
+++ b/Dockerfile.ubuntu
@@ -296,6 +296,15 @@ WORKDIR /example_cpp_client
 COPY demos/common/cpp /example_cpp_client/cpp
 COPY demos/benchmark/cpp/synthetic_client_async_benchmark.cpp demos/image_classification/cpp/*.cpp /example_cpp_client/cpp/src/
 
+# Compile tokenizer custom node
+WORKDIR /ovms/src/custom_nodes/tokenizer/build
+RUN cp /ovms/src/custom_node_interface.h /ovms/src/custom_nodes/tokenizer/ && \
+    cp -r /ovms/src/custom_nodes/common /ovms/src/custom_nodes/tokenizer/ && \
+    cmake -DWITH_TESTS=1 .. && \
+    make -j${JOBS} && \
+    ./test/detokenization_test && \
+    ./test/tokenization_test
+
 # Sample CPU Extension
 WORKDIR /ovms/src/example/SampleCpuExtension/
 RUN make
@@ -514,6 +523,8 @@ RUN if ! [[ $debug_bazel_flags == *"PYTHON_DISABLE=1"* ]]; then true ; else exit
     apt-get install libpython$python_version --no-install-recommends -y && \
     apt-get clean && rm -rf /var/lib/apt/lists/* && rm -rf /tmp/*
 
+RUN pip3 install transformers sentence-transformers open-clip-torch chromadb langchain_experimental opencv-python-headless
+
 ENV LD_LIBRARY_PATH=/ovms/lib
 
 RUN echo "The source code of added GPL components is stored in https://storage.openvinotoolkit.org/repositories/openvino/ci_dependencies/container_gpl_sources/ubuntu20/" > /ovms/thirdparty-licenses/GPL.txt
diff --git a/Makefile b/Makefile
index d2c74ef2..26bf03c7 100644
--- a/Makefile
+++ b/Makefile
@@ -68,9 +68,9 @@ FUZZER_BUILD ?= 0
 # NOTE: when changing any value below, you'll need to adjust WORKSPACE file by hand:
 #         - uncomment source build section, comment binary section
 #         - adjust binary version path - version variable is not passed to WORKSPACE file!
-OV_SOURCE_BRANCH ?= 4ef40f0f0e70abf04f13310f3e7a8f6784667dbf  # 2024.1
-OV_CONTRIB_BRANCH ?= 8f238178616b47fb11fc3df0df99baf9e0df397d  # 2024.1
-OV_TOKENIZERS_BRANCH ?= 01f15a0f3a0b631bdbf2b3b91b8e0b88690545ff  # 04.04.2024 master
+OV_SOURCE_BRANCH ?= f4afc983258bcb2592d999ed6700043fdb58ad78  # 2024.1
+OV_CONTRIB_BRANCH ?= 04885b3dee78f08be0994329a8f248913e4217b2  # 2024.1
+OV_TOKENIZERS_BRANCH ?= ad37623ee33548fe3288441a66a0f28ffbed2936  # 2024.1
 
 OV_SOURCE_ORG ?= openvinotoolkit
 OV_CONTRIB_ORG ?= openvinotoolkit
@@ -154,10 +154,10 @@ ifeq ($(findstring ubuntu,$(BASE_OS)),ubuntu)
   endif
   ifeq ($(BASE_OS_TAG),20.04)
 	INSTALL_DRIVER_VERSION ?= "22.43.24595"
-	DLDT_PACKAGE_URL ?= http://s3.toolbox.iotg.sclab.intel.com/ov-packages/l_openvino_toolkit_ubuntu20_2024.1.0.14988.4ef40f0f0e7_x86_64.tgz
+	DLDT_PACKAGE_URL ?= https://storage.openvinotoolkit.org/repositories/openvino/packages/2024.1/linux/l_openvino_toolkit_ubuntu20_2024.1.0.15008.f4afc983258_x86_64.tgz
   else ifeq  ($(BASE_OS_TAG),22.04)
 	INSTALL_DRIVER_VERSION ?= "23.22.26516"
-	DLDT_PACKAGE_URL ?= http://s3.toolbox.iotg.sclab.intel.com/ov-packages/l_openvino_toolkit_ubuntu22_2024.1.0.14988.4ef40f0f0e7_x86_64.tgz
+	DLDT_PACKAGE_URL ?= https://storage.openvinotoolkit.org/repositories/openvino/packages/2024.1/linux/l_openvino_toolkit_ubuntu22_2024.1.0.15008.f4afc983258_x86_64.tgz
   endif
 endif
 ifeq ($(BASE_OS),redhat)
@@ -172,7 +172,7 @@ ifeq ($(BASE_OS),redhat)
   endif	
   DIST_OS=redhat
   INSTALL_DRIVER_VERSION ?= "23.22.26516"
-  DLDT_PACKAGE_URL ?= http://s3.toolbox.iotg.sclab.intel.com/ov-packages/l_openvino_toolkit_rhel8_2024.1.0.14988.4ef40f0f0e7_x86_64.tgz
+  DLDT_PACKAGE_URL ?= https://storage.openvinotoolkit.org/repositories/openvino/packages/2024.1/linux/l_openvino_toolkit_rhel8_2024.1.0.15008.f4afc983258_x86_64.tgz
 endif
 
 OVMS_CPP_DOCKER_IMAGE ?= openvino/model_server
@@ -362,6 +362,7 @@ endif
 ifeq ($(BUILD_CUSTOM_NODES),true)
 	@echo "Building custom nodes"
 	@cd src/custom_nodes && make USE_BUILDX=$(USE_BUILDX) NO_DOCKER_CACHE=$(NO_DOCKER_CACHE) BASE_OS=$(OS) BASE_IMAGE=$(BASE_IMAGE) 
+	@cd src/custom_nodes/tokenizer && make USE_BUILDX=$(USE_BUILDX) NO_DOCKER_CACHE=$(NO_DOCKER_CACHE) BASE_OS=$(OS) BASE_IMAGE=$(BASE_IMAGE) 
 endif
 	@echo "Building docker image $(BASE_OS)"
 	# Provide metadata information into image if defined
diff --git a/README.md b/README.md
index 4e8eeda1..e6ddd305 100644
--- a/README.md
+++ b/README.md
@@ -15,21 +15,21 @@ OpenVINO&trade; Model Server (OVMS) is a high-performance system for serving mod
 
 ![OVMS picture](docs/ovms_high_level.png)
 
-The models used by the server need to be stored locally or hosted remotely by object storage services. For more details, refer to [Preparing Model Repository](https://docs.openvino.ai/nightly/ovms_docs_models_repository.html) documentation. Model server works inside [Docker containers](https://docs.openvino.ai/nightly/ovms_docs_deploying_server.html#deploying-model-server-in-docker-container), on [Bare Metal](https://docs.openvino.ai/nightly/ovms_docs_deploying_server.html#deploying-model-server-on-baremetal-without-container), and in [Kubernetes environment](https://docs.openvino.ai/nightly/ovms_docs_deploying_server.html#deploying-model-server-in-kubernetes).
-Start using OpenVINO Model Server with a fast-forward serving example from the [Quickstart guide](https://docs.openvino.ai/nightly/ovms_docs_quick_start_guide.html) or explore [Model Server features](https://docs.openvino.ai/nightly/ovms_docs_features.html).
+The models used by the server need to be stored locally or hosted remotely by object storage services. For more details, refer to [Preparing Model Repository](https://docs.openvino.ai/2024/ovms_docs_models_repository.html) documentation. Model server works inside [Docker containers](https://docs.openvino.ai/2024/ovms_docs_deploying_server.html#deploying-model-server-in-docker-container), on [Bare Metal](https://docs.openvino.ai/2024/ovms_docs_deploying_server.html#deploying-model-server-on-baremetal-without-container), and in [Kubernetes environment](https://docs.openvino.ai/2024/ovms_docs_deploying_server.html#deploying-model-server-in-kubernetes).
+Start using OpenVINO Model Server with a fast-forward serving example from the [Quickstart guide](https://docs.openvino.ai/2024/ovms_docs_quick_start_guide.html) or explore [Model Server features](https://docs.openvino.ai/2024/ovms_docs_features.html).
 
 Read [release notes](https://github.com/openvinotoolkit/model_server/releases) to find out whatâ€™s new.
 
 ### Key features:
-- **[NEW]** [Python code execution](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html)
-- **[NEW]** [gRPC streaming](https://docs.openvino.ai/nightly/ovms_docs_streaming_endpoints.html)
-- [MediaPipe graphs serving](https://docs.openvino.ai/nightly/ovms_docs_mediapipe.html) 
-- Model management - including [model versioning](https://docs.openvino.ai/nightly/ovms_docs_model_version_policy.html) and [model updates in runtime](https://docs.openvino.ai/nightly/ovms_docs_online_config_changes.html)
-- [Dynamic model inputs](https://docs.openvino.ai/nightly/ovms_docs_shape_batch_layout.html)
-- [Directed Acyclic Graph Scheduler](https://docs.openvino.ai/nightly/ovms_docs_dag.html) along with [custom nodes in DAG pipelines](https://docs.openvino.ai/nightly/ovms_docs_custom_node_development.html)
-- [Metrics](https://docs.openvino.ai/nightly/ovms_docs_metrics.html) - metrics compatible with Prometheus standard
+- **[NEW]** [Python code execution](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html)
+- **[NEW]** [gRPC streaming](https://docs.openvino.ai/2024/ovms_docs_streaming_endpoints.html)
+- [MediaPipe graphs serving](https://docs.openvino.ai/2024/ovms_docs_mediapipe.html) 
+- Model management - including [model versioning](https://docs.openvino.ai/2024/ovms_docs_model_version_policy.html) and [model updates in runtime](https://docs.openvino.ai/2024/ovms_docs_online_config_changes.html)
+- [Dynamic model inputs](https://docs.openvino.ai/2024/ovms_docs_shape_batch_layout.html)
+- [Directed Acyclic Graph Scheduler](https://docs.openvino.ai/2024/ovms_docs_dag.html) along with [custom nodes in DAG pipelines](https://docs.openvino.ai/2024/ovms_docs_custom_node_development.html)
+- [Metrics](https://docs.openvino.ai/2024/ovms_docs_metrics.html) - metrics compatible with Prometheus standard
 - Support for multiple frameworks, such as TensorFlow, PaddlePaddle and ONNX
-- Support for [AI accelerators](https://docs.openvino.ai/nightly/about-openvino/compatibility-and-support/supported-devices.html)
+- Support for [AI accelerators](https://docs.openvino.ai/2024/about-openvino/compatibility-and-support/supported-devices.html)
 
 **Note:** OVMS has been tested on RedHat, and Ubuntu. The latest publicly released docker images are based on Ubuntu and UBI.
 They are stored in:
@@ -39,26 +39,26 @@ They are stored in:
 
 ## Run OpenVINO Model Server
 
-A demonstration on how to use OpenVINO Model Server can be found in [our quick-start guide](https://docs.openvino.ai/nightly/ovms_docs_quick_start_guide.html). 
+A demonstration on how to use OpenVINO Model Server can be found in [our quick-start guide](https://docs.openvino.ai/2024/ovms_docs_quick_start_guide.html). 
 For more information on using Model Server in various scenarios you can check the following guides:
 
-* [Model repository configuration](https://docs.openvino.ai/nightly/ovms_docs_models_repository.html)
+* [Model repository configuration](https://docs.openvino.ai/2024/ovms_docs_models_repository.html)
 
-* [Deployment options](https://docs.openvino.ai/nightly/ovms_docs_deploying_server.html)
+* [Deployment options](https://docs.openvino.ai/2024/ovms_docs_deploying_server.html)
 
-* [Performance tuning](https://docs.openvino.ai/nightly/ovms_docs_performance_tuning.html)
+* [Performance tuning](https://docs.openvino.ai/2024/ovms_docs_performance_tuning.html)
 
-* [Directed Acyclic Graph Scheduler](https://docs.openvino.ai/nightly/ovms_docs_dag.html)
+* [Directed Acyclic Graph Scheduler](https://docs.openvino.ai/2024/ovms_docs_dag.html)
 
-* [Custom nodes development](https://docs.openvino.ai/nightly/ovms_docs_custom_node_development.html)
+* [Custom nodes development](https://docs.openvino.ai/2024/ovms_docs_custom_node_development.html)
 
-* [Serving stateful models](https://docs.openvino.ai/nightly/ovms_docs_stateful_models.html)
+* [Serving stateful models](https://docs.openvino.ai/2024/ovms_docs_stateful_models.html)
 
 * [Deploy using a Kubernetes Helm Chart](https://github.com/openvinotoolkit/operator/tree/main/helm-charts/ovms)
 
 * [Deployment using Kubernetes Operator](https://operatorhub.io/operator/ovms-operator)
 
-* [Using binary input data](https://docs.openvino.ai/nightly/ovms_docs_binary_input.html)
+* [Using binary input data](https://docs.openvino.ai/2024/ovms_docs_binary_input.html)
 
 
 
@@ -72,7 +72,7 @@ For more information on using Model Server in various scenarios you can check th
 
 * [RESTful API](https://restfulapi.net/)
 
-* [Benchmarking results](https://docs.openvino.ai/nightly/openvino_docs_performance_benchmarks.html)
+* [Benchmarking results](https://docs.openvino.ai/2024/openvino_docs_performance_benchmarks.html)
 
 * [Speed and Scale AI Inference Operations Across Multiple Architectures](https://techdecoded.intel.io/essentials/speed-and-scale-ai-inference-operations-across-multiple-architectures/?elq_cid=3646480_ts1607680426276&erpm_id=6470692_ts1607680426276) - webinar recording
 
diff --git a/ci/Dockerfile.coverity b/ci/Dockerfile.coverity
index ef3d5f5d..dfa512d5 100644
--- a/ci/Dockerfile.coverity
+++ b/ci/Dockerfile.coverity
@@ -27,7 +27,10 @@ RUN apt-get remove --yes cmake && apt-get update && apt-get install -y rapidjson
 WORKDIR /ovms/
 RUN /cov/bin/cov-configure --gcc --config coverity_config.xml && \
     /cov/bin/cov-configure --comptype gcc --compiler /usr/bin/gcc && \
-    /cov/bin/cov-build --dir cov-int bash -c 'bazel shutdown; bazel clean; bazel build --spawn_strategy=standalone //src:static_analysis && cmake /client/cpp/kserve-api && make --jobs=$(nproc)'
+    /cov/bin/cov-build --dir cov-int bash -c 'bazel shutdown; bazel clean; bazel build --spawn_strategy=standalone //src:static_analysis && cmake /client/cpp/kserve-api && make --jobs=$(nproc) && \
+    cd /ovms/src/custom_nodes/tokenizer && \
+    mkdir -p build && cd build && \
+    cmake .. && make --jobs=$(nproc)'
 #    cd /example_cpp_client/cpp && \
 #    bazel build --spawn_strategy=standalone //src:all'
 
diff --git a/client/go/kserve-api/Dockerfile b/client/go/kserve-api/Dockerfile
index b51779c2..3b5200a3 100644
--- a/client/go/kserve-api/Dockerfile
+++ b/client/go/kserve-api/Dockerfile
@@ -26,7 +26,7 @@ RUN go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28
 RUN go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2
 
 # Compile API
-RUN wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/src/kfserving_api/grpc_predict_v2.proto
+RUN wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/src/kfserving_api/grpc_predict_v2.proto
 RUN echo 'option go_package = "./grpc-client";' >> grpc_predict_v2.proto
 RUN protoc --go_out="./" --go-grpc_out="./" ./grpc_predict_v2.proto
 
diff --git a/client/java/kserve-api/pom.xml b/client/java/kserve-api/pom.xml
index 7d8ea120..1f271d5d 100644
--- a/client/java/kserve-api/pom.xml
+++ b/client/java/kserve-api/pom.xml
@@ -64,7 +64,7 @@
             </goals>
             <configuration>
               <url>
-                https://raw.githubusercontent.com/openvinotoolkit/model_server/main/src/kfserving_api/grpc_predict_v2.proto</url>
+                https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/src/kfserving_api/grpc_predict_v2.proto</url>
               <outputFileName>grpc_predict_v2.proto</outputFileName>
               <outputDirectory>src/main/proto</outputDirectory>
             </configuration>
diff --git a/client/python/ovmsclient/lib/README.md b/client/python/ovmsclient/lib/README.md
index 444aefd8..9c650455 100644
--- a/client/python/ovmsclient/lib/README.md
+++ b/client/python/ovmsclient/lib/README.md
@@ -6,7 +6,7 @@ OVMS client library contains only the necessary dependencies, so the whole packa
 
 As OpenVINO Model Server API is compatible with TensorFlow Serving, it's possible to use `ovmsclient` with TensorFlow Serving instances on: Predict, GetModelMetadata and GetModelStatus endpoints.
 
-See [API documentation](https://github.com/openvinotoolkit/model_server/blob/main/client/python/ovmsclient/lib/docs/README.md) for details on what the library provides.
+See [API documentation](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/ovmsclient/lib/docs/README.md) for details on what the library provides.
 
 ```bash
 git clone https://github.com/openvinotoolkit/model_server.git
@@ -136,4 +136,4 @@ results = client.predict(inputs=inputs, model_name="model")
 #
 ```
 
-For more details on `ovmsclient` see [API reference](https://github.com/openvinotoolkit/model_server/blob/main/client/python/ovmsclient/lib/docs/README.md)
\ No newline at end of file
+For more details on `ovmsclient` see [API reference](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/ovmsclient/lib/docs/README.md)
\ No newline at end of file
diff --git a/client/python/ovmsclient/lib/docs/pypi_overview.md b/client/python/ovmsclient/lib/docs/pypi_overview.md
index f78e65ce..fcb85360 100644
--- a/client/python/ovmsclient/lib/docs/pypi_overview.md
+++ b/client/python/ovmsclient/lib/docs/pypi_overview.md
@@ -9,7 +9,7 @@ The `ovmsclient` package works both with OpenVINO&trade; Model Server and Tensor
 The `ovmsclient` can replace `tensorflow-serving-api` package with reduced footprint and simplified interface.
 
 
-See [API reference](https://github.com/openvinotoolkit/model_server/blob/main/client/python/ovmsclient/lib/docs/README.md) for usage details.
+See [API reference](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/ovmsclient/lib/docs/README.md) for usage details.
 
 
 ## Usage example
@@ -38,4 +38,4 @@ results = client.predict(inputs=inputs, model_name="model")
 
 ```
 
-Learn more on `ovmsclient` [documentation site](https://github.com/openvinotoolkit/model_server/tree/main/client/python/ovmsclient/lib).
\ No newline at end of file
+Learn more on `ovmsclient` [documentation site](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/ovmsclient/lib).
\ No newline at end of file
diff --git a/create_package.sh b/create_package.sh
index debbce6b..99e68f5b 100755
--- a/create_package.sh
+++ b/create_package.sh
@@ -22,6 +22,7 @@ mkdir -vp /ovms_release/lib
 mkdir -vp /ovms_release/lib/hddl/config
 mkdir -vp /ovms_release/lib/custom_nodes
 
+cp /ovms/src/custom_nodes/tokenizer/build/src/lib*tokenizer.so /ovms_release/lib/custom_nodes/
 if [ -f /openvino_tokenizers/build/src/libopenvino_tokenizers.so ]; then cp -v /openvino_tokenizers/build/src/libopenvino_tokenizers.so /ovms_release/lib/ ; fi
 if [ -f /openvino_tokenizers/build/src/libcore_tokenizers.so ]; then cp -v /openvino_tokenizers/build/src/libcore_tokenizers.so /ovms_release/lib/ ; fi
 
diff --git a/demos/README.md b/demos/README.md
index 80e3293a..ba4776d2 100644
--- a/demos/README.md
+++ b/demos/README.md
@@ -56,23 +56,23 @@ Check out the list below to see complete step-by-step examples of using OpenVINO
 |[CLIP image classification](python_demos/clip_image_classification/README.md) | Classify image according to provided labels using CLIP model embedded in a multi-node MediaPipe graph.|
 |[Seq2seq translation](python_demos/seq2seq_translation/README.md) | Translate text using seq2seq model via gRPC API.|
 |[Age gender recognition](age_gender_recognition/python/README.md) | Run prediction on a JPEG image using age gender recognition model via gRPC API.|
-|[Horizontal Text Detection in Real-Time](horizontal_text_detection/python/README.md) | Run prediction on camera stream using a horizontal text detection model via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [horizontal_ocr custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/horizontal_ocr) and [demultiplexer](../docs/demultiplexing.md). |
-|[Optical Character Recognition Pipeline](optical_character_recognition/python/README.md) | Run prediction on a JPEG image using a pipeline of text recognition and text detection models with a custom node for intermediate results processing via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [east_ocr custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/east_ocr) and [demultiplexer](../docs/demultiplexing.md). |
+|[Horizontal Text Detection in Real-Time](horizontal_text_detection/python/README.md) | Run prediction on camera stream using a horizontal text detection model via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [horizontal_ocr custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/horizontal_ocr) and [demultiplexer](../docs/demultiplexing.md). |
+|[Optical Character Recognition Pipeline](optical_character_recognition/python/README.md) | Run prediction on a JPEG image using a pipeline of text recognition and text detection models with a custom node for intermediate results processing via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [east_ocr custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/east_ocr) and [demultiplexer](../docs/demultiplexing.md). |
 |[Face Detection](face_detection/python/README.md)|Run prediction on a JPEG image using face detection model via gRPC API.|
 |[Single Face Analysis Pipeline](single_face_analysis_pipeline/python/README.md)|Run prediction on a JPEG image using a simple pipeline of age-gender recognition and emotion recognition models via gRPC API to analyze image with a single face. This demo uses [pipeline](../docs/dag_scheduler.md) |
-|[Multi Faces Analysis Pipeline](multi_faces_analysis_pipeline/python/README.md)|Run prediction on a JPEG image using a pipeline of age-gender recognition and emotion recognition models via gRPC API to extract multiple faces from the image and analyze all of them. This demo uses [pipeline](../docs/dag_scheduler.md) with [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/model_zoo_intel_object_detection) and [demultiplexer](../docs/demultiplexing.md) |
+|[Multi Faces Analysis Pipeline](multi_faces_analysis_pipeline/python/README.md)|Run prediction on a JPEG image using a pipeline of age-gender recognition and emotion recognition models via gRPC API to extract multiple faces from the image and analyze all of them. This demo uses [pipeline](../docs/dag_scheduler.md) with [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/model_zoo_intel_object_detection) and [demultiplexer](../docs/demultiplexing.md) |
 |[Model Ensemble Pipeline](model_ensemble/python/README.md)|Combine multiple image classification models into one [pipeline](../docs/dag_scheduler.md) and aggregate results to improve classification accuracy. |
 |[Image Classification](image_classification/python/README.md)|Run prediction on a JPEG image using image classification model via gRPC API.|
-|[Using ONNX Model](using_onnx_model/python/README.md)|Run prediction on a JPEG image using image classification ONNX model via gRPC API in two preprocessing variants. This demo uses [pipeline](../docs/dag_scheduler.md) with [image_transformation custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/image_transformation). |
+|[Using ONNX Model](using_onnx_model/python/README.md)|Run prediction on a JPEG image using image classification ONNX model via gRPC API in two preprocessing variants. This demo uses [pipeline](../docs/dag_scheduler.md) with [image_transformation custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/image_transformation). |
 |[Using TensorFlow Model](image_classification_using_tf_model/python/README.md)|Run image classification using directly imported TensorFlow model. |
 |[Person, Vehicle, Bike Detection](person_vehicle_bike_detection/python/README.md)|Run prediction on a video file or camera stream using person, vehicle, bike detection model via gRPC API.|
-|[Vehicle Analysis Pipeline](vehicle_analysis_pipeline/python/README.md)|Detect vehicles and recognize their attributes using a pipeline of vehicle detection and vehicle attributes recognition models with a custom node for intermediate results processing via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/model_zoo_intel_object_detection). |
+|[Vehicle Analysis Pipeline](vehicle_analysis_pipeline/python/README.md)|Detect vehicles and recognize their attributes using a pipeline of vehicle detection and vehicle attributes recognition models with a custom node for intermediate results processing via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/model_zoo_intel_object_detection). |
 |[Real Time Stream Analysis](real_time_stream_analysis/python/README.md)| Analyze RTSP video stream in real time with generic application template for custom pre and post processing routines as well as simple results visualizer for displaying predictions in the browser. |
 |[Segmentation with PaddlePaddle](segmentation_using_paddlepaddle_model/python/README.md)| Perform segmentation on an image with a PaddlePaddle model. |
 |[Natural Language Processing with BERT](bert_question_answering/python/README.md)|Provide a knowledge source and a query and use BERT model for question answering use case via gRPC API. This demo uses dynamic shape feature. |
 |[Using inputs data in string format with universal-sentence-encoder model](universal-sentence-encoder/README.md)| Handling AI model with text as the model input. | 
 |[Benchmark App](benchmark/python/README.md)|Generate traffic and measure performance of the model served in OpenVINO Model Server.|
-|[Face Blur Pipeline](face_blur/python/README.md)|Detect faces and blur image using a pipeline of object detection models with a custom node for intermediate results processing via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [face_blur custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/face_blur). |
+|[Face Blur Pipeline](face_blur/python/README.md)|Detect faces and blur image using a pipeline of object detection models with a custom node for intermediate results processing via gRPC API. This demo uses [pipeline](../docs/dag_scheduler.md) with [face_blur custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/face_blur). |
 
 ## With C++ Client
 | Demo | Description |
diff --git a/demos/age_gender_recognition/python/README.md b/demos/age_gender_recognition/python/README.md
index cac20d80..20049d78 100644
--- a/demos/age_gender_recognition/python/README.md
+++ b/demos/age_gender_recognition/python/README.md
@@ -35,7 +35,7 @@ Install python dependencies:
 ```bash
 pip3 install -r requirements.txt
 ```
-Run [age_gender_recognition.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/age_gender_recognition/python/age_gender_recognition.py) script to make an inference:
+Run [age_gender_recognition.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/age_gender_recognition/python/age_gender_recognition.py) script to make an inference:
 ```bash
 python3 age_gender_recognition.py --image_input_path age-gender-recognition-retail-0001.jpg --rest_port 8000
 ```
diff --git a/demos/benchmark/cpp/README.md b/demos/benchmark/cpp/README.md
index bc133878..3fd3177a 100644
--- a/demos/benchmark/cpp/README.md
+++ b/demos/benchmark/cpp/README.md
@@ -10,7 +10,7 @@ make
 ```
 The application can be used with any model or pipeline served in OVMS, by requesting `GetModelMetadata` endpoint and using such information to prepare synthetic inputs with matching shape and precision.
 
-> **Note**: In this directory you can only see the code specific to the benchmark client. The code shared with other C++ demos as well as all building utilities are placed in the [common C++ directory](https://github.com/openvinotoolkit/model_server/tree/main/demos/common/cpp).
+> **Note**: In this directory you can only see the code specific to the benchmark client. The code shared with other C++ demos as well as all building utilities are placed in the [common C++ directory](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/common/cpp).
 
 > **Note**: It is required that endpoint does not use dynamic shape.
 
diff --git a/demos/benchmark/python/README.md b/demos/benchmark/python/README.md
index f0764e64..ea231848 100644
--- a/demos/benchmark/python/README.md
+++ b/demos/benchmark/python/README.md
@@ -77,7 +77,7 @@ usage: main.py [-h] [-i ID] [-c CONCURRENCY] [-a SERVER_ADDRESS]
                [--max_value MAX_VALUE] [--min_value MIN_VALUE] [--xrand XRAND]
                [--dump_png] [--step_timeout STEP_TIMEOUT]
                [--metadata_timeout METADATA_TIMEOUT] [-Y DB_ENDPOINT]
-               [-y [DB_METADATA ...]] [--print_all] [-ps PRINT_SUMMARY] [--print_time]
+               [-y [DB_METADATA ...]] [--print_all] [--print_time]
                [--report_warmup] [--certs_dir CERTS_DIR] [-q STATEFUL_LENGTH]
                [--stateful_id STATEFUL_ID] [--stateful_hop STATEFUL_HOP]
                [--sync_interval SYNC_INTERVAL]
@@ -326,38 +326,6 @@ NO_PROXY=localhost no_proxy=localhost python3 /ovms_benchmark_client/main.py -a
 ...
 ```
 
-## Summarize benchmarking results
-
-Summary of the benchmark results can be viewed with command option ```-ps```.
-```
-docker run --network host benchmark_client -a localhost -r 8000 -m face-detection-retail-0005 -p 9000 -s 2 3 300 300 -t 20 -u 2 -w 10 -ps
-```
-
-Sample output log with results summary:
-
-```
-Client 2.7
-NO_PROXY=localhost no_proxy=localhost python3 /ovms_benchmark_client/main.py -a localhost -r 8000 -m face-detection-retail-0005 -p 9000 -s 2 3 300 300 -t 20 -u 2 -w 10 -ps
-          XI worker: start workload...
-
-### Benchmark Parameters ###
- Model: face-detection-retail-0005
- Input shape: ['2', '3', '300', '300']
- Request concurrency: 1
- Test Duration (s): Total (t): 20.00 | Warmup (u): 2.00 | Window (w): 10.00
-
-### Benchmark Summary ###
- ## General Metrics ##
- Duration(s): Total: 20.01 | Window: 10.01
- Batches: Total: 1781 | Window: 891
-
- ## Latency Metrics (ms) ##
- Mean: 11.20 | stdev: 0.74 | p50: 12.78 | p90: 15.26 | p95: 15.56
-
- ## Throughput Metrics (fps) ##
- Frame Rate (FPS): Brutto: 89.01 | Netto: 89.24
- Batch Rate (batches/s): Brutto: 89.01 | Netto: 89.24
-```
 ## MediaPipe benchmarking
 
 Start OVMS container with `config.json` including mediapipe servable. OVMS should be built with MediaPipe enabled.
@@ -377,4 +345,4 @@ docker run -v ${PWD}/workspace:/workspace --network host benchmark_client -a loc
 ```
 
 Many other client options together with benchmarking examples are presented in
-[an additional PDF document](https://github.com/openvinotoolkit/model_server/blob/main/docs/python-benchmarking-client-16feb.pdf). 
+[an additional PDF document](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/python-benchmarking-client-16feb.pdf). 
\ No newline at end of file
diff --git a/demos/benchmark/python/main.py b/demos/benchmark/python/main.py
index 5de9ade3..90ef57b3 100644
--- a/demos/benchmark/python/main.py
+++ b/demos/benchmark/python/main.py
@@ -262,8 +262,6 @@ if __name__ == "__main__":
                         help="database metadata configuration. default: None")
     parser.add_argument("--print_all", required=False, action="store_true",
                         help="flag to print all output")
-    parser.add_argument("-ps", "--print_summary", required=False, action="store_true",
-                        help="flag to print output summary")
     parser.add_argument("--print_time", required=False, action="store_true",
                         help="flag to print datetime next to each output line")
     parser.add_argument("--report_warmup", required=False, action="store_true",
@@ -335,79 +333,7 @@ if __name__ == "__main__":
     if xargs["json"]:
         jout = json.dumps(common_results)
         print(f"{BaseClient.json_prefix}###{worker_id}###STATISTICS###{jout}")
-
     if xargs["print_all"]:
         for key, value in common_results.items():
             sys.stdout.write(f"{worker_id}: {key}: {value}\n")
-
-    if xargs["print_summary"]:
-        sys.stdout.write("\n### Benchmark Parameters ###\n")
-        if xargs['model_name'] is not None:
-            model_name = xargs['model_name']
-            sys.stdout.write(f" Model: {model_name}\n")
-        if xargs['shape']:
-            inp_shape = xargs['shape']
-            sys.stdout.write(f" Input shape: {inp_shape}\n")
-        if 'submetrics' in common_results:
-            total_clients = common_results["submetrics"]
-        else:
-            total_clients = 1
-
-        sys.stdout.write(f" Request concurrency: {total_clients}\n")
-        if xargs['duration']:
-            total_t = float(xargs['duration'])
-            sys.stdout.write(f" Test Duration (s): Total (t): {total_t:.2f}")
-        if xargs['warmup']:
-            warm_up = float(xargs['warmup'])
-            sys.stdout.write(f" | Warmup (u): {warm_up:.2f}")
-        if xargs['window']:
-            window = float(xargs['window'])
-            sys.stdout.write(f" | Window (w): {window:.2f}\n")
-
-
-        sys.stdout.write("\n### Benchmark Summary ###\n")
-        sys.stdout.write(" ## General Metrics ##\n")
-
-        sys.stdout.write(f" Duration(s): Total: {common_results['total_duration']:.2f}")
-        sys.stdout.write(f" | Window: {common_results['window_total_duration']:.2f}\n")
-
-        sys.stdout.write(f" Batches: Total: {common_results['total_batches']}")
-        sys.stdout.write(f" | Window: {common_results['window_total_batches']}\n")
-
-        if total_clients:
-            sys.stdout.write("\n ## Latency Metrics (ms) ##\n")
-            sys.stdout.write(f" Mean: {common_results['window_mean_latency']*1000:.2f}")
-            sys.stdout.write(f" | stdev: {common_results['window_stdev_latency']*1000:.2f}")
-
-            base, factor = float(xargs["hist_base"]), float(xargs["hist_factor"])
-            xargs["quantile_list"] = [0.5, 0.9, 0.95]
-
-            common_results.recalculate_quantiles("window_", base, factor, xargs["quantile_list"])
-
-            for idx, v in enumerate(xargs["quantile_list"]):
-                # Convert string to float
-                try:
-                    quantile_value = float(v)
-                except ValueError:
-                    # case where the string cannot be converted to a float
-                    sys.stdout.write(f"Invalid quantile value: {v}")
-                    continue
-                # float to percentage
-                q = str(int(quantile_value * 100))
-                p = str("p") + q
-                qv = str("qos_latency_") + str(idx)
-
-                sys.stdout.write(f" | {p}: {common_results[qv]*1000:.2f}")
-            sys.stdout.write("\n")
-            sys.stdout.write("\n ## Throughput Metrics (fps) ##\n")
-
-            # Brutto: Total number of frames processed or produced per second by a system or application.
-            # It doesn't take into account any overhead or inefficiencies in the system
-            # Netto: Effective or net frame rate, which is the frame rate adjusted for any overhead,
-            # delays, or processing inefficiencies in the system
-            sys.stdout.write(f" Frame Rate (FPS): Brutto: {common_results['brutto_frame_rate']:.2f}")
-            sys.stdout.write(f" | Netto: {common_results['netto_frame_rate']:.2f} \n")
-            sys.stdout.write(f" Batch Rate (batches/s): Brutto: {common_results['brutto_batch_rate']:.2f}")
-            sys.stdout.write(f" | Netto: {common_results['netto_batch_rate']:.2f}\n")
-
     sys.exit(return_code)
diff --git a/demos/bert_question_answering/python/README.md b/demos/bert_question_answering/python/README.md
index 64cd0d56..fa4b3b69 100644
--- a/demos/bert_question_answering/python/README.md
+++ b/demos/bert_question_answering/python/README.md
@@ -4,7 +4,7 @@
 
 This document demonstrates how to run inference requests for [BERT model](https://github.com/openvinotoolkit/open_model_zoo/tree/2022.1.0/models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002) with OpenVINO Model Server. It provides questions answering functionality.
 
-In this example docker container with [bert-client image](https://github.com/openvinotoolkit/model_server/blob/main/demos/bert_question_answering/python/Dockerfile) runs the script [bert_question_answering.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/bert_question_answering/python/bert_question_answering.py). It runs inference request for each paragraph on a given page in order to answer the provided question. Since each paragraph can have different size the functionality of dynamic shape is used.
+In this example docker container with [bert-client image](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/bert_question_answering/python/Dockerfile) runs the script [bert_question_answering.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/bert_question_answering/python/bert_question_answering.py). It runs inference request for each paragraph on a given page in order to answer the provided question. Since each paragraph can have different size the functionality of dynamic shape is used.
 
 NOTE: With `min_request_token_num` parameter you can specify the minimum size of the request. If the paragraph has too short, it is concatenated with the next one until it has required length. When there is no paragraphs left to concatenate request is created with the remaining content.
 
diff --git a/demos/common/cpp/Dockerfile b/demos/common/cpp/Dockerfile
index 792d0334..47027822 100644
--- a/demos/common/cpp/Dockerfile
+++ b/demos/common/cpp/Dockerfile
@@ -78,9 +78,9 @@ RUN mkdir /bazel && \
     rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh
 
 WORKDIR /input
-RUN wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/image_classification/input_images.txt && \
+RUN wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/image_classification/input_images.txt && \
     mkdir images && \
-    for I in `cat input_images.txt  | cut -d"/" -f6 | cut -d" " -f1` ; do curl  https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/$I -o images/$I --create-dirs; done
+    for I in `cat input_images.txt  | cut -d"/" -f6 | cut -d" " -f1` ; do curl  https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/static/images/$I -o images/$I --create-dirs; done
 
 WORKDIR /build
 COPY .bazelrc WORKSPACE /build/
diff --git a/demos/face_blur/python/README.md b/demos/face_blur/python/README.md
index 10d73bae..da754fc1 100644
--- a/demos/face_blur/python/README.md
+++ b/demos/face_blur/python/README.md
@@ -1,6 +1,6 @@
 # Face Blur Pipeline Demo with OVMS {#ovms_demo_face_blur_pipeline}
 
-This document demonstrates how to create pipelines using object detection models from OpenVINO Model Zoo in order to blur the image. As an example, we will use [face-detection-retail-0004](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/face-detection-retail-0004/README.md) to detect multiple faces on the image. Then, for each detected face we will blur it using [face_blur](https://github.com/openvinotoolkit/model_server/blob/main/src/custom_nodes/face_blur) example custom node.
+This document demonstrates how to create pipelines using object detection models from OpenVINO Model Zoo in order to blur the image. As an example, we will use [face-detection-retail-0004](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/face-detection-retail-0004/README.md) to detect multiple faces on the image. Then, for each detected face we will blur it using [face_blur](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/custom_nodes/face_blur) example custom node.
 
 ## Pipeline Configuration Graph
 
@@ -10,7 +10,7 @@ Below is depicted graph implementing face blur pipeline execution.
 
 It include the following Nodes:
 - Model `face-detection-retail-0004` - deep learning model which takes user image as input. Its output contain information about faces coordinates and confidence levels.
-- Custom node `face_blur` - it includes C++ implementation of image blurring. By analysing the output it produces image blurred in spots detected by object detection model based on the configurable score level threshold. Custom node also resizes it to the target resolution. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [face_blur custom node](https://github.com/openvinotoolkit/model_server/blob/main/src/custom_nodes/face_blur).
+- Custom node `face_blur` - it includes C++ implementation of image blurring. By analysing the output it produces image blurred in spots detected by object detection model based on the configurable score level threshold. Custom node also resizes it to the target resolution. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [face_blur custom node](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/custom_nodes/face_blur).
 - Response - image blurred in spots detected by object detection model.
 
 ## Prepare workspace to run the demo
diff --git a/demos/face_detection/python/README.md b/demos/face_detection/python/README.md
index 39cd9c1d..747daf47 100644
--- a/demos/face_detection/python/README.md
+++ b/demos/face_detection/python/README.md
@@ -2,7 +2,7 @@
 
 ## Overview
 
-The script [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/face_detection/python/face_detection.py) runs face detection inference requests for all the images
+The script [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/face_detection/python/face_detection.py) runs face detection inference requests for all the images
 saved in `input_images_dir` directory. 
 
 The script can adjust the input image size and change the batch size in the request. It demonstrates how to use
diff --git a/demos/image_classification/cpp/README.md b/demos/image_classification/cpp/README.md
index bf100c2e..3cee9d24 100644
--- a/demos/image_classification/cpp/README.md
+++ b/demos/image_classification/cpp/README.md
@@ -12,7 +12,7 @@ cd model_server/demos/image_classification/cpp
 make
 ```
 
->**Note**: In this directory you can only see the code specific to the benchmark client. The code shared with other C++ demos as well as all building utilities are placed in the [common C++ directory](https://github.com/openvinotoolkit/model_server/tree/main/demos/common/cpp).
+>**Note**: In this directory you can only see the code specific to the benchmark client. The code shared with other C++ demos as well as all building utilities are placed in the [common C++ directory](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/common/cpp).
 
 ## Prepare classification model
 
diff --git a/demos/image_classification/python/README.md b/demos/image_classification/python/README.md
index 58aed4ea..28cea1fb 100644
--- a/demos/image_classification/python/README.md
+++ b/demos/image_classification/python/README.md
@@ -2,7 +2,7 @@
 
 ## Overview
 
-The script [image_classification.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/image_classification/python/image_classification.py) reads all images and their labels specified in the text file. It then classifies them with [ResNet50](https://docs.openvino.ai/2023.1/omz_models_model_resnet50_binary_0001.html) model and presents accuracy results.
+The script [image_classification.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/image_classification/python/image_classification.py) reads all images and their labels specified in the text file. It then classifies them with [ResNet50](https://docs.openvino.ai/2023.1/omz_models_model_resnet50_binary_0001.html) model and presents accuracy results.
 
 
 ## Download ResNet50 model
diff --git a/demos/image_classification_with_string_output/README.md b/demos/image_classification_with_string_output/README.md
index 15b9b0e4..4cb94514 100644
--- a/demos/image_classification_with_string_output/README.md
+++ b/demos/image_classification_with_string_output/README.md
@@ -8,6 +8,8 @@ The script below is downloading a public MobileNet model trained on the ImageNet
 This is a very handy functionality because it allows us to export the model with the included pre/post processing functions as the model layers. The client just receives the string data with the label name for the classified image.
 
 ```bash
+git clone https://github.com/openvinotoolkit/model_server.git
+cd model_server/demos/image_classification_with_string_output
 pip install -r requirements.txt
 python3 download_model.py
 rm model/1/fingerprint.pb
@@ -31,7 +33,7 @@ docker run -d -u $(id -u):$(id -g) -v $(pwd):/workspace -p 8000:8000 openvino/mo
 ## Send request
 Use example client to send requests containing images via KServ REST API:
 ```bash
-python3 image_classification_with_string_output.py 
+python3 image_classification_with_string_output.py --http_port 8000
 ```
 Request may be sent also using other APIs (KServ GRPC, TFS). In this sections you can find short code samples how to do this:
 - [TensorFlow Serving API](../../docs/clients_tfs.md)
diff --git a/demos/mediapipe/holistic_tracking/README.md b/demos/mediapipe/holistic_tracking/README.md
index 2b67aa55..43a5a1dd 100644
--- a/demos/mediapipe/holistic_tracking/README.md
+++ b/demos/mediapipe/holistic_tracking/README.md
@@ -79,5 +79,5 @@ Results saved to :image_0.jpg
 
 ## Real time stream analysis
 
-For demo featuring real time stream application see [real_time_stream_analysis](https://github.com/openvinotoolkit/model_server/tree/main/demos/real_time_stream_analysis/python)
+For demo featuring real time stream application see [real_time_stream_analysis](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/real_time_stream_analysis/python)
 
diff --git a/demos/mediapipe/iris_tracking/README.md b/demos/mediapipe/iris_tracking/README.md
index 98f43096..38b92fcb 100644
--- a/demos/mediapipe/iris_tracking/README.md
+++ b/demos/mediapipe/iris_tracking/README.md
@@ -32,7 +32,7 @@ docker run -d -v $PWD/mediapipe:/mediapipe -v $PWD/ovms:/models -p 9000:9000 ope
 ```bash
 pip install -r requirements.txt
 # download a sample image for analysis
-wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/people/people2.jpeg
+wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/static/images/people/people2.jpeg
 echo "people2.jpeg" > input_images.txt
 # launch the client
 python mediapipe_iris_tracking.py --grpc_port 9000 --images_list input_images.txt
diff --git a/demos/mediapipe/multi_model_graph/README.md b/demos/mediapipe/multi_model_graph/README.md
index 835a1ded..9b2dcbfe 100644
--- a/demos/mediapipe/multi_model_graph/README.md
+++ b/demos/mediapipe/multi_model_graph/README.md
@@ -20,7 +20,7 @@ cp -r ../../../src/test/dummy ./dummyAdd/
 ```
 
 ## Run OpenVINO Model Server
-Prepare virtualenv according to [kserve samples readme](https://github.com/openvinotoolkit/model_server/blob/main/client/python/kserve-api/samples/README.md)
+Prepare virtualenv according to [kserve samples readme](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/kserve-api/samples/README.md)
 ```bash
 docker run -d -v $PWD:/mediapipe -p 9000:9000 openvino/model_server:latest --config_path /mediapipe/config.json --port 9000
 ```
diff --git a/demos/mediapipe/object_detection/README.md b/demos/mediapipe/object_detection/README.md
index 9e75a2eb..7ec5b826 100644
--- a/demos/mediapipe/object_detection/README.md
+++ b/demos/mediapipe/object_detection/README.md
@@ -45,4 +45,4 @@ Received images with bounding boxes will be located in ./results directory.
 
 ## Real time stream analysis
 
-For demo featuring real time stream application see [real_time_stream_analysis](https://github.com/openvinotoolkit/model_server/tree/main/demos/real_time_stream_analysis/python)
+For demo featuring real time stream application see [real_time_stream_analysis](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/real_time_stream_analysis/python)
diff --git a/demos/model_ensemble/python/README.md b/demos/model_ensemble/python/README.md
index 8235d1a3..a18f2304 100644
--- a/demos/model_ensemble/python/README.md
+++ b/demos/model_ensemble/python/README.md
@@ -24,7 +24,7 @@ make
 The steps in `Makefile` are:
 
 1. Download and use the models from [open model zoo](https://github.com/openvinotoolkit/open_model_zoo).
-2. Use [python script](https://github.com/openvinotoolkit/model_server/blob/main/tests/models/argmax_sum.py) located in this repository. Since it uses tensorflow to create models in _saved model_ format, hence tensorflow pip package is required.
+2. Use [python script](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/tests/models/argmax_sum.py) located in this repository. Since it uses tensorflow to create models in _saved model_ format, hence tensorflow pip package is required.
 3. Prepare argmax model with `(1, 1001)` input shapes to match output of the googlenet and resnet output shapes. The generated model will sum inputs and calculate the index with the highest value. The model output will indicate the most likely predicted class from the ImageNet* dataset.
 4. Convert models to IR format and [prepare models repository](../../../docs/models_repository.md).
 
@@ -54,7 +54,7 @@ models
 ## Step 2: Define required models and pipeline
 Pipelines need to be defined in the configuration file to use them. The same configuration file is used to define served models and served pipelines.
 
-Use the [config.json located here](https://github.com/openvinotoolkit/model_server/blob/main/demos/model_ensemble/python/config.json), the content is as follows:
+Use the [config.json located here](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/model_ensemble/python/config.json), the content is as follows:
 ```bash
 cat config.json
 {
diff --git a/demos/multi_faces_analysis_pipeline/python/README.md b/demos/multi_faces_analysis_pipeline/python/README.md
index 3cbcdf63..9bc7ef68 100644
--- a/demos/multi_faces_analysis_pipeline/python/README.md
+++ b/demos/multi_faces_analysis_pipeline/python/README.md
@@ -1,7 +1,7 @@
 # Multi Faces Analysis Pipeline Demo {#ovms_demo_multi_faces_analysis_pipeline}
 
 
-This document demonstrates how to create complex pipelines using object detection and object recognition models from OpenVINO Model Zoo. As an example, we will use [face-detection-retail-0004](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/face-detection-retail-0004/README.md) to detect multiple faces on the image. Then, for each detected face we will crop it using [model_zoo_intel_object_detection](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/model_zoo_intel_object_detection) example custom node. Finally, each image face image will be forwarded to [age-gender-recognition-retail-0013](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/age-gender-recognition-retail-0013/README.md) and [emotion-recognition-retail-0003](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/emotions-recognition-retail-0003/README.md) models.
+This document demonstrates how to create complex pipelines using object detection and object recognition models from OpenVINO Model Zoo. As an example, we will use [face-detection-retail-0004](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/face-detection-retail-0004/README.md) to detect multiple faces on the image. Then, for each detected face we will crop it using [model_zoo_intel_object_detection](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/model_zoo_intel_object_detection) example custom node. Finally, each image face image will be forwarded to [age-gender-recognition-retail-0013](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/age-gender-recognition-retail-0013/README.md) and [emotion-recognition-retail-0003](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/emotions-recognition-retail-0003/README.md) models.
 
 ![Multi Faces Analysis Graph](multi_faces_analysis.png)
 
@@ -20,7 +20,7 @@ Below is depicted graph implementing faces analysis pipeline execution.
 It includes the following Nodes:
 - Model `face-detection` - deep learning model which takes user image as input. Its outputs contain information about face coordinates and confidence levels.
 - Custom node `model_zoo_intel_object_detection` - it includes C++ implementation of common object detection models results processing. By analysing the output it produces cropped face images based on the configurable score level threshold. Custom node also resizes them to the target resolution and combines into a single output of a dynamic batch size. The output batch size is determined by the number of detected
-boxes according to the configured criteria. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/model_zoo_intel_object_detection).
+boxes according to the configured criteria. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/model_zoo_intel_object_detection).
 - demultiplexer - outputs from the custom node model_zoo_intel_object_detection have variable batch size. In order to match it with the sequential recognition models, data is split into individual images with each batch size equal to 1.
 Such smaller requests can be submitted for inference in parallel to the next Model Nodes. Learn more about the [demultiplexing](../../../docs/demultiplexing.md).
 - Model `age-gender-recognition` - this model recognizes age and gender on given face image
@@ -111,7 +111,7 @@ docker run -p 9000:9000 -d -v ${PWD}/workspace:/workspace openvino/model_server
 
 ## Requesting the Service
 
-Exemplary client [multi_faces_analysis_pipeline.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/multi_faces_analysis_pipeline/python/multi_faces_analysis_pipeline.py) can be used to request pipeline deployed in previous step.
+Exemplary client [multi_faces_analysis_pipeline.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/multi_faces_analysis_pipeline/python/multi_faces_analysis_pipeline.py) can be used to request pipeline deployed in previous step.
 
 ```bash
 pip3 install -r requirements.txt
diff --git a/demos/optical_character_recognition/python/README.md b/demos/optical_character_recognition/python/README.md
index f3d7795b..8f435ded 100644
--- a/demos/optical_character_recognition/python/README.md
+++ b/demos/optical_character_recognition/python/README.md
@@ -18,7 +18,7 @@ It includes the following nodes:
 - Custom node east_ocr - it includes C++ implementation of east-resnet50 model results processing. It analyses the detected boxes coordinates, filters the results
 based on the configurable score level threshold and and applies non-max suppression algorithm to remove overlapping boxes. Finally the custom node east-ocr crops all detected boxes
 from the original image, resize them to the target resolution and combines into a single output of a dynamic batch size. The output batch size is determined by the number of detected
-boxes according to the configured criteria. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [east_ocr custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/east_ocr)
+boxes according to the configured criteria. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [east_ocr custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/east_ocr)
 - demultiplexer - output from the Custom node east_ocr have variable batch size. In order to match it with the sequential text detection model, the data is split into individual images with batch size 1 each.
 Such smaller requests can be submitted for inference in parallel to the next Model Node. Learn more about the [demultiplexing](../../../docs/demultiplexing.md)
 - Model text-recognition - this model recognizes characters included in the input image.
@@ -103,11 +103,11 @@ text-recognition model will have the following interface:
 
 ## Building the Custom Node "east_ocr" Library
 
-Custom nodes are loaded into OVMS as dynamic library implementing OVMS API from [custom_node_interface.h](https://github.com/openvinotoolkit/model_server/blob/main/src/custom_node_interface.h).
+Custom nodes are loaded into OVMS as dynamic library implementing OVMS API from [custom_node_interface.h](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/custom_node_interface.h).
 It can use OpenCV libraries included in OVMS or it could use other third party components.
 
 The custom node east_ocr can be built inside a docker container via the following procedure:
-- go to the directory with custom node examples [src/custom_node](https://github.com/openvinotoolkit/model_server/blob/main/src/custom_nodes)
+- go to the directory with custom node examples [src/custom_node](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/custom_nodes)
 - run `make` command:
 
 ```bash
@@ -131,7 +131,7 @@ cp -R EAST/IR/1 OCR/east_fp32/1
 
 ## OVMS Configuration File
 
-The configuration file for running the OCR demo is stored in [config.json](https://github.com/openvinotoolkit/model_server/blob/main/demos/optical_character_recognition/python/config.json)
+The configuration file for running the OCR demo is stored in [config.json](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/optical_character_recognition/python/config.json)
 Copy this file along with the model files and the custom node library like presented below:
 ```bash
 cp model_server/demos/optical_character_recognition/python/config.json OCR
diff --git a/demos/python_demos/clip_image_classification/README.md b/demos/python_demos/clip_image_classification/README.md
index 9ca8992d..e5832481 100644
--- a/demos/python_demos/clip_image_classification/README.md
+++ b/demos/python_demos/clip_image_classification/README.md
@@ -1,6 +1,6 @@
 # CLIP image classification {#ovms_demo_clip_image_classification}
 
-Image classification demo using multi-modal CLIP model for inference and [Python code](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html) for pre and postprocessing.
+Image classification demo using multi-modal CLIP model for inference and [Python code](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html) for pre and postprocessing.
 The client sends request with an image and input labels to the graph and receives the label with the highest probability. The preprocessing python node is executed first and prepares inputs vector based on user inputs from the request. Then inputs are used to get similarity matrix from inference on the CLIP model. After that postprocessing python node is executed and extracts the label with highest score among the input labels and sends it back to the user.
 
 Demo is based on this [CLIP notebook](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/228-clip-zero-shot-image-classification/228-clip-zero-shot-classification.ipynb)
diff --git a/demos/python_demos/llm_text_generation/README.md b/demos/python_demos/llm_text_generation/README.md
index dfdb141b..c6e19ff9 100644
--- a/demos/python_demos/llm_text_generation/README.md
+++ b/demos/python_demos/llm_text_generation/README.md
@@ -177,6 +177,37 @@ Time per generated token  20.0 ms
 Total time 6822 ms
 ```
 
+### Use KServe REST API with curl
+
+Run OVMS :
+```bash
+docker run -d --rm -p 8000:8000 -v ${PWD}/servable_unary:/workspace -v ${PWD}/${SELECTED_MODEL}:/model \
+-e SELECTED_MODEL=${SELECTED_MODEL} openvino/model_server:py --config_path /workspace/config.json --rest_port 8000
+```
+
+Send request using curl:
+```bash
+curl --header "Content-Type: application/json" --data '{"inputs":[{"name" : "pre_prompt", "shape" : [1], "datatype" : "BYTES", "data" : ["What is the theory of relativity?"]}]}' localhost:8000/v2/models/python_model/infer
+```
+
+Example output:
+```bash
+{
+    "model_name": "python_model",
+    "outputs": [{
+            "name": "token_count",
+            "shape": [1],
+            "datatype": "INT32",
+            "data": [249]
+        }, {
+            "name": "completion",
+            "shape": [1],
+            "datatype": "BYTES",
+            "data": ["The theory of relativity is a long-standing field of physics which states that the behavior of matter and energy in relation to space and time is influenced by the principles of special theory of relativity and general theory of relativity. It proposes that gravity is a purely mathematical construct (as opposed to a physical reality), which affects distant masses on superluminal speeds just as they would alter objects on Earth moving at light speed. According to the theory, space and time are more fluid than we perceive them to be, with phenomena like lensing causing distortions that cannot be explained through more traditional laws of physics. Since its introduction in 1905, it has revolutionized the way we understand the world and has shed fresh light on important concepts in modern scientific thought, such as causality, time dilation, and the nature of space-time. The theory was proposed by Albert Einstein in an article published in the British journal 'Philosophical Transactions of the Royal Society A' in 1915, although his findings were first formulated in his 1907 book 'Einstein: Photography & Poetry,' where he introduced the concept of equivalence principle."]
+        }]
+}
+```
+
 ## Run a client with gRPC streaming
 
 ### Deploy OpenVINO Model Server with the Python Calculator
diff --git a/demos/python_demos/llm_text_generation/client_stream.py b/demos/python_demos/llm_text_generation/client_stream.py
index 0bbe00a4..ff0b3150 100644
--- a/demos/python_demos/llm_text_generation/client_stream.py
+++ b/demos/python_demos/llm_text_generation/client_stream.py
@@ -61,16 +61,11 @@ def callback(result, error):
     elif result.as_numpy('token_count') is not None:
         token_count[0] = result.as_numpy('token_count')[0]
     elif result.as_numpy('completion') is not None:
-        if len(prompts) == 1:
-            # For single batch, partial response is represented as single buffer of bytes
-            print(result.as_numpy('completion').tobytes().decode(), flush=True, end='')
-        else:
-            # For multi batch, responses are packed in 4byte len tritonclient format
-            os.system('clear')
-            for i, completion in enumerate(deserialize_bytes_tensor(result._result.raw_output_contents[0])):
-                completions[i] += completion.decode()
-                print(completions[i])
-                print()
+        os.system('cls' if os.name=='nt' else 'clear')
+        for i, completion in enumerate(deserialize_bytes_tensor(result._result.raw_output_contents[0])):
+            completions[i] += completion.decode()
+            print(completions[i])
+            print()
         duration = int((endtime - start_time).total_seconds() * 1000)
         processing_times = np.append(processing_times, duration)
         start_time = datetime.datetime.now()
diff --git a/demos/python_demos/llm_text_generation/client_unary.py b/demos/python_demos/llm_text_generation/client_unary.py
index 46365b1e..33c20e0e 100644
--- a/demos/python_demos/llm_text_generation/client_unary.py
+++ b/demos/python_demos/llm_text_generation/client_unary.py
@@ -39,14 +39,11 @@ infer_input = serialize_prompts(args['prompt'])
 start_time = datetime.datetime.now()
 results = client.infer("python_model", [infer_input], client_timeout=10*60)  # 10 minutes
 endtime = datetime.datetime.now()
-if len(args['prompt']) == 1:
-    print(f"Question:\n{args['prompt'][0]}\n\nCompletion:\n{results.as_numpy('completion').tobytes().decode()}\n")
-else:
-    for i, arr in enumerate(deserialize_bytes_tensor(results.as_numpy("completion"))):
-        if i < len(args['prompt']):
-            print(f"==== Prompt: {args['prompt'][i]} ====")
-            print(arr.decode())
-        print()
+for i, arr in enumerate(results.as_numpy("completion")):
+    if i < len(args['prompt']):
+        print(f"==== Prompt: {args['prompt'][i]} ====")
+        print(arr.decode())
+    print()
 print("Number of tokens ", results.as_numpy("token_count")[0])
 print("Generated tokens per second ", round(results.as_numpy("token_count")[0] / int((endtime - start_time).total_seconds()), 2))
 print("Time per generated token ", round(int((endtime - start_time).total_seconds()) / results.as_numpy("token_count")[0] * 1000, 2), "ms")
diff --git a/demos/python_demos/llm_text_generation/servable_stream/model.py b/demos/python_demos/llm_text_generation/servable_stream/model.py
index 3e5516e1..2682b6e5 100755
--- a/demos/python_demos/llm_text_generation/servable_stream/model.py
+++ b/demos/python_demos/llm_text_generation/servable_stream/model.py
@@ -122,17 +122,13 @@ def convert_history_to_text(history):
 
 
 def deserialize_prompts(batch_size, input_tensor):
-    if batch_size == 1:
-        return [bytes(input_tensor).decode()]
     np_arr = deserialize_bytes_tensor(bytes(input_tensor))
     return [arr.decode() for arr in np_arr]
 
 
 def serialize_completions(batch_size, result):
-    if batch_size == 1:
-        return [Tensor("completion", result.encode())]
     return [Tensor("completion", serialize_byte_tensor(
-        np.array(result, dtype=np.object_)).item())]
+        np.array(result, dtype=np.object_)).item(), shape=[batch_size], datatype="BYTES")]
 
 
 class OvmsPythonModel:
diff --git a/demos/python_demos/llm_text_generation/servable_unary/python_model/model.py b/demos/python_demos/llm_text_generation/servable_unary/python_model/model.py
index a1c31598..b4139f88 100755
--- a/demos/python_demos/llm_text_generation/servable_unary/python_model/model.py
+++ b/demos/python_demos/llm_text_generation/servable_unary/python_model/model.py
@@ -114,18 +114,13 @@ def convert_history_to_text(history):
 
 
 def deserialize_prompts(batch_size, input_tensor):
-    if batch_size == 1:
-        return [bytes(input_tensor).decode()]
     np_arr = deserialize_bytes_tensor(bytes(input_tensor))
     return [arr.decode() for arr in np_arr]
 
 
 def serialize_completions(batch_size, result, token_count):
-    if batch_size == 1:
-        return [Tensor("completion", result[0].encode()), Tensor("token_count", np.array(token_count, dtype=np.int32))]
     return [Tensor("completion", serialize_byte_tensor(
-        np.array(result, dtype=np.object_)).item()), Tensor("token_count", np.array(token_count, dtype=np.int32))]
-
+        np.array(result, dtype=np.object_)).item(), shape=[batch_size], datatype="BYTES"), Tensor("token_count", np.array(token_count, dtype=np.int32))]
 
 class OvmsPythonModel:
     def initialize(self, kwargs: dict):
diff --git a/demos/python_demos/llm_text_generation/utils.py b/demos/python_demos/llm_text_generation/utils.py
index 701df39b..fc2d44b1 100644
--- a/demos/python_demos/llm_text_generation/utils.py
+++ b/demos/python_demos/llm_text_generation/utils.py
@@ -20,11 +20,6 @@ import numpy as np
 
 def serialize_prompts(prompts):
     infer_input = grpcclient.InferInput("pre_prompt", [len(prompts)], "BYTES")
-    if len(prompts) == 1:
-        # Single batch serialized directly as bytes
-        infer_input._raw_content = prompts[0].encode()
-        return infer_input
-    # Multi batch serialized in tritonclient 4byte len format
     infer_input._raw_content = serialize_byte_tensor(
         np.array(prompts, dtype=np.object_)).item()
     return infer_input
diff --git a/demos/python_demos/rag_chatbot/.gitignore b/demos/python_demos/rag_chatbot/.gitignore
index c37e746e..b8871eed 100644
--- a/demos/python_demos/rag_chatbot/.gitignore
+++ b/demos/python_demos/rag_chatbot/.gitignore
@@ -6,3 +6,14 @@ neural-chat*
 notus-7b*
 .venv*
 all-mpnet*
+
+
+**/__pycache__
+
+
+servable_stream/files/frames/
+servable_stream/files/frame_metadata/
+servable_stream/files/frame_emb_storage/
+servable_stream/files/text_emb_storage/
+servable_stream/vdms-visual-rag.ipynb
+servable_stream/test.py
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/Makefile b/demos/python_demos/rag_chatbot/Makefile
new file mode 100644
index 00000000..eaec082b
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/Makefile
@@ -0,0 +1,8 @@
+run:
+	docker run --rm --env https_proxy=http://proxy-iind.intel.com:912 \
+		--env http_proxy=http://proxy-iind.intel.com:911 \
+		--env no_proxy=10.190.0.0/16 \
+		-ti --network=host -v ${PWD}/servable_stream:/workspace -v ${PWD}/${SELECTED_MODEL}:/llm_model \
+		-v ${PWD}/all-mpnet-base-v2:/embed_model -v ${PWD}/documents:/documents -e SELECTED_MODEL=${SELECTED_MODEL} \
+		--entrypoint /bin/bash \
+		openvino/model_server1:py
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/README.md b/demos/python_demos/rag_chatbot/README.md
index c72234b8..ebeda906 100644
--- a/demos/python_demos/rag_chatbot/README.md
+++ b/demos/python_demos/rag_chatbot/README.md
@@ -26,52 +26,18 @@ Building the image with all required python dependencies is required. Follow the
 ```bash
 git clone https://github.com/openvinotoolkit/model_server.git
 cd model_server
-make python_image BASE_OS=redhat OVMS_CPP_DOCKER_IMAGE=registry.connect.redhat.com/intel/openvino-model-server OVMS_CPP_IMAGE_TAG=2024.1
+make python_image BASE_OS=redhat OVMS_CPP_DOCKER_IMAGE=registry.connect.redhat.com/intel/openvino-model-server OVMS_CPP_IMAGE_TAG=2024.0
 ```
 It will create an image called `registry.connect.redhat.com/intel/openvino-model-server:py`
 
 You can also build Ubuntu 22.04 image:
 ```
-make python_image BASE_OS=ubuntu OVMS_CPP_DOCKER_IMAGE=openvino/model_server OVMS_CPP_IMAGE_TAG=2024.1
+make python_image BASE_OS=ubuntu OVMS_CPP_DOCKER_IMAGE=openvino/model_server OVMS_CPP_IMAGE_TAG=2024.0
 ```
 It will create an image called `openvino/model_server:py`
 
-## OpenVINO Model Server deployment with online models pulling from Hugging Face Hub
 
-In this demo, OpenVINO Model Server has an option to pull the required models from Hugging Face Hub.
-It's a simple deployment option because it doesn't require models preparation which have to be attached to the container. Just the demo scripts and configation files are required in the container at startup.
-
-What needs to be prepared is a list of documents, which should give context of RAG analysis. It is provided in a format of text file containing URLs of the documents sources:
-
-```bash
-echo "https://gist.githubusercontent.com/ryanloney/42b8ebe29f95ebd4382ee0b2bb50bea2/raw/cfbb679fefb6babec675c7806254a5fff29a5e6b/aipc.txt" > demos/python_demos/rag_chatbot/servable_stream/docs.txt
-```
-Now the model server can be started:
-```bash
-docker run -d -v ${PWD}/demos/python_demos/rag_chatbot/servable_stream:/config -p 9000:9000 -p 8000:8000 \
--e SELECTED_MODEL=${SELECTED_MODEL} \
-registry.connect.redhat.com/intel/openvino-model-server:py --config_path /config/config.json --port 9000 --rest_port 8000
-```
-> **NOTE** The model loading time for the sequential attempts can be improved by mounting the Hugging Faces cache in the docker command:
-`-v ${HOME}/.cache:/hf-cache -e HF_HOME=/hf-cache/huggingface`.
-> **NOTE** For model llama-2-chat-7b, which require authorization in Hugging Faces hub, it might be needed to pass also HF_TOKEN environment variable.
-`-e HF_TOKEN=<token>`.
-
-
-Wait for the models to be loaded. It can be verified in the container logs or using the REST API:
-```bash
-curl -i http://localhost:8000/v2/models/python_model/ready
-HTTP/1.1 200 OK
-Content-Type: application/json
-Date: Wed, 17 Apr 2024 12:27:27 GMT
-Content-Length: 0
-```
-
-## OpenVINO Model Server deployment with locally attached models
-Models can be downloaded, compressed and prepared in advance for the container. It makes the initialization faster and can be deployed in an offline environment.
-
-
-### Download LLM model
+## Download LLM model
 
 Download the model using `download_model.py` script:
 
@@ -93,11 +59,11 @@ options:
 python download_model.py --model ${SELECTED_MODEL}
 
 ```
-The model will appear in `../servable_stream/tiny-llama-1b-chat` directory.
+The model will appear in `./tiny-llama-1b-chat` directory.
 
 > **NOTE** Llama model requires license agreement, therefore it is required agree and log in to HuggingFace account via `huggingface-cli` before loading the model. [Read more](https://huggingface.co/docs/huggingface_hub/guides/cli) how to.
 
-### Weight Compression - optional
+## Weight Compression - optional
 
 [Weight Compression](https://docs.openvino.ai/canonical/weight_compression.html) may be applied on the original model. Applying 8-bit or 4-bit weight compression reduces the model size and memory requirements while speeding up execution by running calculations on lower precision layers.
 
@@ -121,11 +87,11 @@ Running this script will create new directories with compressed versions of the
 The compressed models can be used in place of the original as they have compatible inputs and outputs.
 
 ```bash
-du -sh ./servable_stream/tiny*
-4.2G    ./servable_stream/tiny-llama-1b-chat
-2.1G    ./servable_stream/tiny-llama-1b-chat_FP16
-702M    ./servable_stream/tiny-llama-1b-chat_INT4_compressed_weights
-1.1G    ./servable_stream/tiny-llama-1b-chat_INT8_compressed_weights
+du -sh tiny*
+4.2G    tiny-llama-1b-chat
+2.1G    tiny-llama-1b-chat_FP16
+702M    tiny-llama-1b-chat_INT4_compressed_weights
+1.1G    tiny-llama-1b-chat_INT8_compressed_weights
 ```
 
 > **NOTE** Applying quantization to model weights may impact the model accuracy. Please test and verify that the results are of acceptable quality for your use case.
@@ -133,51 +99,63 @@ du -sh ./servable_stream/tiny*
 > **NOTE** On target devices that natively support FP16 precision (i.e. GPU), OpenVINO automatically adjusts the precision from FP32 to FP16. This improves the performance and typically does not impact accuracy. Original precision can be enforced with `ov_config` key:
 `{"INFERENCE_PRECISION_HINT": "f32"}`.
 
-### Download embedding model
+## Download embedding model
 
 Download the model using `download_embedding_model.py` script:
 
 ```bash
+python download_embedding_model.py --help
+usage: download_embedding_model.py [-h] --model {all-mpnet-base-v2}
+
+Script to download LLM model based on https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/254-llm-chatbot
+
+options:
+  -h, --help            show this help message and exit
+  --model {all-mpnet-base-v2}
+                        Select the LLM model out of supported list
+
 python download_embedding_model.py --model all-mpnet-base-v2
 
 ```
-The model will appear in `./servable_stream/all-mpnet-base-v2` directory.
+The model will appear in `./all-mpnet-base-v2` directory.
 
+## Prepare documents for knowledge base
+We will use single `aipc.txt` file stored in `documents/` directory:
+```bash
+tree documents
+documents
+â””â”€â”€ aipc.txt
 
+0 directories, 1 file
+```
 
-### Deploy OpenVINO Model Server
+## Deploy OpenVINO Model Server with Python Calculator
 
+Mount the `./documents` directory with knowledge base.
+Mount the `./llm_model` directory with the LLM model.  
+Mount the `./embed_model` directory with the document embedding model.  
 Mount the servable directory which contains:
 - python scripts which are required for execution and use [Hugging Face](https://huggingface.co/) utilities with [optimum-intel](https://github.com/huggingface/optimum-intel) acceleration.
 - `config.json` - which defines which servables should be loaded
 - `graph.pbtxt` - which defines MediaPipe graph containing python node
-- LLM and embedding models
 
 ```bash
-docker run -d --rm -p 9000:9000 -p 8000:8000 -v ${PWD}/servable_stream:/workspace \
--e SELECTED_MODEL=${SELECTED_MODEL} -e LLM_MODEL_DIR=${SELECTED_MODEL} \
-registry.connect.redhat.com/intel/openvino-model-server:py --config_path /workspace/config.json --port 9000 --rest_port 8000
+docker run -d --rm -p 9000:9000 -v ${PWD}/servable_stream:/workspace -v ${PWD}/${SELECTED_MODEL}:/llm_model \
+-v ${PWD}/all-mpnet-base-v2:/embed_model -v ${PWD}/documents:/documents -e SELECTED_MODEL=${SELECTED_MODEL} \
+registry.connect.redhat.com/intel/openvino-model-server:py --config_path /workspace/config.json --port 9000
 ```
 
 You may deploy the compressed model(s) by simply changing the model path mounted to the container. For example, to deploy the 8-bit weight compressed model:
 ```bash
-docker run -d --rm -p 9000:9000 -p 8000:8000 -v ${PWD}/servable_stream:/workspace \
--e SELECTED_MODEL=${SELECTED_MODEL} -e LLM_MODEL_DIR=${SELECTED_MODEL}_INT8_compressed_weights \
-registry.connect.redhat.com/intel/openvino-model-server:py --config_path /workspace/config.json --port 9000 --rest_port 8000
+docker run -d --rm -p 9000:9000 -v ${PWD}/servable_stream:/workspace -v ${PWD}/${SELECTED_MODEL}_INT8_compressed_weights:/llm_model \
+-v ${PWD}/all-mpnet-base-v2:/embed_model -v ${PWD}/documents:/documents -e SELECTED_MODEL=${SELECTED_MODEL} \
+registry.connect.redhat.com/intel/openvino-model-server:py --config_path /workspace/config.json --port 9000
 ```
 > **NOTE** Check the Docker container logs to confirm that the model is loaded before sending requests from a client. Depending on the model and hardware it might take a few seconds or several minutes.
 
 > **Note** If order to run the inference load on Intel GPU instead of CPU, just pass the extra parameters to the docker run `--device /dev/dri --group-add=$(stat -c "%g" /dev/dri/render*)`.
-
-```
-docker run -d --rm -p 9000:9000 -p 8000:8000 -v ${PWD}/servable_stream:/workspace \
---device /dev/dri --group-add=$(stat -c "%g" /dev/dri/render*) \
--e SELECTED_MODEL=${SELECTED_MODEL} -e LLM_MODEL_DIR=${SELECTED_MODEL}_INT8_compressed_weights -e DEVICE=gpu \
-registry.connect.redhat.com/intel/openvino-model-server:py --config_path /workspace/config.json --port 9000 --rest_port 8000
-```
 It will pass the GPU device to the container and set the correct group security context.
 
-
 ## Run gradio client with gRPC streaming
 
 Install gradio and dependencies:
@@ -187,26 +165,8 @@ pip install -r client_requirements.txt
 Start the gradio web server:
 ```bash
 python3 app.py --web_url localhost:9001 --ovms_url localhost:9000
-
-
 ```
 
 Visit the website localhost:9001
 
 ![result](result.png)
-
-## Changing the scope of documents for analysis
-
-Started container is not only responding to the queries related to initial documents. It is also monitoring the file with the list of documents.
-Thanks to that, it is possible to change the scope of documents without restarting the container or even reloading the model. All that is needed is updating docs.txt with the list of URLs pointing to the documents.
-
-```bash
-echo https://gist.githubusercontent.com/dtrawins/2956a7a77aa6732b52b8ae6eab0be205/raw/e05f2ab8fea9c8631ac5f20b8dd640074ae429c7/genai.txt > servable_stream/docs.txt
-```
-
-When a file modification is detected, documents will be downloaded and indexed from scratch. 
-
-After few moments, new queries can be sent with the new context.
-
-![result2](results2.png)
-
diff --git a/demos/python_demos/rag_chatbot/app.py b/demos/python_demos/rag_chatbot/app.py
index 8509b899..7762ae50 100644
--- a/demos/python_demos/rag_chatbot/app.py
+++ b/demos/python_demos/rag_chatbot/app.py
@@ -16,9 +16,9 @@
 
 import gradio as gr
 import argparse
-
 from streamer import OvmsStreamer
-
+import os
+from servable_stream.utils import config_reader as reader
 
 parser = argparse.ArgumentParser(description='Gradio frontend launcher')
 
@@ -30,14 +30,61 @@ parser.add_argument('--ovms_url',
                     help='OVMS server URL')
 args = parser.parse_args()
 
+config = reader.read_config('servable_stream/config.yaml')
+
+list_of_videos = os.listdir('servable_stream/' + config['videos'])
+
+#video_to_play = 'documents/videos_all/op_1_0320241821.mp4'
+video_to_play = None
 
 def callback(message, history):
+    global video_to_play
     streamer = OvmsStreamer(args.ovms_url.split(':')[0], int(args.ovms_url.split(':')[1]))
     streamer.request_async(message)
     result = ""
+    videofile = ""
+    compflag = False
+    last_word = None
     for completion in streamer:
-        print(completion, end='', flush=True)
         result += completion
+        last_word = completion
         yield result
+        
+    videofile = last_word
+    video_to_play = 'servable_stream/' + config['videos'] + videofile
+    print (f'video path {video_to_play}')
+    print(result, flush=True)
+    print(videofile, flush=True)
+
+def vcallback(video):
+    print(video_to_play)
+    return video_to_play
+
+def update_video(input_video):
+    print("In update_video", input_video)
+    return gr.Video(value=video_to_play)
+
+css = """
+.container {
+    height: 100vh;
+}
+"""
+with gr.Blocks(css=css) as demo:
+    with gr.Row():        
+        with gr.Column(scale=1, min_width=200):
+            ChatBlock = gr.ChatInterface(callback, retry_btn=None, undo_btn=None, \
+                examples = [
+                    'People reading product description',
+                    'Picking an item from shelf',
+                    'Man holding red shopping cart',
+                    'Man in blue shirt shopping',
+                ]) 
+        with gr.Column(scale=1, min_width=200):
+            VidBlock = gr.Video(vcallback(video_to_play), height="50vh", interactive=False, scale=4, autoplay=True, show_download_button=False, show_share_button=False)
+            update = gr.Button('Play')
+        
+            update.click(update_video, inputs=None, outputs=[VidBlock])
+        #VidBlock = gr.Interface(fn=vcallback, allow_flagging="never", inputs=None, outputs=gr.Video(None, height="50vh", interactive=False, scale=4, autoplay=True, show_download_button=False, show_share_button=False))
+ 
 
-gr.ChatInterface(callback, concurrency_limit=16).queue().launch(server_name=args.web_url.split(':')[0], server_port=int(args.web_url.split(':')[1]))
+demo.launch(server_name=args.web_url.split(':')[0], server_port=int(args.web_url.split(':')[1]))
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/compress_model.py b/demos/python_demos/rag_chatbot/compress_model.py
index 83cb990f..0e4fbb46 100644
--- a/demos/python_demos/rag_chatbot/compress_model.py
+++ b/demos/python_demos/rag_chatbot/compress_model.py
@@ -43,15 +43,15 @@ SELECTED_MODEL = args['model']
 LANGUAGE = 'English'
 
 model_configuration = SUPPORTED_LLM_MODELS[LANGUAGE][SELECTED_MODEL]
-MODEL_PATH = "./servable_stream/" + SELECTED_MODEL
+MODEL_PATH = "./" + SELECTED_MODEL
 
 print(model_configuration)
 pt_model_id = model_configuration["model_id"]
 pt_model_name = SELECTED_MODEL.split("-")[0]
 model_type = AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True).model_type
-fp16_model_dir = Path("./servable_stream/" + SELECTED_MODEL + "_FP16")
-int8_model_dir = Path("./servable_stream/" + SELECTED_MODEL + "_INT8_compressed_weights")
-int4_model_dir = Path("./servable_stream/" + SELECTED_MODEL + "_INT4_compressed_weights")
+fp16_model_dir = Path(SELECTED_MODEL + "_FP16")
+int8_model_dir = Path(SELECTED_MODEL + "_INT8_compressed_weights")
+int4_model_dir = Path(SELECTED_MODEL + "_INT4_compressed_weights")
 
 def save_tokenizer(model_id, PATH):
     print(f'Downloading tokenizer to {PATH} ...')
diff --git a/demos/python_demos/rag_chatbot/download_embedding_model.py b/demos/python_demos/rag_chatbot/download_embedding_model.py
index 379910a3..648734fb 100644
--- a/demos/python_demos/rag_chatbot/download_embedding_model.py
+++ b/demos/python_demos/rag_chatbot/download_embedding_model.py
@@ -41,9 +41,9 @@ SELECTED_MODEL = args.model
 
 model_configuration = SUPPORTED_EMBEDDING_MODELS[SELECTED_MODEL]
 
-MODEL_PATH = "./servable_stream/" + SELECTED_MODEL
+MODEL_PATH = "./" + SELECTED_MODEL
 
-embedding_model_dir = Path("./servable_stream/" + SELECTED_MODEL)
+embedding_model_dir = Path(SELECTED_MODEL)
 embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[SELECTED_MODEL]
 
 if not (embedding_model_dir / "openvino_model.xml").exists():
diff --git a/demos/python_demos/rag_chatbot/download_model.py b/demos/python_demos/rag_chatbot/download_model.py
index 04677d11..b10bc07c 100755
--- a/demos/python_demos/rag_chatbot/download_model.py
+++ b/demos/python_demos/rag_chatbot/download_model.py
@@ -36,7 +36,7 @@ model_configuration = SUPPORTED_LLM_MODELS[LANGUAGE][SELECTED_MODEL]
 
 model_id = model_configuration["model_id"]
 
-MODEL_PATH = "./servable_stream/" + SELECTED_MODEL
+MODEL_PATH = "./" + SELECTED_MODEL
 
 print('Downloading and converting...')
 ov_model = OVModelForCausalLM.from_pretrained(
diff --git a/demos/python_demos/rag_chatbot/results2.png b/demos/python_demos/rag_chatbot/results2.png
deleted file mode 100644
index 4b85d339..00000000
Binary files a/demos/python_demos/rag_chatbot/results2.png and /dev/null differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/Makefile b/demos/python_demos/rag_chatbot/servable_stream/Makefile
new file mode 100644
index 00000000..573c8e96
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/Makefile
@@ -0,0 +1,22 @@
+install:
+	pip1 instll -r requirements.txt
+
+generate-image-embeddings:	
+	python3 generate_image_embeddings.py
+
+generate-text-embeddings:
+	python3 generate_text_embeddings.py
+
+run-image-retrieval-ui:
+	streamlit run image_retrieval_ui.py --server.address 0.0.0.0 --server.port 50055
+
+run-visual-rag:
+	streamlit run visual-rag-ui.py --server.address 0.0.0.0 --server.port 50055
+
+run:
+	docker run -d -p 8000:8000 chromadb/chroma
+	pip3 install -r requirements.txt
+	python3 generate_store_embeddings.py
+	streamlit run image_retrieval_ui.py --server.address 0.0.0.0 --server.port 50022 &
+	streamlit run visual-rag-ui.py --server.address 0.0.0.0 --server.port 50055 &
+	echo 'Started image ui at port 50022 and visual rag ui at port 50055'
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/README.md b/demos/python_demos/rag_chatbot/servable_stream/README.md
new file mode 100644
index 00000000..8593d1cd
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/README.md
@@ -0,0 +1,88 @@
+# Visual RAG
+
+## Introduction
+Visual RAG is a framework that retrives video based on provided user prompt. It uses both video scene description generated by open source vision models (ex video-llama, video-llava etc.) as text embeddings and frames as image embeddings to perform vector similarity search. The provided solution also supports feature to retrieve more similar videos without prompting it. (see the example video below)
+
+![Example Video](docs/visual-rag-demo.gif)
+
+## Tools
+
+- **UI**: streamlit
+- **Vector Storage**: Chroma DB **or** Intel's VDMS
+- **Image Embeddings**: CLIP
+- **Text Embeddings**: all-MiniLM-L12-v2
+- **RAG Retriever**: Langchain Ensemble Retrieval
+
+## Prerequisites
+
+There are 10 example videos present in ```files/videos``` along with their description generated by open-source vision model.
+If you want these visual RAG to work on your own videos, make sure it matches below format.
+
+## File Structure
+
+```bash
+files/ 
+.
+â”œâ”€â”€ scene_description
+â”‚   â”œâ”€â”€ op_10_0320241830.mp4.txt
+â”‚   â”œâ”€â”€ op_1_0320241830.mp4.txt
+â”‚   â”œâ”€â”€ op_19_0320241830.mp4.txt
+â”‚   â”œâ”€â”€ op_21_0320241830.mp4.txt
+â”‚   â”œâ”€â”€ op_24_0320241830.mp4.txt
+â”‚   â”œâ”€â”€ op_31_0320241830.mp4.txt
+â”‚   â”œâ”€â”€ op_47_0320241830.mp4.txt
+â”‚   â”œâ”€â”€ op_5_0320241915.mp4.txt
+â”‚   â”œâ”€â”€ op_DSCF2862_Rendered_001.mp4.txt
+â”‚   â””â”€â”€ op_DSCF2864_Rendered_006.mp4.txt
+â””â”€â”€ videos
+    â”œâ”€â”€ op_10_0320241830.mp4
+    â”œâ”€â”€ op_1_0320241830.mp4
+    â”œâ”€â”€ op_19_0320241830.mp4
+    â”œâ”€â”€ op_21_0320241830.mp4
+    â”œâ”€â”€ op_24_0320241830.mp4
+    â”œâ”€â”€ op_31_0320241830.mp4
+    â”œâ”€â”€ op_47_0320241830.mp4
+    â”œâ”€â”€ op_5_0320241915.mp4
+    â”œâ”€â”€ op_DSCF2862_Rendered_001.mp4
+    â””â”€â”€ op_DSCF2864_Rendered_006.mp4
+```
+
+## Setup and Installation
+
+Install pip requirements
+
+```bash
+pip3 install -r requirements.txt
+```
+
+The current framework supports both Chroma DB and Intel's VDMS, use either of them,
+
+Running Chroma DB as docker container
+```bash
+docker run -d -p 8000:8000 chromadb/chroma
+```
+**or**
+
+Running VDMS DB as docker container
+```bash
+docker run -d -p 55555:55555 intellabs/vdms:latest
+```
+
+**Note:** If you are not using file structure similar to what is described above, consider changing it in ```config.yaml```.
+
+Update your choice of db and port in ```config.yaml```.
+
+Generating Text & Image embeddigns and store them into selected db
+```bash
+python3 generate_store_embeddings.py
+```
+
+**Web UI1 Clip Retriever**
+```bash
+streamlit run image_retrieval_ui.py --server.address 0.0.0.0 --server.port 50022
+```
+
+**Web UI2 Visual RAG**
+```bash
+streamlit run visual-rag-ui.py --server.address 0.0.0.0 --server.port 50055
+```
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/config.json b/demos/python_demos/rag_chatbot/servable_stream/config.json
index 378b8f94..b3dab09e 100755
--- a/demos/python_demos/rag_chatbot/servable_stream/config.json
+++ b/demos/python_demos/rag_chatbot/servable_stream/config.json
@@ -3,7 +3,7 @@
     "mediapipe_config_list": [
         {
             "name":"python_model",
-            "base_path": "."
+            "graph_path":"/workspace/graph.pbtxt"
         }
     ]
 }
diff --git a/demos/python_demos/rag_chatbot/servable_stream/config.py b/demos/python_demos/rag_chatbot/servable_stream/config.py
index 71adab76..4dc4dc18 100644
--- a/demos/python_demos/rag_chatbot/servable_stream/config.py
+++ b/demos/python_demos/rag_chatbot/servable_stream/config.py
@@ -339,3 +339,73 @@ SUPPORTED_EMBEDDING_MODELS = {
     #     "do_norm": False,
     # },
 }
+
+# Example from https://discuss.huggingface.co/t/textiteratorstreamer-compatibility-with-batch-processing/46763/2
+from transformers import (
+    StoppingCriteria,
+    StoppingCriteriaList,
+    TextIteratorStreamer
+)
+
+
+import torch
+from typing import Optional, List
+import typing
+
+
+# Example from https://discuss.huggingface.co/t/textiteratorstreamer-compatibility-with-batch-processing/46763/2
+class BatchTextIteratorStreamer(TextIteratorStreamer):
+    def __init__(self, batch_size:int, tokenizer: "AutoTokenizer", skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs):
+        super().__init__(tokenizer, skip_prompt, timeout, **decode_kwargs)
+        self.batch_size = batch_size
+        self.token_cache = [[] for _ in range(batch_size)]
+        self.print_len = [0 for _ in range(batch_size)]
+        self.generate_exception = None
+
+    def put(self, value):
+        if len(value.shape) != 2:
+            value = torch.reshape(value, (self.batch_size, value.shape[0] // self.batch_size))
+
+        if self.skip_prompt and self.next_tokens_are_prompt:
+            self.next_tokens_are_prompt = False
+            return
+
+        printable_texts = list()
+        for idx in range(self.batch_size):
+            self.token_cache[idx].extend(value[idx].tolist())
+            text = self.tokenizer.decode(self.token_cache[idx], **self.decode_kwargs)
+
+            if text.endswith("\n"):
+                printable_text = text[self.print_len[idx] :]
+                self.token_cache[idx] = []
+                self.print_len[idx] = 0
+                # If the last token is a CJK character, we print the characters.
+            elif len(text) > 0 and self._is_chinese_char(ord(text[-1])):
+                printable_text = text[self.print_len[idx] :]
+                self.print_len[idx] += len(printable_text)
+            else:
+                printable_text = text[self.print_len[idx] : text.rfind(" ") + 1]
+                self.print_len[idx] += len(printable_text)
+            printable_texts.append(printable_text)
+
+        self.on_finalized_text(printable_texts)
+
+    def end(self):
+        printable_texts = list()
+        for idx in range(self.batch_size):
+            if len(self.token_cache[idx]) > 0:
+                text = self.tokenizer.decode(self.token_cache[idx], **self.decode_kwargs)
+                printable_text = text[self.print_len[idx] :]
+                self.token_cache[idx] = []
+                self.print_len[idx] = 0
+            else:
+                printable_text = ""
+            printable_texts.append(printable_text)
+
+        self.next_tokens_are_prompt = True
+        self.on_finalized_text(printable_texts, stream_end=True)
+
+    def on_finalized_text(self, texts: typing.List[str], stream_end: bool = False):
+        self.text_queue.put(texts, timeout=self.timeout)
+        if stream_end:
+            self.text_queue.put(self.stop_signal, timeout=self.timeout)
diff --git a/demos/python_demos/rag_chatbot/servable_stream/config.yaml b/demos/python_demos/rag_chatbot/servable_stream/config.yaml
new file mode 100755
index 00000000..599f771f
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/config.yaml
@@ -0,0 +1,23 @@
+# Path to all videos
+videos: files/videos/
+# Path to video description generated by open-source vision models (ex. video-llama, video-llava, etc.)
+description: files/scene_description/
+# Do you want to extract frames of videos (True if not done already, else False)
+generate_frames: True
+# Do you wnat to generate image embeddings?
+embed_frames: True
+# Path to store extracted frames
+image_output_dir: files/frames/
+# Path to store metadata files
+meta_output_dir: files/frame_metadata/
+# Number of frames to extract per second, 
+# if 24 fps, and this value is 2, then it will extract 12th and 24th frame
+number_of_frames_per_second: 2
+
+vector_db:
+  choice_of_db: 'chroma' # Supported databases [vdms, chroma]
+  host: 10.190.180.102
+  port: 8000
+
+# LLM path
+model_path: /llm_model/
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/docs/visual-rag-demo.gif b/demos/python_demos/rag_chatbot/servable_stream/docs/visual-rag-demo.gif
new file mode 100644
index 00000000..45bf7a46
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/docs/visual-rag-demo.gif differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/extract_store_frames.py b/demos/python_demos/rag_chatbot/servable_stream/extract_store_frames.py
new file mode 100644
index 00000000..b7d988fa
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/extract_store_frames.py
@@ -0,0 +1,82 @@
+import os
+import cv2
+import json
+
+def process_all_videos(path, image_output_dir, meta_output_dir, N):
+
+    def extract_frames(video_path, image_output_dir, meta_output_dir, N):
+        video = video_path.split('/')[-1]
+        # Create a directory to store frames and metadata
+        os.makedirs(image_output_dir, exist_ok=True)
+        os.makedirs(meta_output_dir, exist_ok=True)
+        
+        # Open the video file
+        cap = cv2.VideoCapture(video_path)
+
+        if int(cv2.__version__.split('.')[0]) < 3:
+            fps = cap.get(cv2.cv.CV_CAP_PROP_FPS)
+        else:
+            fps = cap.get(cv2.CAP_PROP_FPS)
+    
+        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
+        
+        #print (f'fps {fps}')
+        #print (f'total frames {total_frames}')
+        
+        mod = int(fps // N)
+        if mod == 0: mod = 1
+        
+        print (f'total frames {total_frames}, N {N}, mod {mod}')
+        
+        # Variables to track frame count and desired frames
+        frame_count = 0
+        
+        # Metadata dictionary to store timestamp and image paths
+        metadata = {}
+        
+        while cap.isOpened():
+            ret, frame = cap.read()
+            
+            if not ret:
+                break
+            
+            frame_count += 1
+            
+            if frame_count % mod == 0:
+                timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000  # Convert milliseconds to seconds
+                frame_path = os.path.join(image_output_dir, f"{video}_{frame_count}.jpg")
+                
+                cv2.imwrite(frame_path, frame)  # Save the frame as an image
+                
+                metadata[frame_count] = {"timestamp": timestamp, "frame_path": frame_path}
+        
+        # Save metadata to a JSON file
+        metadata_file = os.path.join(meta_output_dir, f"{video}_metadata.json")
+        with open(metadata_file, "w") as f:
+            json.dump(metadata, f, indent=4)
+        
+        # Release the video capture and close all windows
+        cap.release()
+        print(f"{frame_count/mod} Frames extracted and metadata saved successfully.") 
+        return fps, total_frames, metadata_file
+
+    videos = [file for file in os.listdir(path) if file.endswith('.mp4')]
+
+    # print (f'Total {len(videos)} videos will be processed')
+    metadata = {}
+
+    for i, each_video in enumerate(videos):
+        video_path = os.path.join(path, each_video)
+        fps, total_frames, metadata_file = extract_frames(video_path, image_output_dir, meta_output_dir, N)
+        metadata[each_video] = {
+                'fps': fps, 
+                'total_frames': total_frames, 
+                'extracted_frame_metadata_file': metadata_file,
+                'embedding_path': f'embeddings/{each_video}.pt',
+                'video_path': f'{path}/{each_video}',
+            }
+        print (f'âœ…  {i+1}/{len(videos)}')
+
+    metadata_file = os.path.join(meta_output_dir, f"metadata.json")
+    with open(metadata_file, "w") as f:
+        json.dump(metadata, f, indent=4)
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_10_0320241830.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_10_0320241830.mp4.txt
new file mode 100644
index 00000000..11d8d462
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_10_0320241830.mp4.txt
@@ -0,0 +1 @@
+  The video shows a man standing behind a hardware store aisle, looking at the items displayed on the shelves. He appears to be browsing through the products available for sale. As he walks down the aisle, another person can be seen walking towards him. The two men seem to be engaged in conversation as they pass each other by. The man behind the aisle is wearing a blue shirt, while the other man is wearing a plaid shirt. They both appear to be dressed casually, which suggests that they might be shopping for home improvement supplies. The man behind the aisle is holding a cell phone in his hand, possibly checking prices or searching for product reviews online. The other man does not seem to have anything in his hands, but he might be carrying a shopping basket or a list of items to purchase. Overall, the scene depicts a typical shopping experience in a hardware store, where customers can find various products to meet their needs. The colors and properties of the items in the store include different types of tools, paint, and building materials, all of which are essential for home improvement projects.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_19_0320241830.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_19_0320241830.mp4.txt
new file mode 100644
index 00000000..24e6e149
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_19_0320241830.mp4.txt
@@ -0,0 +1 @@
+  The video features two men in the hardware store aisle. One man is standing near the center of the aisle, looking at the items on display. He is wearing a blue plaid shirt and seems to be focused on finding something specific. The other man is walking past him, but he doesn't appear to be interested in anything in particular. As he walks by, he glances at some of the items on the shelves, but doesn't stop to examine them closely. The aisle is filled with various items, including tools, paint supplies, and cleaning products. There are several bottles of different sizes and shapes, which are likely used for painting or cleaning purposes. Additionally, there are multiple boxes containing tools and equipment, such as wrenches, hammers, and saws. These items are arranged neatly on the shelves, making it easy for customers to locate and select what they need. Overall, the scene captures a typical shopping experience in a hardware store, where customers can find everything they need for their home improvement projects.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_1_0320241830.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_1_0320241830.mp4.txt
new file mode 100644
index 00000000..9d3f8b5f
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_1_0320241830.mp4.txt
@@ -0,0 +1,2 @@
+  The main focus of the video is a man who is standing in an aisle at a hardware store, looking at items on the shelves. He is wearing a blue plaid shirt and grey pants, and he appears to be focused on finding a specific item or comparing different products. As he looks at the items, he occasionally reaches out to pick up or examine them more closely. The man seems to be taking his time to find the right item for his needs, and he appears to be engaged in his task.
+There are also several other people visible in the background, but they do not seem to be actively engaging with the items in the store. They appear to be browsing or walking around the store, possibly looking for other items or just passing through. The store itself has a variety of items available, including tools, building materials, and other household goods. The items are displayed on shelves and racks throughout the store, making it easy for customers to find what they need. The overall atmosphere of the store seems to be calm and orderly, with customers browsing and shopping at their own pace.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_21_0320241830.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_21_0320241830.mp4.txt
new file mode 100644
index 00000000..7efd7448
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_21_0320241830.mp4.txt
@@ -0,0 +1,3 @@
+  The video features two men shopping in the hardware store. One man is wearing a black shirt and is pushing a red cart filled with various items. He is holding a magazine in his hand and appears to be reading it while walking down the aisle. The other man is standing next to him, also looking at the items on display. Both men seem to be focused on finding something specific in the store.
+As they walk down the aisle, they come across several items such as a pair of scissors, a hammer, and a set of wrenches. The scissors are silver and black in color, while the hammer has a wooden handle and a metal head. The wrenches are made of metal and come in different sizes. The men appear to be interested in these items and may consider purchasing them for their projects.
+The overall atmosphere of the store seems to be calm and organized, with the men taking their time to explore the products available. The colors of the items they interact with are predominantly silver, black, and brown, which gives the store a classic and traditional feel.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_24_0320241830.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_24_0320241830.mp4.txt
new file mode 100644
index 00000000..35b1b2b4
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_24_0320241830.mp4.txt
@@ -0,0 +1 @@
+  The first person in the video is a man who walks down the aisle of the hardware store, looking at various items on display. He appears to be casually dressed, wearing a t-shirt and jeans. As he stops to look at something on the shelf, his attention is focused on the item, indicating that he might be interested in buying it. The second person in the video is a woman who stands near the end of the aisle, possibly waiting for someone or just browsing the store. She is also casually dressed, wearing a tank top and shorts. Her presence adds to the overall atmosphere of the store, making it feel more lively and social. Overall, both individuals appear to be engaged in their respective activities within the store, either browsing or shopping for specific items. The colors and textures of the items they interact with add to the visual appeal of the store, creating a visually rich environment that invites further exploration and discovery.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_31_0320241830.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_31_0320241830.mp4.txt
new file mode 100644
index 00000000..636899f5
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_31_0320241830.mp4.txt
@@ -0,0 +1 @@
+  The video shows a man standing behind a counter in a hardware store aisle. He is wearing a black shirt and appears to be selling items or providing assistance to customers. Another person can be seen walking by him, but he doesn't seem to pay much attention to them. The store seems to have various items for sale, including tools and other hardware products. The man behind the counter seems to be knowledgeable about the products and is likely an employee of the store. Overall, the scene depicts a typical day at a hardware store where customers come to purchase items or seek assistance from the staff.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_47_0320241830.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_47_0320241830.mp4.txt
new file mode 100644
index 00000000..f848c8e5
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_47_0320241830.mp4.txt
@@ -0,0 +1,2 @@
+  The video captures three men shopping in a hardware store aisle. One man is wearing a black shirt and jeans, holding a red basket. He appears to be examining an item on the shelf, possibly deciding whether to purchase it or not. Another man is wearing a blue shirt and khakis, holding a blue basket. He seems to be looking at a different item on the shelf, possibly comparing its features to the one he's already picked up. The third man is standing behind them, wearing a white shirt and gray pants. He appears to be observing the other two men's choices or waiting for his turn to make a selection.
+As they shop, the men interact with various objects in the store. One man picks up a green object off the shelf, inspecting it closely before placing it back down. Another man holds a yellow object in his hand, likely considering whether to buy it or not. Throughout the scene, there are multiple objects on the shelves, including tools, paint supplies, and other hardware items. The colors of these objects vary, adding visual interest to the scene. Overall, the video showcases a typical day at a hardware store, where customers are browsing through various products and making purchases.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_5_0320241915.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_5_0320241915.mp4.txt
new file mode 100644
index 00000000..a1c689ea
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_5_0320241915.mp4.txt
@@ -0,0 +1,3 @@
+  The video features two men shopping in the hardware store. One man is walking down the aisle, while the other man is standing near him. Both men appear to be focused on finding specific items in the store. They are both wearing casual clothing, which suggests that they might be browsing through the store for personal use or for a project they are working on.
+As they walk down the aisle, they pass by several items, including paintbrushes, scissors, and other tools. The paintbrushes are colorful and come in different sizes, while the scissors have sharp edges and are designed for cutting materials. The men seem to be examining these items closely, likely considering whether they need them for their project or not.
+The overall atmosphere of the store appears to be calm and organized, with the items neatly arranged on shelves and displays. The colors of the items are predominantly neutral, with some accents of bright colors, such as red and blue, adding visual interest to the store's interior. The men's interactions with the objects in the store suggest that they are engaged in a thoughtful and deliberate process of selecting the right tools and supplies for their needs.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_DSCF2862_Rendered_001.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_DSCF2862_Rendered_001.mp4.txt
new file mode 100644
index 00000000..e7a22d0e
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_DSCF2862_Rendered_001.mp4.txt
@@ -0,0 +1,3 @@
+  The video shows a diverse group of people shopping in the hardware store aisle. One man is prominently visible, holding up an item while another man looks at it closely. This suggests that they might be discussing the product's features or comparing it to similar items. Other people in the store are also browsing through the shelves, likely searching for specific items or exploring new products.
+In terms of clothing, most of the individuals appear to be dressed casually, with some wearing jeans and t-shirts, while others may have more formal attire such as button-down shirts or slacks. The store itself has a variety of items displayed on the shelves, ranging from tools and equipment to cleaning supplies and home decorations. The items are arranged neatly, making it easy for customers to find what they need.
+The color palette of the store is predominantly neutral, with white walls and wooden shelves providing a clean and organized look. The items themselves come in various colors, reflecting the diverse range of products available in the store. Some items may have bright colors, while others may have more subdued tones, depending on their purpose and intended use. Overall, the scene captures a typical day at a hardware store, where customers are engaged in their shopping experience and interacting with the products available.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_DSCF2864_Rendered_006.mp4.txt b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_DSCF2864_Rendered_006.mp4.txt
new file mode 100644
index 00000000..6d428d1c
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/files/scene_description/op_DSCF2864_Rendered_006.mp4.txt
@@ -0,0 +1 @@
+  The video shows a man walking down an aisle in a hardware store. He is wearing a blue shirt and khaki pants. As he walks, he looks at various items on the shelves, including tools and cleaning supplies. He stops to pick up a pair of scissors, examines them closely, and then puts them back on the shelf. Throughout the video, there are several other people visible in the background, but the main focus is on the man in the blue shirt. The store has a variety of products displayed on the shelves, including different types of tools, cleaning supplies, and other household items. The color scheme of the store is predominantly white, which gives it a bright and clean appearance. The overall atmosphere of the store appears to be organized and easy to navigate, allowing customers to quickly find what they're looking for.
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_10_0320241830.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_10_0320241830.mp4
new file mode 100644
index 00000000..62ff6ce5
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_10_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_19_0320241830.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_19_0320241830.mp4
new file mode 100644
index 00000000..e5ce24dc
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_19_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_1_0320241830.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_1_0320241830.mp4
new file mode 100644
index 00000000..29c5dffc
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_1_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_21_0320241830.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_21_0320241830.mp4
new file mode 100644
index 00000000..4b67bd4d
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_21_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_24_0320241830.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_24_0320241830.mp4
new file mode 100644
index 00000000..69cbd7f6
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_24_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_31_0320241830.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_31_0320241830.mp4
new file mode 100644
index 00000000..2ee0c1ec
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_31_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_47_0320241830.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_47_0320241830.mp4
new file mode 100644
index 00000000..0ec140b0
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_47_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_5_0320241915.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_5_0320241915.mp4
new file mode 100644
index 00000000..2466dabb
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_5_0320241915.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_DSCF2862_Rendered_001.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_DSCF2862_Rendered_001.mp4
new file mode 100644
index 00000000..f5882aa2
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_DSCF2862_Rendered_001.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_DSCF2864_Rendered_006.mp4 b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_DSCF2864_Rendered_006.mp4
new file mode 100644
index 00000000..5614e436
Binary files /dev/null and b/demos/python_demos/rag_chatbot/servable_stream/files/videos/op_DSCF2864_Rendered_006.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/servable_stream/generate_store_embeddings.py b/demos/python_demos/rag_chatbot/servable_stream/generate_store_embeddings.py
new file mode 100644
index 00000000..305e181f
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/generate_store_embeddings.py
@@ -0,0 +1,138 @@
+import yaml
+import chromadb
+import json
+import os
+from extract_store_frames import process_all_videos
+from langchain_experimental.open_clip import OpenCLIPEmbeddings
+from vector_stores import db
+from utils import config_reader as reader
+
+# EMBEDDING MODEL
+clip_embd = OpenCLIPEmbeddings(model_name="ViT-g-14", checkpoint="laion2b_s34b_b88k")
+
+def read_json(path):
+    with open(path) as f:
+        x = json.load(f)
+    return x
+
+def read_file(path):
+    content = None
+    with open(path, 'r') as file:
+        content = file.read()
+    return content
+
+def store_into_vectordb(metadata_file_path):
+    GMetadata = read_json(metadata_file_path)
+    global_counter = 0
+
+    total_videos = len(GMetadata.keys())
+    
+    for _, (video, data) in enumerate(GMetadata.items()):
+
+        image_name_list = []
+        embedding_list = []
+        metadata_list = []
+        ids = []
+        
+        # process frames
+        frame_metadata = read_json(data['extracted_frame_metadata_file'])
+        for frame_id, frame_details in frame_metadata.items():
+            global_counter += 1
+            meta_data = {
+                'timestamp': frame_details['timestamp'],
+                'frame_path': frame_details['frame_path'],
+                'video': video,
+                'embedding_path': data['embedding_path'],
+            }
+            image_path = frame_details['frame_path']
+            image_name_list.append(image_path)
+
+            metadata_list.append(meta_data)
+            ids.append(str(global_counter))
+
+        # generate clip embeddings
+        embedding_list.extend(clip_embd.embed_image(image_name_list))
+
+        vs.add_images(
+            uris=image_name_list,
+            metadatas=metadata_list
+        )
+        
+        print (f'âœ… {_+1}/{total_videos} video {video}, len {len(image_name_list)}, {len(metadata_list)}, {len(embedding_list)}')
+    
+def generate_image_embeddings():
+    if generate_frames:
+        print ('Processing all videos, Generated frames will be stored at')
+        print (f'input video folder = {path}')
+        print (f'frames output folder = {image_output_dir}')
+        print (f'metadata files output folder = {meta_output_dir}')
+        process_all_videos(path, image_output_dir, meta_output_dir, N)
+    
+    global_metadata_file_path = meta_output_dir + 'metadata.json'
+    print(f'global metadata file available at {global_metadata_file_path}')
+    store_into_vectordb(global_metadata_file_path)
+    
+def generate_text_embeddings():
+    all_videos = os.listdir(path)
+    
+    # each scene description is a document in vector storage
+    text_content = []
+    metadata_list = []
+    
+    for video in all_videos:
+        description_path = os.path.join(config['description'], video + '.txt')
+        if os.path.exists(description_path):
+            # read file content and prepare document 
+            text = read_file(description_path)
+            
+            text_content.append(text)
+            
+            metadata =  {
+                    'video': video
+                }
+        
+            metadata_list.append(metadata)
+            
+    # add it to db
+    vs.add_texts(
+        texts=text_content,
+        metadatas=metadata_list,
+    )
+    
+def retrieval_testing():
+    Q = 'man holding red basket'
+    print (f'Testing Query {Q}')
+    results = vs.MultiModalRetrieval(Q)
+    
+    print (results)
+    
+if __name__ == '__main__':
+    # read config yaml
+    print ('Reading config file')
+    config = reader.read_config('config.yaml')
+    
+    print ('Config file data \n', yaml.dump(config, default_flow_style=False, sort_keys=False))
+
+    generate_frames = config['generate_frames']
+    embed_frames = config['embed_frames']
+    path = config['videos']
+    image_output_dir = config['image_output_dir']
+    meta_output_dir = config['meta_output_dir']
+    N = config['number_of_frames_per_second']
+    
+    host = config['vector_db']['host']
+    port = int(config['vector_db']['port'])
+    selected_db = config['vector_db']['choice_of_db']
+    
+    # Creating DB
+    print ('Creating DB with text and image embedding support, \nIt may take few minutes to download and load all required models if you are running for first time.')
+    
+    vs = db.VS(host, port, selected_db)
+    
+    generate_text_embeddings()
+    
+    retrieval_testing()
+    
+    generate_image_embeddings()
+    
+    retrieval_testing()
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/graph.pbtxt b/demos/python_demos/rag_chatbot/servable_stream/graph.pbtxt
index 7d941850..4dbf3355 100755
--- a/demos/python_demos/rag_chatbot/servable_stream/graph.pbtxt
+++ b/demos/python_demos/rag_chatbot/servable_stream/graph.pbtxt
@@ -42,7 +42,7 @@ node {
   output_stream: "END_SIGNAL:end_signal"
   node_options: {
     [type.googleapis.com / mediapipe.PythonExecutorCalculatorOptions]: {
-      handler_path: "model.py"
+      handler_path: "/workspace/model.py"
     }
   }
 }
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/image_retrieval_ui.py b/demos/python_demos/rag_chatbot/servable_stream/image_retrieval_ui.py
new file mode 100644
index 00000000..5b420bce
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/image_retrieval_ui.py
@@ -0,0 +1,53 @@
+import streamlit as st
+from vector_stores import db
+from utils import config_reader as reader
+
+if 'config' not in st.session_state.keys():
+    st.session_state.config = reader.read_config('config.yaml')
+
+st.set_page_config(initial_sidebar_state='collapsed', layout='wide')
+
+st.title("Image Retriever")
+
+@st.cache_resource       
+def load_db():
+    if 'vs' not in st.session_state.keys():
+        with st.spinner('Loading vector-store chain'):
+            host = st.session_state.config['vector_db']['host']
+            port = int(st.session_state.config['vector_db']['port'])
+            selected_db = st.session_state.config['vector_db']['choice_of_db']
+            st.session_state['vs'] = db.VS(host, port, selected_db)
+            
+load_db()
+            
+def custom_image_retrieval(Q, N=4):
+    print (Q)
+    
+    image_config = {"configurable": {"k_image_docs": {"k": N}}}
+    results = st.session_state.vs.image_retriever.invoke(Q, config=image_config)
+    
+    print (len(results))
+    
+    col1, col2 = st.columns(2)
+
+    image_paths = []
+    captions = []
+    
+    for r in results:
+        image_paths.append(r.metadata['frame_path'])
+        captions.append(r.metadata['video'])
+
+    for i, image in enumerate(image_paths):
+        if i % 2 == 0:
+            with col1:
+                st.image(image, caption=captions[i])
+        else:
+            with col2:
+                st.image(image, caption=captions[i])
+
+N = st.slider('Number of Images', 1, 16, 2)
+N = int(N)
+Q = st.text_input('Enter Text to retrieve Images')
+
+if Q is not None and Q != '':
+    custom_image_retrieval(Q, N)
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/model.py b/demos/python_demos/rag_chatbot/servable_stream/model.py
index 6719ce3d..d55648c9 100755
--- a/demos/python_demos/rag_chatbot/servable_stream/model.py
+++ b/demos/python_demos/rag_chatbot/servable_stream/model.py
@@ -1,5 +1,5 @@
 #*****************************************************************************
-# Copyright 2024 Intel Corporation
+# Copyright 2023 Intel Corporation
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -8,153 +8,45 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
+# printtributed under the License is printtributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #*****************************************************************************
 
-########### Workaround: https://docs.trychroma.com/troubleshooting#sqlite
-__import__('pysqlite3')
-import sys
-import time
-sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
-############
 import os
-import requests
-
-from pyovms import Tensor
-
-from threading import Thread
+import threading
+import numpy as np
+import torch
 
+from typing import Optional, List, Tuple
+from optimum.intel import OVModelForCausalLM
+from transformers import AutoTokenizer, AutoConfig, TextIteratorStreamer, StoppingCriteria, StoppingCriteriaList, set_seed
 from tritonclient.utils import deserialize_bytes_tensor, serialize_byte_tensor
-from langchain_community.llms import HuggingFacePipeline
-from optimum.intel.openvino import OVModelForCausalLM
-import torch
-from langchain.chains import RetrievalQA
-from transformers import (
-    AutoModelForCausalLM,
-    AutoModel,
-    AutoTokenizer,
-    AutoConfig,
-    TextIteratorStreamer,
-    pipeline,
-    StoppingCriteria,
-    StoppingCriteriaList,
-    set_seed
-)
 
-from config import SUPPORTED_EMBEDDING_MODELS, SUPPORTED_LLM_MODELS
-from ov_embedding_model import OVEmbeddings
+from pyovms import Tensor
 
-FILE = "docs.txt"
-TARGET_FOLDER = "/tmp/documents"
+from config import SUPPORTED_LLM_MODELS, BatchTextIteratorStreamer
 
-SELECTED_MODEL = os.environ.get('SELECTED_MODEL', 'tiny-llama-1b-chat')
-LANGUAGE = os.environ.get('LANGUAGE', 'English')
-SEED = os.environ.get('SEED')
-llm_model_configuration = SUPPORTED_LLM_MODELS[LANGUAGE][SELECTED_MODEL]
+from utils import config_reader as reader
+from utils import prompt_handler as ph
+from vector_stores import db
 
-EMBEDDING_MODEL = 'all-mpnet-base-v2'
-embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[EMBEDDING_MODEL]
+config = reader.read_config('config.yaml')
 
-class StopOnTokens(StoppingCriteria):
-    def __init__(self, token_ids):
-        self.token_ids = token_ids
+SELECTED_MODEL = os.environ.get('SELECTED_MODEL', 'llama-2-chat-7b')
+LANGUAGE = os.environ.get("LANGUAGE", 'English')
+SEED = os.environ.get("SEED")
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
-    ) -> bool:
-        for stop_id in self.token_ids:
-            if input_ids[0][-1] == stop_id:
-                return True
-        return False
-
-# Document Splitter
-from typing import List
-from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, MarkdownTextSplitter
-from langchain_community.document_loaders import (
-    CSVLoader,
-    EverNoteLoader,
-    PDFMinerLoader,
-    TextLoader,
-    UnstructuredEPubLoader,
-    UnstructuredHTMLLoader,
-    UnstructuredMarkdownLoader,
-    UnstructuredODTLoader,
-    UnstructuredPowerPointLoader,
-    UnstructuredWordDocumentLoader, )
-from langchain.prompts import PromptTemplate
-from langchain_community.vectorstores import Chroma
-from langchain.chains import RetrievalQA
-from langchain.docstore.document import Document
-
-class ChineseTextSplitter(CharacterTextSplitter):
-    def __init__(self, pdf: bool = False, **kwargs):
-        super().__init__(**kwargs)
-        self.pdf = pdf
-
-    def split_text(self, text: str) -> List[str]:
-        if self.pdf:
-            text = re.sub(r"\n{3,}", "\n", text)
-            text = text.replace("\n\n", "")
-        sent_sep_pattern = re.compile(
-            '([ï¹’ï¹”ï¹–ï¹—ï¼Žã€‚ï¼ï¼Ÿ]["â€™â€ã€ã€]{0,2}|(?=["â€˜â€œã€Œã€Ž]{1,2}|$))')
-        sent_list = []
-        for ele in sent_sep_pattern.split(text):
-            if sent_sep_pattern.match(ele) and sent_list:
-                sent_list[-1] += ele
-            elif ele:
-                sent_list.append(ele)
-        return sent_list
-
-
-TEXT_SPLITERS = {
-    "Character": CharacterTextSplitter,
-    "RecursiveCharacter": RecursiveCharacterTextSplitter,
-    "Markdown": MarkdownTextSplitter,
-    "Chinese": ChineseTextSplitter,
-}
-
-
-LOADERS = {
-    ".csv": (CSVLoader, {}),
-    ".doc": (UnstructuredWordDocumentLoader, {}),
-    ".docx": (UnstructuredWordDocumentLoader, {}),
-    ".enex": (EverNoteLoader, {}),
-    ".epub": (UnstructuredEPubLoader, {}),
-    ".html": (UnstructuredHTMLLoader, {}),
-    ".md": (UnstructuredMarkdownLoader, {}),
-    ".odt": (UnstructuredODTLoader, {}),
-    ".pdf": (PDFMinerLoader, {}),
-    ".ppt": (UnstructuredPowerPointLoader, {}),
-    ".pptx": (UnstructuredPowerPointLoader, {}),
-    ".txt": (TextLoader, {"encoding": "utf8"}),
-}
-
-
-def load_single_document(file_path: str) -> List[Document]:
-    """
-    helper for loading a single document
-
-    Params:
-      file_path: document path
-    Returns:
-      documents loaded
-
-    """
-    ext = "." + file_path.rsplit(".", 1)[-1]
-    if ext in LOADERS:
-        loader_class, loader_args = LOADERS[ext]
-        loader = loader_class(file_path, **loader_args)
-        return loader.load()
-
-    raise ValueError(f"File does not exist '{ext}'")
+print("SELECTED MODEL", SELECTED_MODEL, flush=True)
+model_configuration = SUPPORTED_LLM_MODELS[LANGUAGE][SELECTED_MODEL]
 
+MODEL_PATH = config['model_path'] # relative to container
+OV_CONFIG = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1'}
 
 def default_partial_text_processor(partial_text: str, new_text: str):
     """
-    helper for updating partially generated answer, used by default
+    helper for updating partially generated answer, used by de
 
     Params:
       partial_text: text buffer for storing previosly generated text
@@ -166,11 +58,73 @@ def default_partial_text_processor(partial_text: str, new_text: str):
     partial_text += new_text
     return partial_text
 
-
-text_processor = llm_model_configuration.get(
+text_processor = model_configuration.get(
     "partial_text_processor", default_partial_text_processor
 )
 
+# Model specific configuration
+model_name = model_configuration["model_id"]
+history_template = model_configuration["history_template"]
+current_message_template = model_configuration["current_message_template"]
+start_message = model_configuration["start_message"]
+stop_tokens = model_configuration.get("stop_tokens")
+tokenizer_kwargs = model_configuration.get("tokenizer_kwargs", {})
+tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
+tokenizer.pad_token = tokenizer.eos_token  # For models with tokenizer with uninitialized pad token
+
+# HF class that is capable of stopping the generation
+# when given tokens appear in specific order
+class StopOnTokens(StoppingCriteria):
+    def __init__(self, token_ids):
+        self.token_ids = token_ids
+    def __call__(self, input_ids, scores, **kwargs) -> bool:
+        for stop_id in self.token_ids:
+            if input_ids[0][-1] == stop_id:
+                return True
+        return False
+
+
+if stop_tokens is not None:
+    if isinstance(stop_tokens[0], str):
+        stop_tokens = tokenizer.convert_tokens_to_ids(stop_tokens)
+
+    stop_tokens = [StopOnTokens(stop_tokens)]
+
+
+# For multi Q&A use cases
+# Taken from notebook:
+# https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/254-llm-chatbot/
+def convert_history_to_text(history):
+    """
+    function for conversion history stored as list pairs of user and assistant messages to string according to model expected conversation template
+    Params:
+      history: dialogue history
+    Returns:
+      history in text format
+    """
+    text = start_message + "".join(
+        [
+            "".join(
+                [history_template.format(num=round, user=item[0], assistant=item[1])]
+            )
+            for round, item in enumerate(history[:-1])
+        ]
+    )
+    text += "".join(
+        [
+            "".join(
+                [
+                    current_message_template.format(
+                        num=len(history) + 1,
+                        user=history[-1][0],
+                        assistant=history[-1][1],
+                    )
+                ]
+            )
+        ]
+    )
+    return text
+
 
 def deserialize_prompts(batch_size, input_tensor):
     if batch_size == 1:
@@ -185,189 +139,135 @@ def serialize_completions(batch_size, result):
     return [Tensor("completion", serialize_byte_tensor(
         np.array(result, dtype=np.object_)).item())]
 
-def download_documents(file, target_folder):
-    if not os.path.exists(target_folder):
-        os.makedirs(target_folder)
-    file = open(file, "r")
-    for url in file.readlines():
-        url = url.strip()
-        if url.find('/'):
-            filename = url.rsplit('/', 1)[1]
-        else:
-            filename = url
-        r = requests.get(url, allow_redirects=True, verify=False)
-        if r.status_code == 200:
-            open(os.path.join(target_folder, filename), 'wb').write(r.content)
-            print("Saved", filename, flush=True)
-        else:
-            print(f"Failed to download: {url}, status code: {r.status_code}", flush=True)
-
-def clean_target_folder(target_folder):
-    for file_path in os.listdir(target_folder):
-        os.remove(os.path.join(target_folder,file_path))
+# RAG variables
+host = config['vector_db']['host']
+port = int(config['vector_db']['port'])
+selected_db = config['vector_db']['choice_of_db']
+vs = db.VS(host, port, selected_db)
+qcnt = 0
+
+def get_top_doc(results, qcnt):
+    hit_score = {}
+    for r in results:
+        try:
+            video_name = r.metadata['video']
+            if video_name not in hit_score.keys(): hit_score[video_name] = 0
+            hit_score[video_name] += 1
+        except:
+            pass
+
+    x = dict(sorted(hit_score.items(), key=lambda item: -item[1]))
+    
+    if qcnt >= len(x):
+        return None
+    print (f'top docs = {x}', flush=True)
+    return {'video': list(x)[qcnt]}
+
+def get_description(vn):
+    content = None
+    des_path = os.path.join(config['description'], vn + '.txt')
+    with open(des_path, 'r') as file:
+        content = file.read()
+    return content
+
+def RAG(prompt):
+    
+    results = vs.MultiModalRetrieval(prompt, n_texts = 1, n_images = 3)
+    print (f'promt={prompt}\n')
+                
+    top_doc = get_top_doc(results, qcnt)
+    print ('TOP DOC = ', top_doc)
+    if top_doc == None:
+        return None, None
+    video_name = top_doc['video']
+    
+    return video_name, top_doc
+
+import random
+def setup_seeds(seed):    
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
 
 class OvmsPythonModel:
     def initialize(self, kwargs: dict):
-
-        device = os.environ.get("DEVICE", "CPU")   
-        llm_model_dir = os.environ.get("LLM_MODEL_DIR", SELECTED_MODEL)
-        llm_model_dir = os.path.join(kwargs["base_path"], llm_model_dir)  # use absolute path
-        model_name = llm_model_configuration["model_id"]
-        if os.path.isdir(llm_model_dir):
-            llm_model_2_load = llm_model_dir
-            export = False
-        else:
-            llm_model_2_load = model_name
-            export = True
-
-        print("llm model to load", llm_model_2_load, flush=True)
-        self.stop_tokens = llm_model_configuration.get("stop_tokens")
-        self.db = None
-        class_key = SELECTED_MODEL.split("-")[0]
-        self.tok = AutoTokenizer.from_pretrained(llm_model_2_load, trust_remote_code=True)
-
-        if self.stop_tokens is not None:
-            if isinstance(self.stop_tokens[0], str):
-                self.stop_tokens = self.tok.convert_tokens_to_ids(self.stop_tokens)
-            self.stop_tokens = [StopOnTokens(self.stop_tokens)]
-
-        from ov_llm_model import model_classes
-        model_class = (
-            OVModelForCausalLM
-            if not llm_model_configuration["remote"]
-            else model_classes[class_key]
-        )
-        ov_config = {"PERFORMANCE_HINT": "LATENCY", "NUM_STREAMS": "1", "CACHE_DIR": ""}
-        print(f"Loading LLM model {llm_model_2_load}...class {model_class}", flush=True)
-        self.last_refreshDB = 0
-        self.base_path = kwargs["base_path"]
-        self.active_refresh = True
-        self.ov_model = model_class.from_pretrained(
-            llm_model_2_load,
-            device=device,
-            ov_config=ov_config,
-            compile=True,
-            export=export,
-            config=AutoConfig.from_pretrained(llm_model_2_load, trust_remote_code=True),
-            trust_remote_code=True)
-        print("LLM model loaded", flush=True)
-
-        embedding_model_dir = os.environ.get("EMBEDDING_MODEL_DIR", 'all-mpnet-base-v2')
-        embedding_model_dir = os.path.join(kwargs["base_path"], embedding_model_dir)  # use absolute path
-        if os.path.isdir(embedding_model_dir):
-            embedding_model_2_load = embedding_model_dir
-        else:
-            embedding_model_2_load = EMBEDDING_MODEL
-        print(f"Loading embedding model {embedding_model_2_load}...", flush=True)
-        self.embedding = OVEmbeddings.from_model_id(
-            embedding_model_2_load,
-            do_norm=embedding_model_configuration["do_norm"],
-            ov_config={
-                "device_name": device,
-                "config": {"PERFORMANCE_HINT": "LATENCY"},
-            },
-            model_kwargs={
-                "model_max_length": 512,
-            },
-        )
-        print("Embedding model loaded", flush=True)
-        print("Building document database...", flush=True)
-        file = os.path.join(self.base_path, FILE)
-        if os.path.exists(file):
-            self.last_refreshDB = os.stat(file).st_mtime
-            download_documents(file, TARGET_FOLDER)
-            self.scan_documents(TARGET_FOLDER)
-
-        def refreshDB_if_needed(file, target_folder):
-            while self.active_refresh:
-                if os.path.exists(file):
-                    if (self.last_refreshDB < os.stat(file).st_mtime):
-                        print("Refreshing DB", flush=True)
-                        clean_target_folder(target_folder)
-                        download_documents(file, target_folder)
-                        self.scan_documents(target_folder)
-                        self.last_refreshDB = os.stat(file).st_mtime
-                time.sleep(10)
-        
-        self.t_refresh = Thread(target=refreshDB_if_needed, args=(file, TARGET_FOLDER,))
-        self.t_refresh.start()
-
-        print("Refresh thread started", flush=True)
-
-    def scan_documents(self, target_folder):
-        documents = []
-        for file_path in os.listdir(target_folder):
-            abs_path = os.path.join(target_folder, file_path)
-            print(f"Reading document {abs_path}...", flush=True)
-            documents.extend(load_single_document(abs_path))
-        print("Documents loaded", flush=True)
-        spliter_name = "RecursiveCharacter"  # TODO: Param?
-        chunk_size=1000  # TODO: Param?
-        chunk_overlap=200  # TODO: Param?
-        text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)
-        print("Splitting documents...", flush=True)
-        self.texts = text_splitter.split_documents(documents)
-        print("Documents splitted", self.texts, flush=True)
-        if self.db is not None:
-            self.db.delete_collection()
-        self.db = Chroma.from_documents(self.texts, self.embedding)
-        print("Document database built", flush=True)
-        vector_search_top_k = 4  # TODO: Param?
-        self.retriever = self.db.as_retriever(search_kwargs={"k": vector_search_top_k})
-        print("Document database loaded", flush=True)
+        print("-------- Running initialize")
+        self.ov_model = OVModelForCausalLM.from_pretrained(
+            MODEL_PATH,
+            device="AUTO",
+            ov_config=OV_CONFIG,
+            config=AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True))
+        print("-------- Model loaded")
+        return True
 
     def execute(self, inputs: list):
-        print("Executing", flush=True)
-
+        if SEED is not None: set_seed(int(SEED))
+        print(f"-------- Running execute, shape: {inputs[0].shape}")
         batch_size = inputs[0].shape[0]
-        if batch_size != 1:
-            raise ValueError("Batch size must be 1")
         prompts = deserialize_prompts(batch_size, inputs[0])
+        messages = [convert_history_to_text([[prompt, ""]]) for prompt in prompts]
+        tokens = tokenizer(messages, return_tensors="pt", **tokenizer_kwargs, padding=True)
+
+        if batch_size == 1:
+            streamer = TextIteratorStreamer(tokenizer, timeout=30.0, skip_prompt=True, skip_special_tokens=True)
+        else:
+            streamer = BatchTextIteratorStreamer(batch_size=batch_size, tokenizer=tokenizer, timeout=30.0, skip_prompt=True, skip_special_tokens=True)
 
-        ov_model_exec = self.ov_model.clone()
-        streamer = TextIteratorStreamer(
-            self.tok, timeout=60.0, skip_prompt=True, skip_special_tokens=True)
         generate_kwargs = dict(
-            model=ov_model_exec,
-            tokenizer=self.tok,
-            max_new_tokens=256,
-            temperature=0.1,
+            max_new_tokens=1024,
+            temperature=1.0,
             do_sample=True,
             top_p=1.0,
             top_k=50,
             repetition_penalty=1.1,
             streamer=streamer,
         )
-        if self.stop_tokens is not None:
-            generate_kwargs["stopping_criteria"] = StoppingCriteriaList(self.stop_tokens)
-          
-        pipe = pipeline("text-generation", **generate_kwargs)
-        llm = HuggingFacePipeline(pipeline=pipe)
-
-        prompt = PromptTemplate.from_template(llm_model_configuration["rag_prompt_template"])
-        print("Prompt", prompt, flush=True)
-        chain_type_kwargs = {"prompt": prompt}
-        rag_chain = RetrievalQA.from_chain_type(
-            llm=llm,
-            chain_type="stuff",
-            retriever=self.retriever,
-            chain_type_kwargs=chain_type_kwargs,
-        )
+        if stop_tokens is not None:
+            generate_kwargs["stopping_criteria"] = StoppingCriteriaList(stop_tokens)
 
-        question = prompts[0]
-        def infer(q):
-            rag_chain.invoke(q)
+        ov_model_exec = self.ov_model.clone()
+        
+        def generate():
+            result = ov_model_exec.generate(**tokens, **generate_kwargs)
 
-        if SEED is not None: set_seed(int(SEED))
-        t1 = Thread(target=infer, args=(question,))
+        # Retrieval part
+        question = prompts[0]
+        video_name, top_doc = RAG(question)
+        scene_des = get_description(video_name)
+        yield serialize_completions(batch_size, 'Retrieval is complete, Preparing answer ...\n')
+        print (video_name)
+        print (scene_des)
+        formatted_prompt = ph.get_formatted_prompt(scene=scene_des, prompt=question)
+        
+        tokens = tokenizer(formatted_prompt, return_tensors="pt", padding=True)
+        
+        t1 = threading.Thread(target=generate)
         t1.start()
 
-        for new_text in streamer:
-            print(new_text, flush=True, end='')
-            yield [Tensor("completion", new_text.encode())]
-
+        for partial_result in streamer:
+            yield serialize_completions(batch_size, partial_result)
+        t1.join()
+        
+        yield serialize_completions(batch_size, ' **Top Retrieved Video:** ')
+        yield serialize_completions(batch_size, video_name)
         yield [Tensor("end_signal", "".encode())]
-    def finalize(self):
-        self.active_refresh = False
-        if self.t_refresh:
-            self.t_refresh.join()
+        print('end')
+        
+if __name__ == "__main__":
+    ovmsObj = OvmsPythonModel()
+    ovmsObj.initialize({'1':'2'}) 
+    inputs=[["man holding red basket?"]]    
+    ovmsObj.execute(inputs)
+    # #inputs=[["Describe in detail the video and its contents"]]    
+    # #ovmsObj.execute(inputs)    
+    # inputs=[["man wearing glasses"]]
+    # ovmsObj.execute(inputs)    
+    # inputs=[["which video shows three men"]]
+    # ovmsObj.execute(inputs)    
+    # inputs=[["person holding green object"]]
+    # ovmsObj.execute(inputs)    
+    # inputs=[["what is color of shopping basket"]]
+    # ovmsObj.execute(inputs)    
+    # inputs=[["man bending to pick an object"]]
+    # ovmsObj.execute(inputs)    
+    # print("Done executon")
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/ov_embedding_model.py b/demos/python_demos/rag_chatbot/servable_stream/ov_embedding_model.py
index bb7fc6cd..137cc344 100644
--- a/demos/python_demos/rag_chatbot/servable_stream/ov_embedding_model.py
+++ b/demos/python_demos/rag_chatbot/servable_stream/ov_embedding_model.py
@@ -18,13 +18,11 @@ from langchain.pydantic_v1 import BaseModel, Extra, Field
 from langchain.schema.embeddings import Embeddings
 from typing import Optional, Union, Dict, Tuple, Any, List
 from sklearn.preprocessing import normalize
-from transformers import AutoTokenizer, AutoModel
+from transformers import AutoTokenizer
 from pathlib import Path
-import os
 import openvino as ov
 import torch
 import numpy as np
-from config import SUPPORTED_EMBEDDING_MODELS
 
 class OVEmbeddings(BaseModel, Embeddings):
     """
@@ -47,7 +45,7 @@ class OVEmbeddings(BaseModel, Embeddings):
     @classmethod
     def from_model_id(
         cls,
-        model_name: str,
+        model_id: str,
         do_norm: bool,
         ov_config: Optional[dict],
         model_kwargs: Optional[dict],
@@ -55,20 +53,12 @@ class OVEmbeddings(BaseModel, Embeddings):
     ):
         _model_kwargs = model_kwargs or {}
         _ov_config = ov_config or {}
-        if os.path.isdir(model_name):
-            _model_kwargs = model_kwargs or {}
-            tokenizer = AutoTokenizer.from_pretrained(model_name, **_model_kwargs)
-            core = ov.Core()
-            model_path = os.path.join(model_name, "openvino_model.xml")
-            model = core.compile_model(model_path, **_ov_config)
-        else:
-            tokenizer = AutoTokenizer.from_pretrained(SUPPORTED_EMBEDDING_MODELS[model_name]["model_id"], **_model_kwargs)
-            model = AutoModel.from_pretrained(SUPPORTED_EMBEDDING_MODELS[model_name]["model_id"])
-            dummy_inputs = {"input_ids": torch.ones((1, 10), dtype=torch.long), "attention_mask": torch.ones((1, 10), dtype=torch.long)}
-            ov_model = ov.convert_model(model, example_input=dummy_inputs)
-            model = ov.compile_model(ov_model, **ov_config)
-
+        tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)
+        core = ov.Core()
+        model_path = Path(model_id) / "openvino_model.xml"
+        model = core.compile_model(model_path, **_ov_config)
         num_stream = model.get_property('NUM_STREAMS')
+
         return cls(
             model=model,
             tokenizer=tokenizer,
diff --git a/demos/python_demos/rag_chatbot/servable_stream/requirements.txt b/demos/python_demos/rag_chatbot/servable_stream/requirements.txt
new file mode 100644
index 00000000..5e6ff862
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/requirements.txt
@@ -0,0 +1,9 @@
+chromadb
+opencv-python-headless
+langchain-experimental
+open-clip-torch
+#streamlit
+#metafunctions
+sentence-transformers
+#accelerate
+#vdms
diff --git a/demos/python_demos/rag_chatbot/servable_stream/utils/__init__.py b/demos/python_demos/rag_chatbot/servable_stream/utils/__init__.py
new file mode 100644
index 00000000..8e348776
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/utils/__init__.py
@@ -0,0 +1 @@
+from . import config_reader
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/utils/config_reader.py b/demos/python_demos/rag_chatbot/servable_stream/utils/config_reader.py
new file mode 100644
index 00000000..8c8c96d8
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/utils/config_reader.py
@@ -0,0 +1,8 @@
+import yaml
+
+
+def read_config(path):
+    with open(path, 'r') as f:
+        config = yaml.safe_load(f)
+
+    return config
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/utils/prompt_handler.py b/demos/python_demos/rag_chatbot/servable_stream/utils/prompt_handler.py
new file mode 100644
index 00000000..945f59c7
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/utils/prompt_handler.py
@@ -0,0 +1,8 @@
+from jinja2 import Environment, BaseLoader
+
+PROMPT = open("utils/prompt_template.jinja2").read().strip()
+
+def get_formatted_prompt(scene, prompt):
+    env = Environment(loader=BaseLoader())
+    template = env.from_string(PROMPT)
+    return template.render(scene=scene, prompt=prompt)
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/utils/prompt_template.jinja2 b/demos/python_demos/rag_chatbot/servable_stream/utils/prompt_template.jinja2
new file mode 100644
index 00000000..33dc32df
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/utils/prompt_template.jinja2
@@ -0,0 +1,21 @@
+<<SYS>>
+You are an Intel assistnat who understantds visual and textual content.
+<</SYS>>
+[INST]
+You will be provided with two things, scene description and user's question. You are suppose to understantds scene description \
+and provide answer to user's question. 
+
+As an assistnat, you need to follow these Rules while answering questions,
+
+Rules:
+- Don't answer any question which are not related to provied scene description.
+- Don't be toxic and don't include harmful information.
+- Answer if you can from provided scene description otherwise just say You don't have enough information to answer the question.
+
+Here is the,  
+Scene Description: {{ scene }}
+
+The user wants to know,
+User: {{ prompt }}
+[/INST]\n
+Assistant:
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/vector_stores/db.py b/demos/python_demos/rag_chatbot/servable_stream/vector_stores/db.py
new file mode 100644
index 00000000..d4638c39
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/vector_stores/db.py
@@ -0,0 +1,133 @@
+import chromadb
+#from langchain_community.vectorstores import VDMS
+#from langchain_community.vectorstores.vdms import VDMS_Client
+from langchain_experimental.open_clip import OpenCLIPEmbeddings
+from langchain_community.embeddings.sentence_transformer import (
+    SentenceTransformerEmbeddings,
+)
+from langchain_community.vectorstores import Chroma
+from typing import List, Optional, Iterable
+from langchain_core.runnables import ConfigurableField
+
+class VS:
+    
+    def __init__(self, host, port, selected_db):
+        self.host = host
+        self.port = port
+        self.selected_db = selected_db
+        
+        # initializing important variables
+        self.client = None
+        self.text_db = None
+        self.image_db = None
+        self.text_embedder = SentenceTransformerEmbeddings(model_name="all-MiniLM-L12-v2")
+        self.image_embedder = OpenCLIPEmbeddings(model_name="ViT-g-14", checkpoint="laion2b_s34b_b88k")
+        self.text_collection = 'text-test'
+        self.image_collection = 'image-test'
+        self.text_retriever = None
+        self.image_retriever = None
+        
+        # initialize_db
+        self.get_db_client()
+        self.init_db()
+        
+    def get_db_client(self):
+        
+        if self.selected_db == 'chroma':
+            print ('Connecting to Chroma db server . . .')
+            self.client = chromadb.HttpClient(host=self.host, port=self.port)
+        
+        if self.selected_db == 'vdms':
+            print ('Connecting to VDMS db server . . .')
+            self.client = VDMS_Client(host=self.host, port=self.port)
+
+    def init_db(self):
+        print ('Loading db instances')
+        if self.selected_db ==  'chroma':
+            self.text_db = Chroma(
+                client = self.client,
+                embedding_function = self.text_embedder,
+                collection_name = self.text_collection,
+            )
+
+            self.image_db = Chroma(
+                client = self.client,
+                embedding_function = self.image_embedder,
+                collection_name = self.image_collection,
+            )
+
+        if self.selected_db == 'vdms':
+            self.text_db = VDMS (
+                client = self.client,
+                embedding = self.text_embedder,
+                collection_name = self.text_collection,
+                engine = "FaissFlat",
+            )
+
+            self.image_db = VDMS (
+                client = self.client,
+                embedding = self.image_embedder,
+                collection_name = self.image_collection,
+                engine = "FaissFlat",
+            )
+
+        self.image_retriever = self.image_db.as_retriever(search_type="mmr", search_kwargs={"k": 3}).configurable_fields(
+            search_kwargs=ConfigurableField(
+                id="k_image_docs",
+                name="Search Kwargs",
+                description="The search kwargs to use",
+            )
+        )
+        
+        self.text_retriever = self.text_db.as_retriever(search_kwargs={"k": 1}).configurable_fields(
+            search_kwargs=ConfigurableField(
+                id="k_text_docs",
+                name="Search Kwargs",
+                description="The search kwargs to use",
+            )
+        )
+        
+    def length(self):
+        if self.selected_db == 'chroma':
+            texts = self.text_db.__len__()
+            images = self.image_db.__len__()
+            return (texts, images)
+        
+        if self.selected_db == 'vdms':
+            pass
+        
+        return (None, None)
+        
+    def delete_collection(self, collection_name):
+        self.client.delete_collection(collection_name=collection_name)
+        
+    def add_images(
+            self,
+            uris: List[str],
+            metadatas: Optional[List[dict]] = None,
+        ):
+
+        self.image_db.add_images(uris, metadatas)
+
+    def add_texts(
+            self,
+            texts: Iterable[str],
+            metadatas: Optional[List[dict]] = None,
+        ):
+
+        self.text_db.add_texts(texts, metadatas)
+
+    def MultiModalRetrieval(
+            self,
+            query: str,
+            n_texts: Optional[int] = 1,
+            n_images: Optional[int] = 3,
+        ):
+        
+        text_config = {"configurable": {"k_text_docs": {"k": n_texts}}}
+        image_config = {"configurable": {"k_image_docs": {"k": n_images}}}
+
+        text_results = self.text_retriever.invoke(query, config=text_config)
+        image_results = self.image_retriever.invoke(query, config=image_config)
+
+        return text_results + image_results
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/servable_stream/visual-rag-ui.py b/demos/python_demos/rag_chatbot/servable_stream/visual-rag-ui.py
new file mode 100644
index 00000000..4c7dd8b0
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/servable_stream/visual-rag-ui.py
@@ -0,0 +1,280 @@
+import os
+import time
+import torch
+import streamlit as st
+
+import torch
+import streamlit as st
+from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
+
+from transformers import TextIteratorStreamer
+from typing import Any, List, Mapping, Optional
+from langchain_core.callbacks.manager import CallbackManagerForLLMRun
+from langchain.llms.base import LLM
+import threading
+from transformers import set_seed
+from utils import config_reader as reader
+from utils import prompt_handler as ph
+from vector_stores import db
+
+set_seed(22)
+
+if 'config' not in st.session_state.keys():
+    st.session_state.config = reader.read_config('config.yaml')
+
+config = st.session_state.config
+
+model_path = config['model_path']
+video_dir = config['videos']
+
+st.set_page_config(initial_sidebar_state='collapsed', layout='wide')
+
+st.title("Visual RAG")
+
+title_alignment="""
+<style>
+h1 {
+  text-align: center
+}
+
+video.stVideo {
+    width: 200px;
+    height: 500px;      
+}
+</style>
+"""
+st.markdown(title_alignment, unsafe_allow_html=True)
+
+@st.cache_resource       
+def load_models():
+    model = AutoModelForCausalLM.from_pretrained(
+        model_path, torch_dtype=torch.float32, device_map='auto', trust_remote_code=True,
+    )
+    
+    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
+    tokenizer.padding_size = 'right'
+    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)
+    
+    return model, tokenizer, streamer
+
+model, tokenizer, streamer = load_models()
+
+class CustomLLM(LLM):
+        
+    @torch.inference_mode()
+    def _call(
+            self, 
+            prompt: str,
+            stop: Optional[List[str]] = None,
+            run_manager: Optional[CallbackManagerForLLMRun] = None,
+            streamer: Optional[TextIteratorStreamer] = None,  # Add streamer as an argument
+        ) -> str:
+        
+        tokens = tokenizer.encode(prompt, return_tensors='pt')
+        
+        with torch.no_grad():
+            output = model.generate(input_ids = tokens,
+                                    max_new_tokens = 100,
+                                    num_return_sequences = 1,
+                                    num_beams = 1,
+                                    min_length = 1,
+                                    top_p = 0.9,
+                                    top_k = 50,
+                                    repetition_penalty = 1.2,
+                                    length_penalty = 1,
+                                    temperature = 0.1,
+                                    streamer=streamer,
+                                    # pad_token_id=tokenizer.eos_token_id,
+                                    do_sample=True
+                    )
+        
+    def stream_res(self, prompt):
+        thread = threading.Thread(target=self._call, args=(prompt, None, None, streamer))  # Pass streamer to _call
+        thread.start()
+        
+        for text in streamer:
+            yield text
+
+    @property
+    def _identifying_params(self) -> Mapping[str, Any]:
+        return model_path # {"name_of_model": model_path}
+
+    @property
+    def _llm_type(self) -> str:
+        return "custom"
+    
+def get_top_doc(results, qcnt):
+    hit_score = {}
+    for r in results:
+        try:
+            video_name = r.metadata['video']
+            if video_name not in hit_score.keys(): hit_score[video_name] = 0
+            hit_score[video_name] += 1
+        except:
+            pass
+
+    x = dict(sorted(hit_score.items(), key=lambda item: -item[1]))
+    
+    if qcnt >= len(x):
+        return None
+    print (f'top docs = {x}')
+    return {'video': list(x)[qcnt]}
+
+def play_video(x):
+    if x is not None:
+        video_file = x.replace('.pt', '')
+        path = video_dir + video_file
+        
+        video_file = open(path, 'rb')
+        video_bytes = video_file.read()
+
+        st.video(video_bytes, start_time=0)
+
+if 'llm' not in st.session_state.keys():
+    with st.spinner('Loading Models . . .'):
+        time.sleep(1)
+        st.session_state['llm'] = CustomLLM()
+        
+if 'vs' not in st.session_state.keys():
+    with st.spinner('Preparing RAG pipeline'):
+        time.sleep(1)
+        host = st.session_state.config['vector_db']['host']
+        port = int(st.session_state.config['vector_db']['port'])
+        selected_db = st.session_state.config['vector_db']['choice_of_db']
+        st.session_state['vs'] = db.VS(host, port, selected_db)
+        
+        if st.session_state.vs.client == None:
+            print ('Error while connecting to vector DBs')
+        
+# Store LLM generated responses
+if "messages" not in st.session_state.keys():
+    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
+        
+def clear_chat_history():
+    st.session_state.example_video = 'Enter Text'
+    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
+        
+def RAG(prompt):
+    
+    with st.status("Querying database . . . ", expanded=True) as status:
+        st.write('Retrieving 1 text doc and 3 image docs')
+        results = st.session_state.vs.MultiModalRetrieval(prompt, n_texts = 1, n_images = 3)
+        status.update(label="Retrived Top matching video!", state="complete", expanded=False)
+    
+    print (f'promt={prompt}\n')
+                
+    top_doc = get_top_doc(results, st.session_state['qcnt'])
+    print ('TOP DOC = ', top_doc)
+    if top_doc == None:
+        return None, None
+    video_name = top_doc['video']
+    
+    return video_name, top_doc
+
+def get_description(vn):
+    content = None
+    des_path = os.path.join(config['description'], vn + '.txt')
+    with open(des_path, 'r') as file:
+        content = file.read()
+    return content
+    
+st.sidebar.button('Clear Chat History', on_click=clear_chat_history)
+
+if 'prevprompt' not in st.session_state.keys():
+    st.session_state['prevprompt'] = ''
+    print("Setting prevprompt to None")
+if 'prompt' not in st.session_state.keys():
+    st.session_state['prompt'] = ''
+if 'qcnt' not in st.session_state.keys():
+    st.session_state['qcnt'] = 0
+
+def handle_message():
+    # Generate a new response if last message is not from assistant
+    if st.session_state.messages[-1]["role"] != "assistant":
+        # Handle user messages here
+        with st.chat_message("assistant"):
+            placeholder = st.empty()
+            start = time.time()
+            prompt = st.session_state['prompt']
+            
+            if prompt == 'Find similar videos':
+                prompt = st.session_state['prevprompt']
+                st.session_state['qcnt'] += 1
+            else:
+                st.session_state['qcnt'] = 0
+                st.session_state['prevprompt'] = prompt
+            video_name, top_doc = RAG(prompt)
+            if video_name == None:
+                full_response = f"No more relevant videos found. Select a different query. \n\n"
+                placeholder.markdown(full_response)
+                end = time.time()
+            else:
+                with col2:
+                    play_video(video_name)
+                
+                scene_des = get_description(video_name)
+                formatted_prompt = ph.get_formatted_prompt(scene=scene_des, prompt=prompt)
+                
+                full_response = ''
+                full_response = f"Most relevant retrived video is **{video_name}** \n\n"
+                
+                for new_text in st.session_state.llm.stream_res(formatted_prompt):
+                    full_response += new_text
+                    placeholder.markdown(full_response)
+                
+                end = time.time()
+                full_response += f'\n\nðŸš€ Generated in {end - start} seconds.'
+                placeholder.markdown(full_response)
+        message = {"role": "assistant", "content": full_response}
+        st.session_state.messages.append(message)
+      
+def display_messages():
+    # Display or clear chat messages
+    for message in st.session_state.messages:
+        with st.chat_message(message["role"]):
+            st.write(message["content"])
+
+col1, col2 = st.columns([2, 1])
+
+with col1:
+    st.selectbox(
+        'Example Prompts',
+        (
+            'Enter Text', 
+            'Find similar videos', 
+            'Man wearing glasses', 
+            'People reading item description',
+            'Man wearing khaki pants',
+            'Man laughing',
+            'Black tshirt guy holding red basket',
+            'Man holding red shopping basket',
+            'Man wearing blue shirt',
+            'Man putting object into his pocket',
+        ),
+        key='example_video'
+    )
+
+    st.write('You selected:', st.session_state.example_video)
+
+if st.session_state.example_video == 'Enter Text':
+    if prompt := st.chat_input(disabled=False):
+        st.session_state['prompt'] = prompt
+        st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
+        if prompt == 'Find similar videos':            
+            st.session_state.messages.append({"role": "assistant", "content": "Not supported"})
+        else:
+            st.session_state.messages.append({"role": "user", "content": prompt})
+        #st.session_state.messages.append({"role": "user", "content": prompt})
+else:
+    prompt = st.session_state.example_video
+    st.session_state['prompt'] = prompt
+    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
+    st.chat_input(disabled=True)
+    if prompt == 'Find similar videos':
+        st.session_state.messages.append({"role": "user", "content": prompt+': '+st.session_state['prevprompt']})
+    else:
+        st.session_state.messages.append({"role": "user", "content": prompt})
+
+with col1:
+    display_messages()
+    handle_message()
\ No newline at end of file
diff --git a/demos/python_demos/rag_chatbot/temp/25a75d62cf8860b73d9b59a849fff464f54f2781/op_DSCF2862_Rendered_001.mp4 b/demos/python_demos/rag_chatbot/temp/25a75d62cf8860b73d9b59a849fff464f54f2781/op_DSCF2862_Rendered_001.mp4
new file mode 100644
index 00000000..f5882aa2
Binary files /dev/null and b/demos/python_demos/rag_chatbot/temp/25a75d62cf8860b73d9b59a849fff464f54f2781/op_DSCF2862_Rendered_001.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/temp/55fff6ab0a299bdbee739dab6c9acab155ef38f7/op_24_0320241830.mp4 b/demos/python_demos/rag_chatbot/temp/55fff6ab0a299bdbee739dab6c9acab155ef38f7/op_24_0320241830.mp4
new file mode 100644
index 00000000..69cbd7f6
Binary files /dev/null and b/demos/python_demos/rag_chatbot/temp/55fff6ab0a299bdbee739dab6c9acab155ef38f7/op_24_0320241830.mp4 differ
diff --git a/demos/python_demos/rag_chatbot/visual-rag-readme.md b/demos/python_demos/rag_chatbot/visual-rag-readme.md
new file mode 100644
index 00000000..193918b9
--- /dev/null
+++ b/demos/python_demos/rag_chatbot/visual-rag-readme.md
@@ -0,0 +1,51 @@
+# Visual RAG using Intel's OVMS
+
+### Step 1. Build docker image
+
+Using default ```README.md```, 
+
+Modify ```Dockerfile.ubuntu``` and add ```RUN pip3 install opencv-python-headless langchain_experimental sentence-transformers open-clip-torch torch``` to install required packages.
+
+Generate openvino modelserver python image. (follow steps mentioned in default ```README.md``` file.
+
+### Step 2. Download models
+
+```bash
+cd demos/python_demos/rag_chatbot
+export SELECTED_MODEL=llama-2-chat-7b
+pip install -r requirements.txt
+
+# download llm model
+python download_model.py --model ${SELECTED_MODEL}
+
+# download embedding model 
+python download_embedding_model.py --model all-mpnet-base-v
+```
+
+### Step 3. start chroma db
+
+
+```bash
+docker run -p 8000:8000 chromadb/chroma
+```
+
+### Step 4. Generate Embeddings
+
+```bash
+cd rag_chatbot/servable_streams
+pip3 install -r requirements.txt 
+python3 generate_store_embeddings.py
+```
+
+### Step 5. Run model-server and webUI
+
+```bash
+# from directory
+cd rag_chatbot
+
+docker run -d --rm -p 9000:9000 -v ${PWD}/servable_stream:/workspace -v ${PWD}/${SELECTED_MODEL}:/llm_model \
+-v ${PWD}/all-mpnet-base-v2:/embed_model -v ${PWD}/documents:/documents -e SELECTED_MODEL=${SELECTED_MODEL} \
+registry.connect.redhat.com/intel/openvino-model-server:py --config_path /workspace/config.json --port 9000
+
+python3 app.py --web_url 10.190.180.102:50055 --ovms_url 10.190.180.102:9000
+```
\ No newline at end of file
diff --git a/demos/python_demos/requirements.txt b/demos/python_demos/requirements.txt
index d16abf0f..fdc4bfbe 100644
--- a/demos/python_demos/requirements.txt
+++ b/demos/python_demos/requirements.txt
@@ -17,7 +17,3 @@ langchain-community==0.0.28
 scikit-learn==1.4.1.post1
 chromadb==0.4.24
 pysqlite3-binary==0.5.2.post3  # Workaround: https://docs.trychroma.com/troubleshooting#sqlite
-unstructured==0.13.2
-markdown==3.6
-nncf==2.9.0
-
diff --git a/demos/real_time_stream_analysis/python/README.md b/demos/real_time_stream_analysis/python/README.md
index ebba4a89..1ea13bed 100644
--- a/demos/real_time_stream_analysis/python/README.md
+++ b/demos/real_time_stream_analysis/python/README.md
@@ -30,7 +30,7 @@ In the demo will be used two gRPC communication patterns which might be advantag
 ## gRPC streaming with MediaPipe graphs
 
 gRPC stream connection is allowed for served [MediaPipe graphs](../../../docs/mediapipe.md). It allows sending asynchronous calls to the endpoint all linked in a single session context. Responses are sent back via a stream and processed in the callback function.
-The helper class [StreamClient](https://github.com/openvinotoolkit/model_server/blob/main/demos/common/stream_client/stream_client.py) provides a mechanism for flow control and tracking the sequence of the requests and responses. In the StreamClient initialization the streaming mode is set via the parameter `streaming_api=True`.
+The helper class [StreamClient](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/common/stream_client/stream_client.py) provides a mechanism for flow control and tracking the sequence of the requests and responses. In the StreamClient initialization the streaming mode is set via the parameter `streaming_api=True`.
 
 Using the streaming API has the following advantages:
 - good performance thanks to asynchronous calls and sharing the graph execution for multiple calls
@@ -39,7 +39,7 @@ Using the streaming API has the following advantages:
 
 ### Preparing the model server for gRPC streaming with a Holistic graph
 
-The [holistic graph](https://github.com/openvinotoolkit/model_server/blob/main/demos/mediapipe/holistic_tracking/holistic_tracking.pbtxt) is expecting and IMAGE object on the input and returns an IMAGE on the output.
+The [holistic graph](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/mediapipe/holistic_tracking/holistic_tracking.pbtxt) is expecting and IMAGE object on the input and returns an IMAGE on the output.
 As such it doesn't require any preprocessing and postprocessing. In this demo the returned stream will be just visualized or sent to the target sink.
 
 The model server with the holistic use case can be deployed with the following steps:
diff --git a/demos/single_face_analysis_pipeline/python/README.md b/demos/single_face_analysis_pipeline/python/README.md
index c6f81e85..6ffe999b 100644
--- a/demos/single_face_analysis_pipeline/python/README.md
+++ b/demos/single_face_analysis_pipeline/python/README.md
@@ -52,7 +52,7 @@ docker run -p 9000:9000 -d -v ${PWD}/workspace:/workspace openvino/model_server
 
 ## Requesting the Service
 
-Exemplary client [single_face_analysis_pipeline.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/single_face_analysis_pipeline/python/single_face_analysis_pipeline.py) can be used to request pipeline deployed in previous step.
+Exemplary client [single_face_analysis_pipeline.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/single_face_analysis_pipeline/python/single_face_analysis_pipeline.py) can be used to request pipeline deployed in previous step.
 
 ```bash
 pip3 install -r requirements.txt
diff --git a/demos/using_onnx_model/python/README.md b/demos/using_onnx_model/python/README.md
index 19a167b2..4ab671e1 100644
--- a/demos/using_onnx_model/python/README.md
+++ b/demos/using_onnx_model/python/README.md
@@ -77,7 +77,7 @@ docker run -d -u $(id -u):$(id -g) -v $(pwd)/workspace:/workspace -p 9001:9001 o
 --config_path /workspace/config.json --port 9001
 ```
 
-The `onnx_model_demo.py` script can run inference both with and without performing preprocessing. Since in this variant preprocessing is done by the model server (via custom node), there's no need to perform any image preprocessing on the client side. In that case, run without `--run_preprocessing` option. See [preprocessing function](https://github.com/openvinotoolkit/model_server/blob/main/demos/using_onnx_model/python/onnx_model_demo.py#L26-L33) run in the client.
+The `onnx_model_demo.py` script can run inference both with and without performing preprocessing. Since in this variant preprocessing is done by the model server (via custom node), there's no need to perform any image preprocessing on the client side. In that case, run without `--run_preprocessing` option. See [preprocessing function](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/using_onnx_model/python/onnx_model_demo.py#L26-L33) run in the client.
 
 Run the client without preprocessing:
 ```bash
@@ -88,7 +88,7 @@ Detected class name: bee
 ```
 
 ## Node parameters explanation
-Additional preprocessing step applies a division and an subtraction to each pixel value in the image. This calculation is configured by passing two parameters to _image transformation_ custom node in [config.json](https://github.com/openvinotoolkit/model_server/blob/main/demos/using_onnx_model/python/config.json#L32-L33):
+Additional preprocessing step applies a division and an subtraction to each pixel value in the image. This calculation is configured by passing two parameters to _image transformation_ custom node in [config.json](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/using_onnx_model/python/config.json#L32-L33):
 ```
 "params": {
   ...
diff --git a/demos/vehicle_analysis_pipeline/python/README.md b/demos/vehicle_analysis_pipeline/python/README.md
index ab1b2146..af75fa0e 100644
--- a/demos/vehicle_analysis_pipeline/python/README.md
+++ b/demos/vehicle_analysis_pipeline/python/README.md
@@ -1,5 +1,5 @@
 # Vehicle Analysis Pipeline Demo {#ovms_demo_vehicle_analysis_pipeline}
-This document demonstrates how to create complex pipelines using object detection and object recognition models from OpenVINO Model Zoo. As an example, we will use [vehicle-detection-0202](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/vehicle-detection-0202/README.md) to detect multiple vehicles on the image. Then, for each detected vehicle we will crop it using [model_zoo_intel_object_detection](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/model_zoo_intel_object_detection) example custom node. Finally, each vehicle image will be forwarded to [vehicle-attributes-recognition-barrier-0042](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/vehicle-attributes-recognition-barrier-0042/README.md) model.
+This document demonstrates how to create complex pipelines using object detection and object recognition models from OpenVINO Model Zoo. As an example, we will use [vehicle-detection-0202](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/vehicle-detection-0202/README.md) to detect multiple vehicles on the image. Then, for each detected vehicle we will crop it using [model_zoo_intel_object_detection](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/model_zoo_intel_object_detection) example custom node. Finally, each vehicle image will be forwarded to [vehicle-attributes-recognition-barrier-0042](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/vehicle-attributes-recognition-barrier-0042/README.md) model.
 
 ![Vehicles analysis visualization](vehicles_analysis.png)
 
@@ -14,7 +14,7 @@ Below is depicted graph implementing vehicles analysis pipeline execution.
 It includes the following Nodes:
 - Model `vehicle_detection` - deep learning model which takes user image as input. Its outputs contain information about vehicle coordinates and confidence levels.
 - Custom node `model_zoo_intel_object_detection` - it includes C++ implementation of common object detection models results processing. By analysing the output it produces cropped vehicle images based on the configurable score level threshold. Custom node also resizes them to the target resolution and combines into a single output of a dynamic batch size. The output batch size is determined by the number of detected
-boxes according to the configured criteria. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/model_zoo_intel_object_detection).
+boxes according to the configured criteria. All operations on the images employ OpenCV libraries which are preinstalled in the OVMS. Learn more about the [model_zoo_intel_object_detection custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/model_zoo_intel_object_detection).
 - demultiplexer - outputs from the custom node model_zoo_intel_object_detection have variable batch size. In order to match it with the sequential recognition models, data is split into individual images with each batch size equal to 1.
 Such smaller requests can be submitted for inference in parallel to the next Model Nodes. Learn more about the [demultiplexing](../../../docs/demultiplexing.md).
 - Model `vehicle_attributes_recognition` - this model recognizes type and color for given vehicle image
diff --git a/docs/accelerators.md b/docs/accelerators.md
index f1f39ec5..dde21df9 100644
--- a/docs/accelerators.md
+++ b/docs/accelerators.md
@@ -50,7 +50,7 @@ docker run --rm -it  --device=/dev/dxg --volume /usr/lib/wsl:/usr/lib/wsl -u $(i
 > **NOTE**:
 > The public docker image includes the OpenCL drivers for GPU in version 22.28 (RedHat) and 22.35 (Ubuntu).
 
-If you need to build the OpenVINO Model Server with different driver version, refer to the [building from sources](https://github.com/openvinotoolkit/model_server/blob/main/docs/build_from_source.md)
+If you need to build the OpenVINO Model Server with different driver version, refer to the [building from sources](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/build_from_source.md)
 
 ## Using Multi-Device Plugin
 
@@ -173,7 +173,7 @@ cd model_server
 make docker_build NVIDIA=1 OV_USE_BINARY=0
 cd ..
 ```
-Check also [building from sources](https://github.com/openvinotoolkit/model_server/blob/main/docs/build_from_source.md).
+Check also [building from sources](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/build_from_source.md).
 
 Example command to run container with NVIDIA support:
 
diff --git a/docs/binary_input_kfs.md b/docs/binary_input_kfs.md
index 94339f39..53cd9111 100644
--- a/docs/binary_input_kfs.md
+++ b/docs/binary_input_kfs.md
@@ -23,7 +23,7 @@ KServe API also allows sending encoded images via HTTP interface to the model or
 For binary inputs, the `parameters` map in the JSON part contains `binary_data_size` field for each binary input that indicates the size of the data on the input. Since there's no strict limitations on image resolution and format (as long as it can be loaded by OpenCV), images might be of different sizes. To send a batch of images you need to precede data of every batch by 4 bytes(little endian) containing size of this batch and specify their combined size in `binary_data_size`. For example, if batch would contain three images of sizes 370, 480, 500 bytes the content of input buffer inside binary extension would look like this: 
 <0x72010000 (=370)><370 bytes of first image><0xE0010000 (=480)><480 bytes of second image> <0xF4010000 (=500)><500 bytes of third image>
 And in that case binary_data_size would be 1350(370 + 480 + 500)
-Function set_data_from_numpy in triton client lib that we use in our [REST sample](https://github.com/openvinotoolkit/model_server/blob/main/client/python/kserve-api/samples/http_infer_binary_resnet.py) automatically converts given images to this format.
+Function set_data_from_numpy in triton client lib that we use in our [REST sample](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/kserve-api/samples/http_infer_binary_resnet.py) automatically converts given images to this format.
 
 If the request contains only one input `binary_data_size` parameter can be omitted - in this case whole buffer is treated as a input image.
 
@@ -48,8 +48,8 @@ For the Raw Data binary inputs `binary_data_size` parameter can be omitted since
 
 ## Usage examples
 
-Sample clients that use binary inputs via KFS API can be found here ([REST sample](https://github.com/openvinotoolkit/model_server/blob/main/client/python/kserve-api/samples/http_infer_binary_resnet.py))/([GRPC sample](https://github.com/openvinotoolkit/model_server/blob/main/client/python/kserve-api/samples/grpc_infer_binary_resnet.py))
-Also, see the ([README](https://github.com/openvinotoolkit/model_server/blob/main/client/python/kserve-api/samples/README.md))
+Sample clients that use binary inputs via KFS API can be found here ([REST sample](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/kserve-api/samples/http_infer_binary_resnet.py))/([GRPC sample](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/kserve-api/samples/grpc_infer_binary_resnet.py))
+Also, see the ([README](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/kserve-api/samples/README.md))
 
 
 ## Recommendations:
diff --git a/docs/binary_input_tfs.md b/docs/binary_input_tfs.md
index 285f0a37..0eba00f0 100644
--- a/docs/binary_input_tfs.md
+++ b/docs/binary_input_tfs.md
@@ -28,8 +28,8 @@ On the server side, the Base64 encoded data is decoded to raw binary and loaded
 
 ## Usage examples
 
-Sample clients that use binary inputs via TFS API can be found here ([REST sample](https://github.com/openvinotoolkit/model_server/blob/main/client/python/ovmsclient/samples/http_predict_binary_resnet.py))/([GRPC sample](https://github.com/openvinotoolkit/model_server/blob/main/client/python/ovmsclient/samples/grpc_predict_binary_resnet.py))
-Also, see the ([README](https://github.com/openvinotoolkit/model_server/blob/main/client/python/ovmsclient/samples/README.md))
+Sample clients that use binary inputs via TFS API can be found here ([REST sample](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/ovmsclient/samples/http_predict_binary_resnet.py))/([GRPC sample](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/ovmsclient/samples/grpc_predict_binary_resnet.py))
+Also, see the ([README](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/ovmsclient/samples/README.md))
 
 
 ## Recommendations:
diff --git a/docs/build_from_source.md b/docs/build_from_source.md
index fc5cadc7..57672f84 100644
--- a/docs/build_from_source.md
+++ b/docs/build_from_source.md
@@ -196,4 +196,4 @@ dist/ubuntu
 ```
 
 
-Read more detailed usage in [developer guide](https://github.com/openvinotoolkit/model_server/blob/main/docs/developer_guide.md).
\ No newline at end of file
+Read more detailed usage in [developer guide](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/developer_guide.md).
\ No newline at end of file
diff --git a/docs/clients_kfs.md b/docs/clients_kfs.md
index bd4fc6e5..ed2526a0 100644
--- a/docs/clients_kfs.md
+++ b/docs/clients_kfs.md
@@ -8,7 +8,7 @@ hidden:
 
 gRPC API <ovms_docs_grpc_api_kfs>
 RESTful API <ovms_docs_rest_api_kfs>
-Examples <https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples>
+Examples <https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples>
 ```
 
 ## Python Client
@@ -821,4 +821,4 @@ client.stop_stream()
 :::
 ::::
 
-For complete usage examples see [Kserve samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples).
+For complete usage examples see [Kserve samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples).
diff --git a/docs/clients_tfs.md b/docs/clients_tfs.md
index 73d42c0e..2a45c8bb 100644
--- a/docs/clients_tfs.md
+++ b/docs/clients_tfs.md
@@ -8,7 +8,7 @@ hidden:
 
 gRPC API <ovms_docs_grpc_api_tfs>
 RESTful API <ovms_docs_rest_api_tfs>
-Examples <https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md>
+Examples <https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md>
 ```
 
 ## Python Client
@@ -329,7 +329,7 @@ curl -X POST http://localhost:8000/v1/models/my_model:predict
 ::::
 
 
-For complete usage examples see [ovmsclient samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/ovmsclient/samples).
+For complete usage examples see [ovmsclient samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/ovmsclient/samples).
 
 ## C++ and Go Clients
 
diff --git a/docs/custom_model_loader.md b/docs/custom_model_loader.md
index b6213a57..1ed878dd 100644
--- a/docs/custom_model_loader.md
+++ b/docs/custom_model_loader.md
@@ -35,7 +35,7 @@ To enable a particular model to load using custom loader, add extra parameter in
 
 
 ### C++ API Interface for custom loader:
-A base class **CustomLoaderInterface** along with interface API is defined in [src/customloaderinterface.hpp](https://github.com/openvinotoolkit/model_server/blob/main/src/customloaderinterface.hpp)
+A base class **CustomLoaderInterface** along with interface API is defined in [src/customloaderinterface.hpp](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/customloaderinterface.hpp)
 
 Refer to this file  for API details. 
 
@@ -45,7 +45,7 @@ Derive the new custom loader class from base class **CustomLoaderInterface** and
 **CustomLoaderInterface* createCustomLoader**
 which allocates the new custom loader and returns a pointer to the base class.
 
-An example custom loader which reads files and returns required buffers to be loaded is implemented and provided as reference in **[src/example/SampleCustomLoader](https://github.com/openvinotoolkit/model_server/blob/main/src/example/SampleCustomLoader)**
+An example custom loader which reads files and returns required buffers to be loaded is implemented and provided as reference in **[src/example/SampleCustomLoader](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/example/SampleCustomLoader)**
 
 This custom loader is built with the model server build and available in the docker *openvino/model_server-build:latest*. The shared library can be either copied from this docker or built using makefile. An example Makefile is provided as  a reference in the directory.
 
@@ -89,7 +89,7 @@ chmod -R 755 ./model
 Step 4: Download the required Client Components
 
 ```bash
-curl --fail https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/python/client_utils.py -o client_utils.py https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/face_detection/python/face_detection.py -o face_detection.py https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/python/requirements.txt -o requirements.txt
+curl --fail https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/python/client_utils.py -o client_utils.py https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/face_detection/python/face_detection.py -o face_detection.py https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/python/requirements.txt -o requirements.txt
 
 pip3 install --upgrade pip
 pip3 install -r requirements.txt
@@ -99,7 +99,7 @@ pip3 install -r requirements.txt
 Step 5: Download Data for Inference
 
 ```bash
-curl --fail --create-dirs https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/people/people1.jpeg -o images/people1.jpeg
+curl --fail --create-dirs https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/static/images/people/people1.jpeg -o images/people1.jpeg
 ```
 
 Step 6: Prepare the config json.
diff --git a/docs/custom_node_development.md b/docs/custom_node_development.md
index 74144484..45c49353 100644
--- a/docs/custom_node_development.md
+++ b/docs/custom_node_development.md
@@ -11,7 +11,7 @@ developed in C++ or C to perform arbitrary data transformations.
 ## Custom Node API
 
 
-The custom node library must implement the API interface defined in [custom_node_interface.h](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_node_interface.h).
+The custom node library must implement the API interface defined in [custom_node_interface.h](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_node_interface.h).
 The interface is defined in `C` to simplify compatibility with various compilers. The library could use third party components
 linked statically or dynamically. OpenCV is a built in component in OVMS which could be used to perform manipulation on the image
 data. 
@@ -67,7 +67,7 @@ Note that during the function execution all the output data buffers need to be a
 the request processing is completed and returned to the user. The cleanup is triggered by calling the `release` function 
 which also needs to be implemented in the custom library.
 
-In some cases, dynamic allocation in `execute` call might be a performance bottleneck or cause memory fragmentation. Starting from 2022.1 release, it is possible to preallocate memory during DAG initialization and reuse it in subsequent inference requests. Refer to `initialize` and `deinitialize` functions below. Those can be used to implement preallocated memory pool. Example implementation can be seen in [custom node example source](https://github.com/openvinotoolkit/model_server/blob/main/src/custom_nodes/add_one/add_one.cpp#L141).
+In some cases, dynamic allocation in `execute` call might be a performance bottleneck or cause memory fragmentation. Starting from 2022.1 release, it is possible to preallocate memory during DAG initialization and reuse it in subsequent inference requests. Refer to `initialize` and `deinitialize` functions below. Those can be used to implement preallocated memory pool. Example implementation can be seen in [custom node example source](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/custom_nodes/add_one/add_one.cpp#L141).
 
 Execute function returns an integer value that defines the success (`0` value) or failure (other than 0). When the function 
 reports error, the pipeline execution is stopped and the error is returned to the user. 
@@ -123,8 +123,8 @@ Just add include statement like:
 #include "opencv2/core.hpp"
 ```
 
-## String support (deprecated)
-There are special consideration when handling in the custom nodes the input sent by the clients as string. Such data when received by the OVMS frontend, is automatically converted to a 2D array with shape [-1,-1].
+## String support
+There are special consideration when handling in the custom nodes the input sent by the clients as string. Such data when received by the OVMS frontend, is automatically converted to a 2D array with shape [-1,-1]. Example of custom node using this feature is our [Tokenizer](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/tokenizer). 
 
 ### inputs
 When strings are send to the custom node that has 2-dimensional shape and U8 precision OVMS, after receiving request containing such inputs converts them to the 2 dimensional U8 array of  shape [number of strings, length of the longest string + 1] with padding filled with zeros. For example batch of three strings ["String_123", "", "zebra"] would be converted to:
@@ -147,7 +147,7 @@ would be converted to ["String_123", "", "zebra"].
 ## Building
 
 Custom node library can be compiled using any tool. It is recommended to follow the example based 
-a docker container with all build dependencies included. It is described in this [Makefile](https://github.com/openvinotoolkit/model_server/blob/main/src/custom_nodes/Makefile).
+a docker container with all build dependencies included. It is described in this [Makefile](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/custom_nodes/Makefile).
 
 ## Testing 
 The recommended method for testing the custom library is via OVMS execution:
@@ -157,7 +157,7 @@ The recommended method for testing the custom library is via OVMS execution:
 - Submit a request to OVMS endpoint using a gRPC or REST client.
 - Analyse the logs on the OVMS server.
 
-For debugging steps, refer to the OVMS [developer guide](https://github.com/openvinotoolkit/model_server/blob/main/docs/developer_guide.md)
+For debugging steps, refer to the OVMS [developer guide](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/developer_guide.md)
 
 
 ## Built-in custom nodes
@@ -167,16 +167,16 @@ Below you can see the list of fully functional custom nodes embedded in the mode
 
 | Custom Node      | Location in the container |
 | :---        |    :----   |
-| [east-resnet50 OCR custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/east_ocr) | `/ovms/lib/custom_nodes/libcustom_node_east_ocr.so`|
-| [horizontal OCR custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/horizontal_ocr) | `/ovms/lib/custom_nodes/libcustom_node_horizontal_ocr.so`|
-| [model zoo intel object detection custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/model_zoo_intel_object_detection) | `/ovms/lib/custom_nodes/libcustom_node_model_zoo_intel_object_detection.so`|
-| [image transformation custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/image_transformation) | `/ovms/lib/custom_nodes/libcustom_node_image_transformation.so`|
-| [add one custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/add_one) | `/ovms/lib/custom_nodes/libcustom_node_add_one.so`|
-| [face blur custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/face_blur) | `/ovms/lib/custom_nodes/libcustom_node_face_blur.so`|
+| [east-resnet50 OCR custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/east_ocr) | `/ovms/lib/custom_nodes/libcustom_node_east_ocr.so`|
+| [horizontal OCR custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/horizontal_ocr) | `/ovms/lib/custom_nodes/libcustom_node_horizontal_ocr.so`|
+| [model zoo intel object detection custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/model_zoo_intel_object_detection) | `/ovms/lib/custom_nodes/libcustom_node_model_zoo_intel_object_detection.so`|
+| [image transformation custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/image_transformation) | `/ovms/lib/custom_nodes/libcustom_node_image_transformation.so`|
+| [add one custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/add_one) | `/ovms/lib/custom_nodes/libcustom_node_add_one.so`|
+| [face blur custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/face_blur) | `/ovms/lib/custom_nodes/libcustom_node_face_blur.so`|
 
 
 **Example:** 
-Including built-in [horizontal OCR custom node](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/horizontal_ocr) in the `config.json` would look like:
+Including built-in [horizontal OCR custom node](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/horizontal_ocr) in the `config.json` would look like:
 ```json
 ...
     "custom_node_library_config_list": [
@@ -191,8 +191,8 @@ Including built-in [horizontal OCR custom node](https://github.com/openvinotoolk
 The custom node is already available under this path. No need to build anything and mounting to the container. 
 
 Additional examples are included in the unit tests:
-- [node_add_sub.c](https://github.com/openvinotoolkit/model_server/tree/main/src/test/custom_nodes/node_add_sub.c)
-- [node_choose_maximum.cpp](https://github.com/openvinotoolkit/model_server/tree/main/src/test/custom_nodes/node_choose_maximum.cpp)
-- [node_missing_implementation.c](https://github.com/openvinotoolkit/model_server/tree/main/src/test/custom_nodes/node_missing_implementation.c)
-- [node_perform_different_operations.cpp](https://github.com/openvinotoolkit/model_server/tree/main/src/test/custom_nodes/node_perform_different_operations.cpp)
+- [node_add_sub.c](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/test/custom_nodes/node_add_sub.c)
+- [node_choose_maximum.cpp](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/test/custom_nodes/node_choose_maximum.cpp)
+- [node_missing_implementation.c](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/test/custom_nodes/node_missing_implementation.c)
+- [node_perform_different_operations.cpp](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/test/custom_nodes/node_perform_different_operations.cpp)
 
diff --git a/docs/dag_scheduler.md b/docs/dag_scheduler.md
index 14a88826..1083a477 100644
--- a/docs/dag_scheduler.md
+++ b/docs/dag_scheduler.md
@@ -44,7 +44,7 @@ There are two special kinds of nodes - Request and Response node. Both of them a
 ### Custom node type
 
 * custom - that node can be used to implement all operations on the data which can not be handled by the neural network model. It is represented by
-a C++ dynamic library implementing OVMS API defined in [custom_node_interface.h](https://github.com/openvinotoolkit/model_server/blob/main/src/custom_node_interface.h). Custom nodes can run the data
+a C++ dynamic library implementing OVMS API defined in [custom_node_interface.h](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/custom_node_interface.h). Custom nodes can run the data
 processing using OpenCV, which is included in OVMS, or include other third-party components. Custom node libraries are loaded into OVMS
  by adding their definition to the pipeline configuration. The configuration includes a path to the compiled binary with the `.so` extension.
 Custom nodes are not versioned, meaning one custom node library is bound to one name. To load another version, another name needs to be used.
diff --git a/docs/deploying_server.md b/docs/deploying_server.md
index 3d0933c5..06118bac 100644
--- a/docs/deploying_server.md
+++ b/docs/deploying_server.md
@@ -47,8 +47,8 @@ docker run -u $(id -u) -v $(pwd)/models:/models -p 9000:9000 openvino/model_serv
 ##### 2.2 Download input files: an image and a label mapping file
 
 ```bash
-wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/zebra.jpeg
-wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/python/classes.py
+wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/static/images/zebra.jpeg
+wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/python/classes.py
 ```
 
 ##### 2.3 Install the Python-based ovmsclient package
diff --git a/docs/dynamic_bs_auto_reload.md b/docs/dynamic_bs_auto_reload.md
index 45d8c629..2d97c31a 100644
--- a/docs/dynamic_bs_auto_reload.md
+++ b/docs/dynamic_bs_auto_reload.md
@@ -7,7 +7,7 @@ This guide shows how to configure a model to accept input data with different ba
 
 Enabling dynamic batch size via model reload is as simple as setting the `batch_size` parameter to `auto`. To configure and use the dynamic batch size, take advantage of:
 
-- An example client in Python [grpc_predict_resnet.py](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/grpc_predict_resnet.py) that can be used to request inference with the desired batch size.
+- An example client in Python [grpc_predict_resnet.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/grpc_predict_resnet.py) that can be used to request inference with the desired batch size.
 
 - A sample [resnet](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/resnet50-binary-0001/README.md) model.
 
diff --git a/docs/dynamic_bs_demultiplexer.md b/docs/dynamic_bs_demultiplexer.md
index d00a9754..ec8d70a6 100644
--- a/docs/dynamic_bs_demultiplexer.md
+++ b/docs/dynamic_bs_demultiplexer.md
@@ -9,7 +9,7 @@ More information about this feature can be found in [dynamic batch size in demul
 
 > **NOTE**: Only one dynamic demultiplexer (`demultiply_count` with value `-1`) can exist in the pipeline.
 
-- Example client in python [grpc_predict_resnet.py](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/grpc_predict_resnet.py) can be used to request the pipeline. Use `--dag-batch-size-auto` flag to add an additional dimension to the input shape which is required for demultiplexing feature.
+- Example client in python [grpc_predict_resnet.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/grpc_predict_resnet.py) can be used to request the pipeline. Use `--dag-batch-size-auto` flag to add an additional dimension to the input shape which is required for demultiplexing feature.
 
 - The example uses model [resnet](https://github.com/openvinotoolkit/open_model_zoo/blob/2022.1.0/models/intel/resnet50-binary-0001/README.md).
 
diff --git a/docs/dynamic_input.md b/docs/dynamic_input.md
index f717683d..b45e33b1 100644
--- a/docs/dynamic_input.md
+++ b/docs/dynamic_input.md
@@ -43,5 +43,5 @@ OpenVINO Model Server accepts several data types that can be handled on [MediaPi
 
 - Next node in the graph uses a calculator that can decode raw KServe request. In such case dynamic input handling must be implemented as part of the calculator logic since model server passes the request to the calculator as-is. Such node expects input stream with a tag starting with `REQUEST` prefix.
 
-- Next node in the graph uses `PythonExecutorCalculator`. In such case data in the KServe request will be available to the user as input argument of their Python [execute function](https://github.com/openvinotoolkit/model_server/blob/main/docs/python_support/reference.md#ovmspythonmodel-class). Such node expects input stream with a tag starting with `OVMS_PY_TENSOR` prefix.
+- Next node in the graph uses `PythonExecutorCalculator`. In such case data in the KServe request will be available to the user as input argument of their Python [execute function](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/python_support/reference.md#ovmspythonmodel-class). Such node expects input stream with a tag starting with `OVMS_PY_TENSOR` prefix.
 
diff --git a/docs/dynamic_shape_auto_reload.md b/docs/dynamic_shape_auto_reload.md
index 552e000e..fc13f498 100644
--- a/docs/dynamic_shape_auto_reload.md
+++ b/docs/dynamic_shape_auto_reload.md
@@ -7,7 +7,7 @@ This guide explains how to configure a model to accept input data in different s
 
 Enable dynamic shape via model reloading by setting the `shape` parameter to `auto`. To configure and use the dynamic batch size, take advantage of:
 
-- Example client in Python [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/face_detection/python/face_detection.py) that can be used to request inference with the desired input shape.
+- Example client in Python [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/face_detection/python/face_detection.py) that can be used to request inference with the desired input shape.
 
 - An example [face_detection_retail_0004](https://docs.openvinotoolkit.org/2021.4/omz_models_model_face_detection_retail_0004.html) model.
 
diff --git a/docs/dynamic_shape_binary_inputs.md b/docs/dynamic_shape_binary_inputs.md
index d8618a98..77d44061 100644
--- a/docs/dynamic_shape_binary_inputs.md
+++ b/docs/dynamic_shape_binary_inputs.md
@@ -36,9 +36,9 @@ pip3 install ovmsclient
 
 ### Download a Sample Image and Label Mappings
 ```bash
-wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/zebra.jpeg
+wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/static/images/zebra.jpeg
 
-wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/python/classes.py
+wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/python/classes.py
 ```
 
 ### Run Inference
diff --git a/docs/dynamic_shape_custom_node.md b/docs/dynamic_shape_custom_node.md
index 05b25d71..f2f186ae 100644
--- a/docs/dynamic_shape_custom_node.md
+++ b/docs/dynamic_shape_custom_node.md
@@ -3,12 +3,12 @@
 ## Introduction
 This guide shows how to configure a simple Directed Acyclic Graph (DAG) with a custom node that performs input resizing before passing input data to the model. 
 
-The node below is provided as a demonstration. See instructions for how to build and use the custom node: [Image Transformation](https://github.com/openvinotoolkit/model_server/tree/main/src/custom_nodes/image_transformation).
+The node below is provided as a demonstration. See instructions for how to build and use the custom node: [Image Transformation](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/image_transformation).
 
 
 To run inference with this setup, we will use the following:
 
-- Example client in Python [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/face_detection/python/face_detection.py) that can be used to request inference on with the desired input shape.
+- Example client in Python [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/face_detection/python/face_detection.py) that can be used to request inference on with the desired input shape.
 
 - An example [face_detection_retail_0004](https://docs.openvinotoolkit.org/2021.4/omz_models_model_face_detection_retail_0004.html) model.
 
diff --git a/docs/dynamic_shape_dynamic_model.md b/docs/dynamic_shape_dynamic_model.md
index f98506ac..cba7ed1b 100644
--- a/docs/dynamic_shape_dynamic_model.md
+++ b/docs/dynamic_shape_dynamic_model.md
@@ -14,7 +14,7 @@ Another option to use dynamic shape feature is to export the model with dynamic
 
 To the demonstrate dynamic dimensions, take advantage of:
 
-- Example client in Python [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/main/demos/face_detection/python/face_detection.py) that can be used to request inference with the desired input shape.
+- Example client in Python [face_detection.py](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/face_detection/python/face_detection.py) that can be used to request inference with the desired input shape.
 
 - An example [face_detection_retail_0004](https://docs.openvinotoolkit.org/2021.4/omz_models_model_face_detection_retail_0004.html) model.
 
diff --git a/docs/mediapipe.md b/docs/mediapipe.md
index 02d49c73..0b237d46 100644
--- a/docs/mediapipe.md
+++ b/docs/mediapipe.md
@@ -12,7 +12,7 @@ MediaPipe is an open-source framework for building pipelines to perform inferenc
 
 Thanks to the integration between MediaPipe and OpenVINO Model Server, the graphs can be exposed over the network and the complete load can be delegated to a remote host or a microservice.
 We support the following scenarios:
-- stateless execution via unary to unary gRPC calls
+- stateless execution via unary to unary gRPC/REST calls
 - stateful graph execution via [gRPC streaming sessions](./streaming_endpoints.md).
 
 With the introduction of OpenVINO calculator it is possible to optimize inference execution in the OpenVINO Runtime backend. This calculator can be applied both in the graphs deployed inside the Model Server but also in the standalone applications using the MediaPipe framework.
@@ -79,10 +79,10 @@ The required data layout for the MediaPipe `IMAGE` conversion is HWC and the sup
 |UINT16|1,3,4|
 |INT16|1,3,4|
 
-> **Note**: Input serialization to MediaPipe ImageFrame format, requires the data in the KServe request to be encapsulated in `raw_input_contents` field based on [KServe API](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/grpc_predict_v2.proto). That is the default behavior in the client libs like `triton-client`.
+> **Note**: Input serialization to MediaPipe ImageFrame format, requires the data in the KServe request to be encapsulated in `raw_input_contents` field based on [KServe API GRPC](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/grpc_predict_v2.proto) or in binary extension of based on [KServe API REST](./binary_input_kfs.md#http). That is the default behavior in the client libs like `triton-client`.
 
-When the client is sending in the gRPC request the input as an numpy array, it will be deserialized on the Model Server side to the format specified in the graph.
-For example when the graph has the input type IMAGE, the gRPC client could send the input data with the shape `(300, 300, 3)` and precision INT8. It would not be allowed to send the data in the shape for example `(1,300,300,1)` as that would be incorrect layout and the number of dimensions.
+When the client is sending in the gRPC/REST request the input as an numpy array, it will be deserialized on the Model Server side to the format specified in the graph.
+For example when the graph has the input type IMAGE, the gRPC/REST client could send the input data with the shape `(300, 300, 3)` and precision INT8. It would not be allowed to send the data in the shape for example `(1,300,300,1)` as that would be incorrect layout and the number of dimensions.
 
 When the input graph would be set as `OVTENSOR`, any shape and precisions of the input would be allowed. It will be converted to `ov::Tensor` object and passed to the graph. For example input can have shape `(1,3,300,300)` and precision `FP32`. If passed tensor would not be accepted by model, calculator and graph will return error.
 
@@ -94,7 +94,7 @@ There is also an option to avoid any data conversions in the serialization and d
 
 ### Side packets
 Side packets are special parameters which can be passed to the calculators at the beginning of the graph initialization. It can tune the behavior of the calculator like set the object detection threshold or number of objects to process.
-With KServe gRPC API you are also able to push side input packets into graph. They are to be passed as KServe request parameters. They can be of type `string`, `int64` or `boolean`.
+With KServe API you are also able to push side input packets into graph. They are to be passed as KServe request parameters. They can be of type `string`, `int64` or `boolean`.
 Note that with the gRPC stream connection, only the first request in the stream can include the side package parameters. On the client side, the snippet below illustrates how it can be defined:
 ```python
 client.async_stream_infer(
@@ -207,7 +207,7 @@ It can generate the load to gRPC stream and the mediapipe graph based on the con
 
 ## Using MediaPipe graphs from the remote client
 
-MediaPipe graphs can use the same gRPC KServe Inference API both for the unary calls and the streaming.
+MediaPipe graphs can use the same gRPC/REST KServe Inference API both for the unary calls and the streaming.
 The same client libraries with KServe API support can be used in both cases. The client code for the unary and streaming is different.
 Check the [code snippets](https://docs.openvino.ai/2024/ovms_docs_clients_kfs.html)
 
@@ -223,8 +223,8 @@ the version parameter is ignored. MediaPipe graphs are not versioned. Though, th
 MediaPipe graphs can include only the calculators built-in the model server image.
 If you want to add your own mediapipe calculator to OpenVINO Model Server functionality you need to add it as a dependency and rebuild the OpenVINO Model Server binary.
 
-If you have it in external repository, you need to add the http_archive() definition or git_repository() definition to the bazel [WORKSPACE](https://github.com/openvinotoolkit/model_server/blob/main/WORKSPACE) file.
-Then you need to add the calculator target as a bazel dependency to the [src/BUILD](https://github.com/openvinotoolkit/model_server/blob/main/src/BUILD) file. This should be done for:
+If you have it in external repository, you need to add the http_archive() definition or git_repository() definition to the bazel [WORKSPACE](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/WORKSPACE) file.
+Then you need to add the calculator target as a bazel dependency to the [src/BUILD](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/BUILD) file. This should be done for:
 
 ```
 cc_library(
@@ -261,7 +261,7 @@ in the conditions:default section of the deps property:
 
 
 ## Current limitations
-- MediaPipe graphs are supported only for gRPC KServe API.
+- Inputs of type string are supported only for inputs tagged as OVMS_PY_TENSOR.
 
 - KServe ModelMetadata call response contains only input and output names. In the response, shapes will be empty and datatypes will be `"INVALID"`.
 
diff --git a/docs/mediapipe_conversion.md b/docs/mediapipe_conversion.md
index 4ff0cee9..b5fb06bd 100644
--- a/docs/mediapipe_conversion.md
+++ b/docs/mediapipe_conversion.md
@@ -190,7 +190,7 @@ input_order_list: ["Identity","Identity_1","Identity_2","Identity_3"]
 ### 3. Adjust graph input/output streams
 
 This step is required if you plan to deploy the graph in OpenVINO Model Server and existing graph does not have supported input/output packet types. Check for supported input and output packet types [here](./mediapipe.md).
-In that cases you may need to add converter calculators as it was done [here](https://github.com/openvinotoolkit/model_server/blob/main/demos/mediapipe/object_detection/graph.pbtxt#L31).
+In that cases you may need to add converter calculators as it was done [here](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/mediapipe/object_detection/graph.pbtxt#L31).
 
 ### 4. Set the config.json file path in the session calculator
 
diff --git a/docs/metrics.md b/docs/metrics.md
index 0995b401..bfb528fd 100644
--- a/docs/metrics.md
+++ b/docs/metrics.md
@@ -201,7 +201,7 @@ To use data from metrics endpoint you can use the curl command:
 ```bash
 curl http://localhost:8000/metrics
 ```
-[Example metrics output](https://raw.githubusercontent.com/openvinotoolkit/model_server/main/docs/metrics_output.out)
+[Example metrics output](https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/docs/metrics_output.out)
 
 ## Performance considerations
 Collecting metrics has negligible performance overhead when used with models of average size and complexity. However when used with very lightweight, fast models which inference time is very short, the metric incrementation can take noticeable proportion of the processing time. Consider it while enabling metrics for such models.
@@ -230,7 +230,7 @@ For [MediaPipe Graphs](./mediapipe.md) metrics endpoint is not supported.
 
 With server metrics being scraped by [Prometheus](https://prometheus.io/) it is possible to integrate [Grafana](https://grafana.com/) to visualize them on the dashboards. Once you have Grafana configured with Prometheus as a data source, you can create your own dashboard or import one. 
 
-In OpenVINO Model Server repository you can find [grafana_dashboard.json](https://github.com/openvinotoolkit/model_server/blob/main/extras/grafana_dashboard.json) file that can be used to visualize per model metrics like:
+In OpenVINO Model Server repository you can find [grafana_dashboard.json](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/extras/grafana_dashboard.json) file that can be used to visualize per model metrics like:
 - Throughput [RPS] - number of requests being processed by the model per second.
 - Mean Latency [ms] - latency averaged across all requests processed by the model in a certain timeframe.
 - Latency Quantile [ms] - value of latency for quantiles [0.75, 0.90, 0.99], meaning the latency that has NOT been exceeded by 75%, 90% and 99% of the requests.
diff --git a/docs/model_server_c_api.md b/docs/model_server_c_api.md
index 1b433f5c..cb030be5 100644
--- a/docs/model_server_c_api.md
+++ b/docs/model_server_c_api.md
@@ -19,7 +19,7 @@ With OpenVINO Model Server 2023.1 release C-API is no longer in preview state an
 
 ## API Description
 
-Server functionalities are encapsulated in shared library built from OpenVINO Model Server source. To include OpenVINO Model Server you need to link this library with your application and use C API defined in [header file](https://github.com/openvinotoolkit/model_server/blob/main/src/ovms.h). 
+Server functionalities are encapsulated in shared library built from OpenVINO Model Server source. To include OpenVINO Model Server you need to link this library with your application and use C API defined in [header file](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/src/ovms.h). 
 
 
 Calling a method to start the model serving in your application initiates the OpenVINO Model Server as a separate thread. Then you can schedule inference both directly from app using C API and gRPC/HTTP endpoints.
diff --git a/docs/model_server_grpc_api_kfs.md b/docs/model_server_grpc_api_kfs.md
index 2ed4b039..01a5f397 100644
--- a/docs/model_server_grpc_api_kfs.md
+++ b/docs/model_server_grpc_api_kfs.md
@@ -13,7 +13,7 @@ The API includes following endpoints:
 * [Inference API](#inference-api)
 * [Streaming Inference API](#streaming-inference-api-extension)
 
-> **NOTE**: Examples of using each of above endpoints can be found in [KServe samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples/README.md).
+> **NOTE**: Examples of using each of above endpoints can be found in [KServe samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples/README.md).
 
 
 ## Server Live API
@@ -57,7 +57,7 @@ Check documentation for more [details](./streaming_endpoints.md).
 
 ## See Also
 
-- [Example client code](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples/README.md) shows how to use GRPC API and REST API.
+- [Example client code](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples/README.md) shows how to use GRPC API and REST API.
 - [KServe API](https://github.com/kserve/kserve/tree/master/docs/predict-api/v2)
 - [gRPC](https://grpc.io/)
 
diff --git a/docs/model_server_grpc_api_tfs.md b/docs/model_server_grpc_api_tfs.md
index 73c0a91f..5eff2e1c 100644
--- a/docs/model_server_grpc_api_tfs.md
+++ b/docs/model_server_grpc_api_tfs.md
@@ -18,7 +18,7 @@ Gets information about the status of served models including Model Version
 
  [Get Model Status proto](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/get_model_status.proto) defines three message definitions used while calling Status endpoint: *GetModelStatusRequest*, *ModelVersionStatus*, *GetModelStatusResponse* that are used to report all exposed versions including their state in their lifecycle.
 
- Read more about [Get Model Status API usage](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md#model-status-api).
+ Read more about [Get Model Status API usage](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md#model-status-api).
 
 
 ## Model Metadata API
@@ -27,7 +27,7 @@ Gets information about the served models. A function called GetModelMetadata acc
 
 [Get Model Metadata proto](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/get_model_metadata.proto) has three message definitions: *SignatureDefMap*, *GetModelMetadataRequest*, *GetModelMetadataResponse*.
 
-Read more about [Get Model Metadata API usage](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md#model-metadata-api).
+Read more about [Get Model Metadata API usage](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md#model-metadata-api).
 
 
 ## Predict API
@@ -40,13 +40,13 @@ Endpoint for running an inference with loaded models or [DAGs](./dag_scheduler.m
  * *PredictResponse* includes a map of outputs serialized by
 [TensorProto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.proto) and information about the used model spec.
 
-Read more about [Predict API usage](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md#predict-api)
+Read more about [Predict API usage](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md#predict-api)
 
 Also, using `string_val` field it is possible to send binary encoded images that would be preprocessed by OVMS using opencv and converted to OpenVINO-friendly format. For more information check [how binary data is handled in OpenVINO Model Server](./binary_input_tfs.md)
 
 ## See Also
 
-- [Example client code](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md) shows how to use GRPC API and REST API.
+- [Example client code](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md) shows how to use GRPC API and REST API.
 - [TensorFlow Serving](https://github.com/tensorflow/serving)
 - [gRPC](https://grpc.io/)
 
diff --git a/docs/model_server_rest_api_kfs.md b/docs/model_server_rest_api_kfs.md
index 06a5e87a..4f5adc77 100644
--- a/docs/model_server_rest_api_kfs.md
+++ b/docs/model_server_rest_api_kfs.md
@@ -36,7 +36,7 @@ Date: Tue, 09 Aug 2022 09:20:24 GMT
 Content-Length: 2
 ```
 
-See also [code samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples) for getting server liveness with KServe API on HTTP Server Live endpoint.
+See also [code samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples) for getting server liveness with KServe API on HTTP Server Live endpoint.
 
 ## Server Ready API
 **Description**
@@ -63,7 +63,7 @@ Date: Tue, 09 Aug 2022 09:22:14 GMT
 Content-Length: 2
 ```
 
-See also [code samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples) for getting server readiness with KServe API on HTTP Server Ready endpoint.
+See also [code samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples) for getting server readiness with KServe API on HTTP Server Ready endpoint.
 
 ## Server Metadata API
 **Description**
@@ -103,7 +103,7 @@ $ curl http://localhost:5000/v2
 
 For detailed description of the response contents see [KServe API docs](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#server-metadata).
 
-See also [code samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples) for getting server metadata with KServe API on HTTP Server Metadata endpoint.
+See also [code samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples) for getting server metadata with KServe API on HTTP Server Metadata endpoint.
 
 ## Model Ready API
 **Description**
@@ -130,7 +130,7 @@ Date: Tue, 09 Aug 2022 09:25:31 GMT
 Content-Length: 2
 ```
 
-See also [code samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples) for getting model readiness with KServe API on HTTP Model Ready endpoint.
+See also [code samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples) for getting model readiness with KServe API on HTTP Model Ready endpoint.
 
 
 
@@ -185,7 +185,7 @@ $ curl http://localhost:8000/v2/models/resnet
 
 For detailed description of the response contents see [KServe API docs](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#model-metadata).
 
-See also [code samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples) for running getting model metadata with KServe API on HTTP Model Metadata endpoint.
+See also [code samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples) for running getting model metadata with KServe API on HTTP Model Metadata endpoint.
 
 ## Inference API
 **Description**
@@ -352,4 +352,4 @@ For detailed description of request and response contents see [KServe API docs](
 
 > Note: Using //.. at the end of request URI results in truncated path, which might result in different response than expected.
 
-See also [code samples](https://github.com/openvinotoolkit/model_server/tree/main/client/python/kserve-api/samples) for running inference with KServe API on HTTP Inference endpoint.
+See also [code samples](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/client/python/kserve-api/samples) for running inference with KServe API on HTTP Inference endpoint.
diff --git a/docs/model_server_rest_api_tfs.md b/docs/model_server_rest_api_tfs.md
index 1666a5da..42f30091 100644
--- a/docs/model_server_rest_api_tfs.md
+++ b/docs/model_server_rest_api_tfs.md
@@ -65,7 +65,7 @@ $ curl http://localhost:8001/v1/models/person-detection/versions/1
   ]
 }
 ```
-Read more about [Get Model Status API usage](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md#model-status-api-1)
+Read more about [Get Model Status API usage](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md#model-status-api-1)
 
 ## Model Metadata API
 **Description**
@@ -148,7 +148,7 @@ $ curl http://localhost:8001/v1/models/person-detection/versions/1/metadata
 }
 ```
 
-Read more about [Get Model Metadata API usage](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md#model-metadata-api-1)
+Read more about [Get Model Metadata API usage](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md#model-metadata-api-1)
 
 ## Predict API
 **Description**
@@ -212,7 +212,7 @@ On the server side, the binary encoded data is loaded using OpenCV which then co
 
 Check [how binary data is handled in OpenVINO Model Server](./binary_input.md) for more informations.
 
-Read more about [Predict API usage](https://github.com/openvinotoolkit/model_server/blob/main/client/python/tensorflow-serving-api/samples/README.md#predict-api-1)
+Read more about [Predict API usage](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/client/python/tensorflow-serving-api/samples/README.md#predict-api-1)
 
 ## Config Reload API
 **Description**
diff --git a/docs/ovms_quickstart.md b/docs/ovms_quickstart.md
index a7dc336c..bb24d0e3 100644
--- a/docs/ovms_quickstart.md
+++ b/docs/ovms_quickstart.md
@@ -79,8 +79,8 @@ During this step, the `model` folder is mounted to the Docker container.  This f
 Client scripts are available for quick access to the Model Server. Run an example command to download all required components:
 
 ```bash
-wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/object_detection/python/object_detection.py
-wget https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/object_detection/python/requirements.txt
+wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/object_detection/python/object_detection.py
+wget https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/object_detection/python/requirements.txt
 wget https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/data/dataset_classes/coco_91cl.txt
 ```
 
diff --git a/docs/python_support/reference.md b/docs/python_support/reference.md
index 157dbda1..c7ac8eed 100644
--- a/docs/python_support/reference.md
+++ b/docs/python_support/reference.md
@@ -6,11 +6,11 @@
 
  Starting with version 2023.3, OpenVINO Model Server supports execution of custom Python code. Such code can execute simple pre- or post-processing as well as complex tasks like image or text generation.
 
- Python execution is enabled via [MediaPipe](../mediapipe.md) by the built-in [`PythonExecutorCalculator`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#pythonexecutorcalculator) that allows creating graph nodes to execute Python code. Python nodes can be used as standalone servables (single node graphs) or be part of larger MediaPipe graphs.
+ Python execution is enabled via [MediaPipe](../mediapipe.md) by the built-in [`PythonExecutorCalculator`](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#pythonexecutorcalculator) that allows creating graph nodes to execute Python code. Python nodes can be used as standalone servables (single node graphs) or be part of larger MediaPipe graphs.
 
  Check out the [quickstart guide](quickstart.md) for a simple example that shows how to use this feature.
 
- Check out [Generative AI demos](https://docs.openvino.ai/nightly/ovms_docs_demos.html#check-out-new-generative-ai-demos) for real life use cases.
+ Check out [Generative AI demos](https://docs.openvino.ai/2024/ovms_docs_demos.html#check-out-new-generative-ai-demos) for real life use cases.
 
  ## Building Docker Image
 
@@ -27,7 +27,7 @@ RUN pip3 install numpy
 ENTRYPOINT [ `/ovms/bin/ovms` ]
 ```
 
-You can also modify `requirements.txt` from our [python demos](https://github.com/openvinotoolkit/model_server/tree/main/demos/python_demos) and from repository top level directory run `make python_image`
+You can also modify `requirements.txt` from our [python demos](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/python_demos) and from repository top level directory run `make python_image`
 
 ## `OvmsPythonModel` class
 
@@ -100,7 +100,7 @@ class OvmsPythonModel:
 
 `initialize` is called when model server loads graph definition. It allows to initialize and maintain state between subsequent `execute` calls and even graph instances.
 
-For gRPC unary, graphs are recreated per request.
+For unary endpoint, graphs are recreated per request.
 
 For gRPC streaming, there can be multiple graph instances existing at the same time.
 
@@ -109,7 +109,7 @@ For gRPC streaming, there can be multiple graph instances existing at the same t
 #### Parameters and return value
 
 `initialize` is called with `kwargs` parameter which is a dictionary.
-`kwargs` contain information from [node configuration](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#pythonexecutorcalculator). Considering a sample:
+`kwargs` contain information from [node configuration](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#pythonexecutorcalculator). Considering a sample:
 
 ```pbtxt
 node {
@@ -158,7 +158,7 @@ def execute(self, inputs):
     return outputs
 ```
 
-More information along with the configuration aspect described can be found in [execution modes](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#execution-modes) section.
+More information along with the configuration aspect described can be found in [execution modes](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#execution-modes) section.
 
 #### Generative
 
@@ -172,7 +172,7 @@ def execute(self, inputs):
         yield outputs
 ```
 
-More information along with the configuration aspect described can be found in [execution modes](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#execution-modes) section.
+More information along with the configuration aspect described can be found in [execution modes](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#execution-modes) section.
 
 #### Parameters and return value
 
@@ -191,7 +191,7 @@ Note that this method returns outputs as a list, but since each output is a sepa
 
 - For unary endpoints model server gathers all outputs from the graph and sends them all together in a single response
 
-- For streaming endpoints model server packs output and sends it in the response as soon as it arrives. It means that if `execute` returns a list of `X` outputs, the client will receive those outputs in `X` separate responses. The outputs can then be [gathered using timestamp](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#outputs-synchronization-in-grpc-streaming) that can be found in received responses.
+- For streaming endpoints model server packs output and sends it in the response as soon as it arrives. It means that if `execute` returns a list of `X` outputs, the client will receive those outputs in `X` separate responses. The outputs can then be [gathered using timestamp](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#outputs-synchronization-in-grpc-streaming) that can be found in received responses.
 
 #### Error handling
 
@@ -199,7 +199,7 @@ Signaling that something went wrong should be done by throwing an exception.
 The exception is caught by the `PythonExecutorCalculator` which logs it and returns non-OK status.
 Model Server then reads that status and sets graph in an error state. Then it closes all graph's input streams and waits until in-progress actions are finished. Once it's done the graph gets removed.
 
-This behavior has different effect on the client depending on the kind of gRPC endpoint used - unary or streaming:
+This behavior has different effect on the client depending on the kind of endpoint used - unary or streaming:
 
 - **Unary**
 
@@ -256,7 +256,7 @@ This `Tensor` class is a C++ class with a Python binding that implements Python
 
 *Note*: `datatype` attribute is not part of buffer protocol implementation.
 Buffer protocol uses `format` value that uses [struct format characters](https://docs.python.org/3/library/struct.html#format-characters). It can be read from `data` memoryview.
-There's a mapping between those two - see [datatype considerations](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#datatype-considerations).
+There's a mapping between those two - see [datatype considerations](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#datatype-considerations).
 
 As `pyovms.Tensor` implements buffer protocol it can be converted to another types that also implement buffer protocol:
 
@@ -274,7 +274,7 @@ Inputs will be provided to the `execute` function, but outputs must be prepared
 
 `Tensor(name, data, shape=None, datatype=None)`
 
-- `name`: a string that associates Tensor data with specific name. This name is also used by `PythonExecutorCalculator` to push data to the correct output stream in the node. More about it in [node configuration section](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#input-and-output-streams-in-python-code).
+- `name`: a string that associates Tensor data with specific name. This name is also used by `PythonExecutorCalculator` to push data to the correct output stream in the node. More about it in [node configuration section](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#input-and-output-streams-in-python-code).
 
 - `data`: an object that implements Python [Buffer Protocol](https://docs.python.org/3/c-api/buffer.html#buffer-protocol). This could be an instance of some built-in type like `bytes` or types from external modules like `numpy.ndarray`.
 
@@ -304,7 +304,7 @@ class OvmsPythonModel:
 
 As `Tensor` gets created from another type it adapts all fields required by the buffer protocol as its own.
 Depending on how `Tensor` is created `shape` or `datatype` may be overridden.
-If they are not provided `Tensor` will adapt another buffer `shape` as it's own and will map it's `format` to a `datatype`. Learn more in [datatype considerations](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#datatype-considerations) section.
+If they are not provided `Tensor` will adapt another buffer `shape` as it's own and will map it's `format` to a `datatype`. Learn more in [datatype considerations](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#datatype-considerations) section.
 
 If the node is connected to another Python node, then Tensors pushed to the output of this node, are inputs of another node.
 
@@ -314,7 +314,7 @@ There are two places where `pyovms.Tensor` objects are created and accessed:
 - in `execute` method of `OvmsPythonModel` class
 - in model server core during serialization and deserialization if Python node inputs or outputs as also graph inputs or outputs
 
-Model Server receives requests and sends responses on gRPC interface via KServe API which defines [expected data types for tensors](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#tensor-data-types).
+Model Server receives requests and sends responses on interface via KServe API which defines [expected data types for tensors](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#tensor-data-types).
 On the other hand Python [Buffer Protocol](https://docs.python.org/3/c-api/buffer.html#buffer-protocol) requires `format` to be specified as [struct format characters](https://docs.python.org/3/library/struct.html#format-characters).
 
 In order to let users work with KServe types without enforcing the usage of struct format characters on the client side, model server attempts to do the mapping as follows when creating `Tensor` objects from the request:
@@ -341,6 +341,15 @@ The same mapping is applied the other way around when creating `Tensor` from ano
 
 In some cases, users may work with more complex types that are not listed above and model server also allows that.
 
+#### BYTES datatype
+If `datatype` "BYTES" is specified and data is located in bytes_contents field of input(for gRPC) or in JSON body(for REST) OVMS converts it to `pyovms.Tensor` buffer according to the format where every input is preceeded by four bytes of its size.
+
+For example this gRPC request:
+ bytes_content: [<240 byte element>, <1024 byte element>, <567 byte element>]
+ 
+would be converted to this pyovms.Tensor.data contents:
+| 240 |   < first element>  | 1024 |   <second element> | 567 | <third element> |
+
 #### Custom types
 
 The `datatype` field in the tensor is a `string` and model server will not reject datatype that is not among above KServe types. If some custom type is defined in the request and server cannot map it to a format character it will translate it to `B` treating it as a 1-D raw binary buffer. For consistency the shape of the underlying buffer in that case will also differ from the shape defined in the request. Let's see it on an example:
@@ -468,7 +477,7 @@ class OvmsPythonModel:
         ...
 ```
 
-**Note**: Node configuration and `execute` implementation should always match. For example if the node is configured to work with [incomplete inputs](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#incomplete-inputs), then accessing `Tensors` via index will not be useful.
+**Note**: Node configuration and `execute` implementation should always match. For example if the node is configured to work with [incomplete inputs](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#incomplete-inputs), then accessing `Tensors` via index will not be useful.
 
 ### Graph input and output streams
 
@@ -590,23 +599,24 @@ Where `name` defines the name of the graph and `graph_path` contains the path to
 
 ### Inference API and available endpoints
 
-Since Python execution is supported via MediaPipe serving flow, it inherits it's enhancements and limitations. First thing to note is that MediaPipe graphs are available **only via KServe API**.
+Since Python execution is supported via MediaPipe serving flow, it inherits it's enhancements and limitations. First thing to note is that MediaPipe graphs are available [**only via KServe API**](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md)
 
 From the client perspective model server serves a graph and user interacts with a graph. Single node in the graph cannot be accessed from the outside.
 
 For a graph client can:
 
-- request status (gRPC and REST)
-- request metadata (gRPC and REST)
-- request inference (gRPC)
+- request status
+- request metadata
+- request inference
 
 Learn more about how [MediaPipe flow works in OpenVINO Model Server](../mediapipe.md)
 
-For inference, if the format of graph input stream is [`OvmsPyTensor`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#graph-input-and-output-streams), then the data in the KServe request must be encapsulated in either `ModelInferRequest`'s [InferTensorContents](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/grpc_predict_v2.proto#L155) or [raw_input_contents](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/grpc_predict_v2.proto#L202). If the graph has a `OvmsPyTensor` output stream, then the data in the KServe response can be found in `raw_output_contents` field (even if data in the request has been placed in `InferTensorContents`).
+For inference, data can be send both via [gRPC API](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#grpc) and [KServe API](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#httprest)(only for unary calls). If the graph has a `OvmsPyTensor` output stream, then the data in the KServe response can be found in `raw_output_contents` field (even if data in the request has been placed in `InferTensorContents`).
 
-The data passed in the request is accessible in `execute` method of the node connected to graph input via `data` attribute of [`pyovms.Tensor`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#python-tensor) object.
+The data passed in the request is accessible in `execute` method of the node connected to graph input via `data` attribute of [`pyovms.Tensor`](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#python-tensor) object.
+For data of type BYTES send in bytes_contents field of input(for gRPC) or in JSON body(for REST) OVMS converts it to `pyovms.Tensor` buffer according to the format where every input is preceeded by four bytes of its size.
 
-Inputs and outputs also define `shape` and `datatype` parameters. Those values are also accessible in `pyovms.Tensor`. However for outputs, you don't provide those values directly to the response. See [datatype considerations](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#datatype-considerations).
+Inputs and outputs also define `shape` and `datatype` parameters. Those values are also accessible in `pyovms.Tensor`. For outputs, `datatype` and `shape` are by default read from the underlying buffer, but it is possible to overwrite them (see [`pyovms.Tensor constructor`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#creating-output-tensors). If you specify `datatype` as `BYTES` in your requests, make sure to review [datatype considerations](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#datatype-considerations), since this type is treated differently than the others.
 
 Let's see it on an example:
 
@@ -672,11 +682,11 @@ class OvmsPythonModel:
 
 Mediapipe graph works with packets and every packet has its timestamp. The timestamps of packets on all streams (both input and output) must be ascending.
 
-When requesting inference, user can decide to use automatic timestamping, or send timestamps themself along with the request as `OVMS_MP_TIMESTAMP` parameter. Learn more about [timestamping](https://docs.openvino.ai/nightly/ovms_docs_streaming_endpoints.html#timestamping)
+When requesting inference, user can decide to use automatic timestamping, or send timestamps themself along with the request as `OVMS_MP_TIMESTAMP` parameter. Learn more about [timestamping](https://docs.openvino.ai/2024/ovms_docs_streaming_endpoints.html#timestamping)
 
 When it comes to Python node `PythonExecutorCalculator`:
-- for [regular execution mode](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#regular-mode) simply propagates timestamp i.e. uses input timestamp as output timestamp.
-- for [generative execution mode](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#generative-mode) it saves timestamp of the input and sends first set of outputs downstream with this timestamp. Then timestamp gets incremented with each generation, so next sets of output packages have ascending timestamp.
+- for [regular execution mode](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#regular-mode) simply propagates timestamp i.e. uses input timestamp as output timestamp.
+- for [generative execution mode](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#generative-mode) it saves timestamp of the input and sends first set of outputs downstream with this timestamp. Then timestamp gets incremented with each generation, so next sets of output packages have ascending timestamp.
 
 **Multiple generation cycles on a single graph instance**
 
@@ -701,7 +711,7 @@ timestamp = result.get_response().parameters["OVMS_MP_TIMESTAMP"].int64_param
 
 Python nodes can be configured to run in two execution modes - regular and generative.
 
-In regular execution mode the node produces one set of outputs per one set of inputs. It works via both gRPC unary and streaming endpoints and is a common mode for use cases like computer vision.
+In regular execution mode the node produces one set of outputs per one set of inputs. It works via both gRPC/REST unary and gRPC streaming endpoints and is a common mode for use cases like computer vision.
 
 In generative execution mode the node produces multiple sets of outputs over time per single set of inputs. It works only via gRPC streaming endpoints and is useful for use cases where total processing time is big and you want to return some intermediate results before the execution is completed. That mode is well suited to Large Language Models to serve them in a more interactive manner.
 
@@ -709,7 +719,7 @@ Depending on which mode is used, both the Python code and graph configuration mu
 
 #### Regular mode
 
-When using regular mode, the `execute` method in [`OvmsPythonModel`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#ovmspythonmodel-class) class must `return` value.
+When using regular mode, the `execute` method in [`OvmsPythonModel`](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#ovmspythonmodel-class) class must `return` value.
 
 ```python
 from pyovms import Tensor
@@ -720,7 +730,7 @@ from pyovms import Tensor
         return [my_output]
 ```
 
-When `execute` returns, the [`PythonExecutorCalculator`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#pythonexecutorcalculator) grabs the outputs and pushes them down the graph. Node `Process` method is called once per inputs set. Such implementation can be paired with basic graph setting, like:
+When `execute` returns, the [`PythonExecutorCalculator`](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#pythonexecutorcalculator) grabs the outputs and pushes them down the graph. Node `Process` method is called once per inputs set. Such implementation can be paired with basic graph setting, like:
 
 ```pbtxt
 node {
@@ -739,7 +749,7 @@ node {
 
 #### Generative mode
 
-When using generative mode, the `execute` method in [`OvmsPythonModel`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#ovmspythonmodel-class) class must `yield` value.
+When using generative mode, the `execute` method in [`OvmsPythonModel`](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#ovmspythonmodel-class) class must `yield` value.
 
 ```python
 from pyovms import Tensor
@@ -751,7 +761,7 @@ from pyovms import Tensor
           yield [my_output]
 ```
 
-When `execute` yields, the [`PythonExecutorCalculator`](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#pythonexecutorcalculator) saves the generator. Then it repeatedly calls it until it reaches the end of generated sequence. Node `Process` method is called multiple times per single inputs set. To trigger such behavior a specific graph configuration is needed. See below:
+When `execute` yields, the [`PythonExecutorCalculator`](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#pythonexecutorcalculator) saves the generator. Then it repeatedly calls it until it reaches the end of generated sequence. Node `Process` method is called multiple times per single inputs set. To trigger such behavior a specific graph configuration is needed. See below:
 
 ```pbtxt
 node {
@@ -833,14 +843,14 @@ Apart from basic configuration present also in regular mode, this graph contains
 It's recommended not to reuse the same graph instance when the cycle is finished.
 Instead, if you want to generate for new data, create new gRPC stream.
 
-For working configurations and code samples see the [demos](https://docs.openvino.ai/nightly/ovms_docs_demos.html#check-out-new-generative-ai-demos).
+For working configurations and code samples see the [demos](https://docs.openvino.ai/2024/ovms_docs_demos.html#check-out-new-generative-ai-demos).
 
 ### Incomplete inputs
 
 There are usecases when firing `Process` with only a subset of inputs defined in node configuration is desired. By default, node waits for all inputs with the same timestamp and launches `Process` once they're all available. Such behavior is implemented by the `DefaultInputStreamHandler` which is used by default.
 To configure the node to launch `Process` with only a subset of inputs you should use a different input stream handler for different [input policy](https://developers.google.com/mediapipe/framework/framework_concepts/synchronization#input_policies).
 
-Such configuration is used in [generative execution mode](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#generative-mode), but let's see another example:
+Such configuration is used in [generative execution mode](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#generative-mode), but let's see another example:
 
 ```pbtxt
 node {
@@ -928,18 +938,18 @@ class OvmsPythonModel:
 
 In such case, the client could implement different actions depending on which output it receives on the stream.
 
-Another example of such configuration is signaling that generation is finished when running in [generative mode](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#generative-mode). This solution is used in [text generation demo](https://github.com/openvinotoolkit/model_server/tree/main/demos/python_demos/llm_text_generation).
+Another example of such configuration is signaling that generation is finished when running in [generative mode](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#generative-mode). This solution is used in [text generation demo](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/python_demos/llm_text_generation).
 
 
 ### Calculator type conversions
 
-Python nodes work with a dedicated [Python Tensor](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#python-tensor) objects that can be used both on C++ and Python side. The downside of that approach is that usually other calculators cannot read and create such objects. It means that Python nodes cannot be directly connected to any other, non-Python nodes.
+Python nodes work with a dedicated [Python Tensor](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#python-tensor) objects that can be used both on C++ and Python side. The downside of that approach is that usually other calculators cannot read and create such objects. It means that Python nodes cannot be directly connected to any other, non-Python nodes.
 
 That's why converter calculators exists. They work as adapters between nodes and implement necessary conversions needed to create a connection between calculators that work on two different types of packets.
 
 #### PyTensorOvTensorConverterCalculator
 
-OpenVINO Model Server comes with a built-in `PyTensorOvTensorConverterCalculator` that provides conversion between [Python Tensor](https://docs.openvino.ai/nightly/ovms_docs_python_support_reference.html#python-tensor) and [OV Tensor](https://docs.openvino.ai/2024/api/c_cpp_api/classov_1_1_tensor.html).
+OpenVINO Model Server comes with a built-in `PyTensorOvTensorConverterCalculator` that provides conversion between [Python Tensor](https://docs.openvino.ai/2024/ovms_docs_python_support_reference.html#python-tensor) and [OV Tensor](https://docs.openvino.ai/2024/api/c_cpp_api/classov_1_1_tensor.html).
 
 Currently `PyTensorOvTensorConverterCalculator` works with only one input and one output.
 - The stream that expects Python Tensor **must** have tag `OVMS_PY_TENSOR`
@@ -1009,4 +1019,4 @@ node {
 }
 ```
 
-See a [CLIP demo](https://github.com/openvinotoolkit/model_server/tree/main/demos/python_demos/clip_image_classification) for a complete example of a graph that uses Python nodes, OV Inference nodes and converter nodes.
+See a [CLIP demo](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/python_demos/clip_image_classification) for a complete example of a graph that uses Python nodes, OV Inference nodes and converter nodes.
diff --git a/docs/text_handling.md b/docs/text_handling.md
index 3bab5c21..a22ed90c 100644
--- a/docs/text_handling.md
+++ b/docs/text_handling.md
@@ -33,6 +33,7 @@ Example of batch size 2 of the string input - `abcd` and `ab`:
 ]
 ```
 Such data in a tensor format can be passed to the custom node to perform the preprocessing like string tokenization. The output of the preprocessing node can be passed to the model.
+There is a built-in [Tokenizer](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/custom_nodes/tokenizer) custom node for that use case based on Blingfire library.
 
 Similarly, a custom node can perform string detokenization and return a string to the model server client.
 
diff --git a/extras/nginx-mtls-auth/get_model.sh b/extras/nginx-mtls-auth/get_model.sh
index b68049c6..fbaac091 100755
--- a/extras/nginx-mtls-auth/get_model.sh
+++ b/extras/nginx-mtls-auth/get_model.sh
@@ -17,7 +17,7 @@
 
 
 curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.xml https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.bin -o model/face-detection-retail-0004.xml -o model/face-detection-retail-0004.bin
-curl --fail --create-dirs https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/people/people1.jpeg -o images/people1.jpeg
+curl --fail --create-dirs https://raw.githubusercontent.com/openvinotoolkit/model_server/releases/2024/1/demos/common/static/images/people/people1.jpeg -o images/people1.jpeg
 chmod 666 -vR ./images/ ./model/
 chmod +x ./images/ ./model/
 
diff --git a/src/BUILD b/src/BUILD
index d8cfc5bb..19801f56 100644
--- a/src/BUILD
+++ b/src/BUILD
@@ -346,8 +346,6 @@ cc_library(
                 "mediapipe_internal/mediapipegraphexecutor.cpp",
                 "mediapipe_internal/mediapipegraphexecutor.hpp",
                 "mediapipe_internal/packettypes.hpp",
-                "kfs_frontend/kfs_graph_executor_impl.cpp",
-                "kfs_frontend/kfs_graph_executor_impl.hpp",
             ],
             "//:disable_mediapipe" : [],
         }),
diff --git a/src/custom_nodes/east_ocr/README.md b/src/custom_nodes/east_ocr/README.md
index 1f3634f8..1c01afad 100644
--- a/src/custom_nodes/east_ocr/README.md
+++ b/src/custom_nodes/east_ocr/README.md
@@ -7,7 +7,7 @@ DAG pipeline.
 Additionally to the detected text boxes, in the two additional outputs are returned their coordinates with information about geometry
 and confidence levels for the filtered list of detections.  
 
-**NOTE** Exemplary [configuration file](https://github.com/openvinotoolkit/model_server/blob/main/demos/optical_character_recognition/python/config.json) is available in [optical character recognition demo](https://github.com/openvinotoolkit/model_server/blob/main/demos/optical_character_recognition/python/).
+**NOTE** Exemplary [configuration file](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/optical_character_recognition/python/config.json) is available in [optical character recognition demo](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/optical_character_recognition/python/).
 
 # Building custom node library
 
diff --git a/src/custom_nodes/face_blur/README.md b/src/custom_nodes/face_blur/README.md
index d5b355fa..15866e29 100644
--- a/src/custom_nodes/face_blur/README.md
+++ b/src/custom_nodes/face_blur/README.md
@@ -18,7 +18,7 @@ All [OpenVINO Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/tree/
 - vehicle-license-plate-detection
 - pedestrian-and-vehicle-detector
 
-**NOTE** Exemplary [configuration file](https://github.com/openvinotoolkit/model_server/blob/main/demos/face_blur/python/config.json) is available in [face_blur demo](https://github.com/openvinotoolkit/model_server/blob/main/demos/face_blur/python/).
+**NOTE** Exemplary [configuration file](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/face_blur/python/config.json) is available in [face_blur demo](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/face_blur/python/).
 
 # Building custom node library
 
@@ -48,7 +48,7 @@ make BASE_OS=redhat NODES=face_blur
 | image | Returns blurred image in place of detected boxes. Boxes are filtered based on confidence_threshold param. Resolution is defined by the node parameters.   | `N,C,H,W` | FP32  |
 
 # Custom node parameters
-Parameters can be defined in pipeline definition in OVMS configuration file. [Read more](https://github.com/openvinotoolkit/model_server/blob/main/docs/custom_node_development.md) about node parameters.
+Parameters can be defined in pipeline definition in OVMS configuration file. [Read more](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/custom_node_development.md) about node parameters.
 | Parameter        | Description           | Default  | Required |
 | ------------- | ------------- | ------------- | ------------ |
 | original_image_width  | Required input image width |  | &check; |
diff --git a/src/custom_nodes/horizontal_ocr/README.md b/src/custom_nodes/horizontal_ocr/README.md
index d4c9eeb5..7cf65d4f 100644
--- a/src/custom_nodes/horizontal_ocr/README.md
+++ b/src/custom_nodes/horizontal_ocr/README.md
@@ -8,7 +8,7 @@ Additionally to the detected text boxes, in the two additional outputs are retur
 
 This custom node can be used to process video frames via [camera example](../../../demos/horizontal_text_detection/python/README.md).
 
-**NOTE** Exemplary [configuration file](https://github.com/openvinotoolkit/model_server/blob/main/demos/horizontal_text_detection/python/config.json) is available in [demo with camera](https://github.com/openvinotoolkit/model_server/blob/main/demos/horizontal_text_detection/python/).
+**NOTE** Exemplary [configuration file](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/horizontal_text_detection/python/config.json) is available in [demo with camera](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/horizontal_text_detection/python/).
 
 # Building custom node library
 
diff --git a/src/custom_nodes/image_transformation/README.md b/src/custom_nodes/image_transformation/README.md
index 351362ab..5a928fe5 100644
--- a/src/custom_nodes/image_transformation/README.md
+++ b/src/custom_nodes/image_transformation/README.md
@@ -9,7 +9,7 @@ This custom node takes image with dynamic shape (color, width, height) as an inp
 Important to note that this node uses OpenCV for processing so for good performance results prefers NHWC layout.
 In other cases conversion applies which reduces performance of this node.
 
-**NOTE** Exemplary configuration files are available in [onnx model with server preprocessing demo](https://github.com/openvinotoolkit/model_server/tree/main/demos/using_onnx_model/python) and [config with single node](example_config.json).
+**NOTE** Exemplary configuration files are available in [onnx model with server preprocessing demo](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/demos/using_onnx_model/python) and [config with single node](example_config.json).
 
 # Building custom node library
 
diff --git a/src/custom_nodes/model_zoo_intel_object_detection/README.md b/src/custom_nodes/model_zoo_intel_object_detection/README.md
index ff31f2d3..9c851504 100644
--- a/src/custom_nodes/model_zoo_intel_object_detection/README.md
+++ b/src/custom_nodes/model_zoo_intel_object_detection/README.md
@@ -25,7 +25,7 @@ All [OpenVINO Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/tree/
 Public [OpenVINO Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public) object detection models with output tensor shape: `[1, 1, 100, 7]`:
 - ssdlite_mobilenet_v2
 
-**NOTE** Exemplary configuration files are available in [vehicle analysis pipeline demo](https://github.com/openvinotoolkit/model_server/blob/main/demos/horizontal_text_detection/python/config.json) and [multiple faces analysis demo](https://github.com/openvinotoolkit/model_server/blob/main/demos/multi_faces_analysis_pipeline/python/config.json).
+**NOTE** Exemplary configuration files are available in [vehicle analysis pipeline demo](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/horizontal_text_detection/python/config.json) and [multiple faces analysis demo](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/demos/multi_faces_analysis_pipeline/python/config.json).
 
 # Building custom node library
 
diff --git a/src/custom_nodes/tokenizer/.gitignore b/src/custom_nodes/tokenizer/.gitignore
new file mode 100644
index 00000000..9bc2095c
--- /dev/null
+++ b/src/custom_nodes/tokenizer/.gitignore
@@ -0,0 +1,4 @@
+build
+common
+lib
+custom_node_interface.h
diff --git a/src/custom_nodes/tokenizer/CMakeLists.txt b/src/custom_nodes/tokenizer/CMakeLists.txt
new file mode 100644
index 00000000..3c2ba69b
--- /dev/null
+++ b/src/custom_nodes/tokenizer/CMakeLists.txt
@@ -0,0 +1,29 @@
+#
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+cmake_minimum_required(VERSION 3.10)
+project(node)
+
+include(ExternalProject)
+
+set(BLINGFIRE_SHA 5089d31914cbed7a24589e753bd6cd362a377fbb)
+
+set(CMAKE_POSITION_INDEPENDENT_CODE ON)
+
+add_subdirectory(src)
+
+if (WITH_TESTS)
+    add_subdirectory(test)
+endif()
diff --git a/src/custom_nodes/tokenizer/Dockerfile.redhat b/src/custom_nodes/tokenizer/Dockerfile.redhat
new file mode 100644
index 00000000..f7543f37
--- /dev/null
+++ b/src/custom_nodes/tokenizer/Dockerfile.redhat
@@ -0,0 +1,27 @@
+#
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+FROM registry.access.redhat.com/ubi8/ubi:8.9
+RUN dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm && yum update -d6 -y && yum install -d6 -y gcc-c++ curl xz cmake git
+
+WORKDIR /
+COPY ./common /custom_nodes/common
+COPY ./src /custom_nodes/tokenizer/src
+COPY ./test /custom_nodes/tokenizer/test
+COPY ./CMakeLists.txt /custom_nodes/tokenizer/CMakeLists.txt
+COPY custom_node_interface.h /
+WORKDIR /custom_nodes/tokenizer/build
+RUN cmake -DWITH_TESTS=1 .. && make -j`nproc` && ./test/detokenization_test && ./test/tokenization_test
diff --git a/src/custom_nodes/tokenizer/Dockerfile.ubuntu b/src/custom_nodes/tokenizer/Dockerfile.ubuntu
new file mode 100644
index 00000000..332a04bf
--- /dev/null
+++ b/src/custom_nodes/tokenizer/Dockerfile.ubuntu
@@ -0,0 +1,28 @@
+#
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+FROM ubuntu:20.04
+
+RUN apt update && DEBIAN_FRONTEND=noninteractive apt install -y build-essential curl cmake git
+
+WORKDIR /
+COPY ./common /custom_nodes/common
+COPY ./src /custom_nodes/tokenizer/src
+COPY ./test /custom_nodes/tokenizer/test
+COPY ./CMakeLists.txt /custom_nodes/tokenizer/CMakeLists.txt
+COPY custom_node_interface.h /
+WORKDIR /custom_nodes/tokenizer/build
+RUN cmake -DWITH_TESTS=1 .. && make -j`nproc` && ./test/detokenization_test && ./test/tokenization_test
diff --git a/src/custom_nodes/tokenizer/Makefile b/src/custom_nodes/tokenizer/Makefile
new file mode 100644
index 00000000..69cf8aaa
--- /dev/null
+++ b/src/custom_nodes/tokenizer/Makefile
@@ -0,0 +1,40 @@
+#
+# Copyright (c) 2021 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+HEADER_FILE_PATH ?= ../../custom_node_interface.h
+COMMON_DIR ?= ../common
+
+BASE_OS ?= ubuntu
+
+.PHONY: all
+
+default: all
+
+all:
+ifeq ($(NO_DOCKER_CACHE),true)
+	$(eval NO_CACHE_OPTION:=--no-cache)
+	@echo "Docker image will be rebuilt from scratch"
+endif
+	@cp $(HEADER_FILE_PATH) .
+	@cp -r $(COMMON_DIR) ./
+	echo "Building tokenizer"
+	docker build $(NO_CACHE_OPTION) -f Dockerfile.$(BASE_OS) -t tokenizer_build_image:latest --build-arg http_proxy=${http_proxy} --build-arg https_proxy=${https_proxy} --build-arg no_proxy=${no_proxy} .
+	mkdir -p ./lib/$(BASE_OS)
+	docker cp $$(docker create --rm tokenizer_build_image:latest):/custom_nodes/tokenizer/build/src/libdetokenizer.so ./lib/$(BASE_OS)/
+	docker cp $$(docker create --rm tokenizer_build_image:latest):/custom_nodes/tokenizer/build/src/libtokenizer.so ./lib/$(BASE_OS)/
+	echo "Built tokenizer"
+	@rm -rf ./common
+	@rm custom_node_interface.h
diff --git a/src/custom_nodes/tokenizer/README.md b/src/custom_nodes/tokenizer/README.md
new file mode 100644
index 00000000..76b27a59
--- /dev/null
+++ b/src/custom_nodes/tokenizer/README.md
@@ -0,0 +1,102 @@
+# Custom node for text tokenization and detokenization for Large Language Models
+
+This custom node project compiles into 2 libraries:
+- libtokenizer.so
+- libdetokenizer.so
+
+It can be used in OVMS DAG to tokenize string input into tokens accepted by NLP model. The other library can be used to create string from tokens generated by NLP model.
+
+![diagram](diagram.svg)
+
+The nodes statically link [BlingFire from Microsoft](https://github.com/microsoft/BlingFire) library and use it for tokenization.
+
+# Supported models
+All models which accept the tokens via 2 inputs: `input_ids` of shape `[B, N]` (where `B` is batch and `N` is max length of tokens per batch) and `attention_mask` of the same shape `[B, N]` which declare the padding using `0` and `1` values. The inputs must have `I64` precision.
+
+The model must output logits with 3 dimensions: `[B, N, K]`, where `K` represents vocabulary size. The precision of the logits should be `FP32`.
+
+Known models with such format:
+- [GPT-2](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/gpt-2)
+- [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)
+
+**NOTE:** Both models work with `gpt2.bin`/`gpt2.i2w` pretrained tokenization models from BlingFire repo. Pretrained tokenization models [can be found here](https://github.com/microsoft/BlingFire/tree/5089d31914cbed7a24589e753bd6cd362a377fbb/ldbsrc/ldb).
+
+# Building custom node library
+
+You can build the shared library of the custom node simply by running the following commands:
+```bash
+git clone https://github.com/openvinotoolkit/model_server && cd model_server/src/custom_nodes/tokenizer
+make
+```
+It will compile the library inside a docker container and save the results in `lib/<OS>/` folder.
+
+You can also select base OS between UBI 8.6 (redhat) and Ubuntu 20.04 (ubuntu) by setting `BASE_OS` environment variable.
+```bash
+make BASE_OS=redhat
+```
+
+It will compile the libraries:
+```bash
+ls lib/redhat -A1
+
+libdetokenizer.so
+libtokenizer.so
+```
+
+
+# libtokenizer.so inputs
+
+| Input name       | Description           | Shape | Precision |
+| ------------- |:-------------:| -----:| -----:|
+| texts      | 2D array of bytes, where each row represents single, null terminated sentence, padded with `\0` alignment to longest batch. | `-1,-1` | U8 |
+
+Example 2 strings - `abcd` and `ab`:
+```
+[
+    'a', 'b', 'c', 'd', 0,
+    'a', 'b',  0 ,  0 , 0
+]
+```
+OVMS supports automatic conversion from TensorflowServing/KServe proto to 2D U8 data matching the node input format.
+
+# libtokenizer.so outputs
+The outputs of this node match inputs of GPT-like models.
+
+| Output name        | Description           | Shape  | Precision |
+| ------------- |:-------------:| -----:| -----:|
+| input_ids | Returns 2D array of tokenized sentences padded to longest batch of tokens. Padding info required by GPT models are contained in `attention_mask` output | `-1,-1` | I64  |
+| attention_mask | Returns padding information, 2D array of `1` and `0` where `0` indicates padding. The same is always identical to `input_ids` output. | `-1,-1` | I64  |
+
+# libdetokenizer.so inputs
+
+| Input name       | Description           | Shape | Precision |
+| ------------- |:-------------:| -----:| -----:|
+| logits | Output from GPT-like model, the next token prediction tensor | `-1,-1,-1` | FP32 |
+| input_ids | Output from tokenizer library required for knowledge of previous tokens (context) | `-1,-1` | I64 |
+| attention_mask | Output from tokenizer library required for knowledge of previous tokens (context) | `-1,-1` | I64 |
+
+
+# libtokenizer.so outputs
+
+| Output name        | Description           | Shape  | Precision |
+| ------------- |:-------------:| -----:| -----:|
+| texts | Returns 2D array of sentences padded to longest batch of tokens. The sentences contain original content plus the autocompletion provided by prediction of subsequent token | `-1,-1` | U8  |
+
+OVMS supports automatic conversion from `U8 2D` format to TensorflowServing/KServe proto.
+
+# Custom node parameters
+Parameters can be defined in pipeline definition in OVMS configuration file. [Read more](https://github.com/openvinotoolkit/model_server/blob/releases/2024/1/docs/custom_node_development.md) about node parameters.
+
+## Parameters for libtokenizer.so
+| Parameter        | Description           | Default  | Required |
+| ------------- | ------------- | ------------- | ------------ |
+| model_path | Local path to [tokenization model](https://github.com/microsoft/BlingFire/tree/5089d31914cbed7a24589e753bd6cd362a377fbb/ldbsrc/ldb) in BlingFire format |  | &check; |
+| max_ids_arr_length | Maximum number of tokens to be generated from input sentences. If input string exceeds this amount, the generated tokens are cut. | 1024 | |
+| debug  | Defines if debug messages should be displayed | false | |
+
+## Parameters for libdetokenizer.so
+| Parameter        | Description           | Default  | Required |
+| ------------- | ------------- | ------------- | ------------ |
+| model_path | Local path to [detokenization model](https://github.com/microsoft/BlingFire/tree/5089d31914cbed7a24589e753bd6cd362a377fbb/ldbsrc/ldb) in BlingFire format |  | &check; |
+| max_buffer_length | Maximum size of text generated by detokenization. This includes context (sentence before autocompletion by GPT-model). If generated text is larger than buffer, it is shrank. This value should generally be larger than `max_ids_arr_length` in tokenization node | 4096 | |
+| debug  | Defines if debug messages should be displayed | false | |
diff --git a/src/custom_nodes/tokenizer/diagram.svg b/src/custom_nodes/tokenizer/diagram.svg
new file mode 100644
index 00000000..0ada1d04
--- /dev/null
+++ b/src/custom_nodes/tokenizer/diagram.svg
@@ -0,0 +1,4 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Do not edit this file with editors other than diagrams.net -->
+<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
+<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="781px" height="171px" viewBox="-0.5 -0.5 781 171" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2023-03-13T10:41:37.453Z&quot; agent=&quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36&quot; etag=&quot;HS3HZVnh3gvxtq_Xcyg_&quot; version=&quot;21.0.4&quot; type=&quot;device&quot;&gt;&lt;diagram name=&quot;Page-1&quot; id=&quot;s8ovUqlWPwGgRYUs28aZ&quot;&gt;7VnbctowEP0aHsnYli/w2BJ6maZJJmnapG/CXttKjeSR5QD5+spYvmEgbhsuZfLCSEfSStqzZxdED42m848cx+FX5kHUMzRv3kPnPcMwh0P5mQGLHLAMJwcCTrwc0ivgljyDAjWFpsSDpDFRMBYJEjdBl1EKrmhgmHM2a07zWdTcNcYBtIBbF0dt9AfxRJijA0ur8E9AgrDYWdfUyBQXkxWQhNhjsxqExj004oyJvDWdjyDKfFf4JV/3YcNoeTAOVHRZAHcofYx+hmYqrp3+lzvqoHFfN9ThxKK4MXjSAarLuAhZwCiOxhX6nrOUepCZ1WSvmnPBWCxBXYKPIMRCsYlTwSQUimmkRmFOxH2t/ZCZOrNU73yuLC87i6JDBV/c1zu1VVm3WrbsFevy+2WX2ug3BSUs5S5scZbylcA8ALFlnl2yK1UBbAryPHIdhwgL8tQ8B1bxGZTzKgplQ7H4B4yqQz7hKC3uQgu7dZqbJM5CIuA2xsv7z6SSm4Qpm8AFzLe7sX1ttWCgZLBodmeVqPRCKWFNULa2Kz8dNPKraH+oB/sLkV8F+0Nhbz+RjzpGfi2xHSL0zVbsBzfXma2b8e23tXRf4ImsVw2KcEQCKtuu9BJwCWRhT2RFeKcGpsTz8miAhDzjydJe5vCYESqWl7Le96zzbbpR5UotropEnZwtYbtRZX3tzED2IDfW2evK3HV2/toU5vuJpHuVlnLXv2cKtZOUjPiD5yjHOLIkpaO38tw5Sdkdk9TgkDnKbkW+YL+ASir4v4W/T6JoxCLGl2sR6J4FjsQTweUGtZGh7SBsv45gDPPYBGO+CaazYAYdBaMftKoP2kU9Fv3Hvj15VcH4vm+47jrBePbEtl5JMKZ2bIKx3gTTWTDFo8SLikGHFExxyppiPPifq4xlH5to2lU8IpPSxWcJa3lZ3l40Xdl0GWUUVvyroO6/R9Zx12R3ByVfNzuygXbGhrOOjVrInzQfq+I4Aj7aFfsqBvr98+VV9i6avxCfKh2rBf7wdJTVbb8VfsOjVVWfd1+thx2r9YYnlT09WrXfQk7v0Wq7ZrJHqyFCDd301Z7H+4ZVhNf6NEeSnmFHWU6byK9cdpC1LiHljCbZ6TjITx8nLqGnmwudNanP3Gvqa38TXqFIK1kqSMIzSOT2BS73LYe6M7pqUxAarLd4stzruyNfdqu/TXM5V/89o/Fv&lt;/diagram&gt;&lt;/mxfile&gt;" style="background-color: rgb(255, 255, 255);"><defs/><g><path d="M 130 110 L 163.63 110" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 168.88 110 L 161.88 113.5 L 163.63 110 L 161.88 106.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><rect x="10" y="80" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 110px; margin-left: 11px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">entry</div></div></div></foreignObject><text x="70" y="114" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">entry</text></switch></g><path d="M 710 80 L 710 36.37" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 710 31.12 L 713.5 38.12 L 710 36.37 L 706.5 38.12 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 61px; margin-left: 710px;"><div data-drawio-colors="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; background-color: rgb(255, 255, 255); white-space: nowrap;">gRPC/REST</div></div></div></foreignObject><text x="710" y="64" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="11px" text-anchor="middle">gRPC/REST</text></switch></g><rect x="650" y="80" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 110px; margin-left: 651px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">exit</div></div></div></foreignObject><text x="710" y="114" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">exit</text></switch></g><path d="M 290 110 L 323.63 110" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 328.88 110 L 321.88 113.5 L 323.63 110 L 321.88 106.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><rect x="170" y="80" width="120" height="60" fill="#e1d5e7" stroke="#9673a6" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 110px; margin-left: 171px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">tokenizer</div></div></div></foreignObject><text x="230" y="114" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">tokenizer</text></switch></g><path d="M 450 110 L 483.63 110" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 488.88 110 L 481.88 113.5 L 483.63 110 L 481.88 106.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><rect x="330" y="80" width="120" height="60" fill="#fff2cc" stroke="#d6b656" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 110px; margin-left: 331px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">gpt-j-6b</div></div></div></foreignObject><text x="390" y="114" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">gpt-j-6b</text></switch></g><path d="M 610 110 L 643.63 110" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 648.88 110 L 641.88 113.5 L 643.63 110 L 641.88 106.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><rect x="490" y="80" width="120" height="60" fill="#e1d5e7" stroke="#9673a6" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 110px; margin-left: 491px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">detokenizer</div></div></div></foreignObject><text x="550" y="114" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">detokenizer</text></switch></g><rect x="170" y="140" width="120" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 155px; margin-left: 171px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">libtokenizer.so</div></div></div></foreignObject><text x="230" y="159" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">libtokenizer.so</text></switch></g><rect x="490" y="140" width="120" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 155px; margin-left: 491px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">libdetokenizer.so</div></div></div></foreignObject><text x="550" y="159" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">libdetokenizer.so</text></switch></g><rect x="330" y="140" width="120" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 155px; margin-left: 331px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">OpenVINO model</div></div></div></foreignObject><text x="390" y="159" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">OpenVINO model</text></switch></g><path d="M 70 30 L 70 73.63" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 70 78.88 L 66.5 71.88 L 70 73.63 L 73.5 71.88 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 48px; margin-left: 69px;"><div data-drawio-colors="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; background-color: rgb(255, 255, 255); white-space: nowrap;">gRPC/REST</div></div></div></foreignObject><text x="69" y="51" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="11px" text-anchor="middle">gRPC/REST</text></switch></g><rect x="0" y="0" width="140" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 138px; height: 1px; padding-top: 15px; margin-left: 1px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">OpenVINO is<br />Neurons are fascin</div></div></div></foreignObject><text x="70" y="19" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">OpenVINO is...</text></switch></g><rect x="640" y="0" width="140" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 138px; height: 1px; padding-top: 15px; margin-left: 641px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">OpenVINO is <b>awesome</b><br />Neurons are fascin<b>ating</b></div></div></div></foreignObject><text x="710" y="19" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">OpenVINO is awesome...</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Text is not SVG - cannot display</text></a></switch></svg>
\ No newline at end of file
diff --git a/src/custom_nodes/tokenizer/src/CMakeLists.txt b/src/custom_nodes/tokenizer/src/CMakeLists.txt
new file mode 100644
index 00000000..1d97e567
--- /dev/null
+++ b/src/custom_nodes/tokenizer/src/CMakeLists.txt
@@ -0,0 +1,51 @@
+#
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+cmake_minimum_required(VERSION 3.10)
+project(tokenizer)
+
+include(ExternalProject)
+
+set(CMAKE_CXX_STANDARD 17)
+
+set(BLINGFIRE_INSTALL_DIR ${CMAKE_BINARY_DIR}/external/blingfire)
+
+ExternalProject_Add(blingfire
+    PREFIX blingfire
+    GIT_REPOSITORY https://github.com/microsoft/BlingFire.git
+    GIT_TAG ${BLINGFIRE_SHA}
+    CMAKE_ARGS -DCMAKE_INSTALL_PREFIX=${BLINGFIRE_INSTALL_DIR}
+)
+
+set(BLINGFIRE_STATIC_LIB_DIR ${CMAKE_BINARY_DIR}/src/blingfire/src/blingfire-build)
+set(BLINGFIRE_STATIC_LIBS
+    ${BLINGFIRE_STATIC_LIB_DIR}/libbingfirtinydll_static.a
+    ${BLINGFIRE_STATIC_LIB_DIR}/libfsaClient.a)
+
+set(UTIL_DIRS
+    ../../common/       # for common/utils.hpp
+    ../../../)          # for custom_node_interface.h
+
+add_library(tokenizer SHARED model.cpp tokenizer.cpp)
+target_include_directories(tokenizer PRIVATE ${BLINGFIRE_INSTALL_DIR}/include ${UTIL_DIRS})
+target_link_libraries(tokenizer ${BLINGFIRE_STATIC_LIBS} stdc++fs)
+target_compile_options(tokenizer PRIVATE -fstack-protector -fno-omit-frame-pointer -fno-strict-overflow -Wall -Wno-unknown-pragmas -Werror -Wno-error=sign-compare -fno-delete-null-pointer-checks -fwrapv -fstack-clash-protection -Wformat -Wformat-security -Werror=format-security)
+add_dependencies(tokenizer blingfire)
+
+add_library(detokenizer SHARED model.cpp detokenizer.cpp)
+target_include_directories(detokenizer PRIVATE ${BLINGFIRE_INSTALL_DIR}/include ${UTIL_DIRS})
+target_link_libraries(detokenizer ${BLINGFIRE_STATIC_LIBS} stdc++fs)
+target_compile_options(detokenizer PRIVATE -fstack-protector -fno-omit-frame-pointer -fno-strict-overflow -Wall -Wno-unknown-pragmas -Werror -Wno-error=sign-compare -fno-delete-null-pointer-checks -fwrapv -fstack-clash-protection -Wformat -Wformat-security -Werror=format-security)
+add_dependencies(detokenizer blingfire)
diff --git a/src/custom_nodes/tokenizer/src/detokenizer.cpp b/src/custom_nodes/tokenizer/src/detokenizer.cpp
new file mode 100644
index 00000000..5a018886
--- /dev/null
+++ b/src/custom_nodes/tokenizer/src/detokenizer.cpp
@@ -0,0 +1,272 @@
+//*****************************************************************************
+// Copyright 2023 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//*****************************************************************************
+#include <algorithm>
+#include <chrono>
+#include <cstring>
+#include <iostream>
+#include <string>
+#include <vector>
+
+#include "custom_node_interface.h"  // NOLINT
+#include "model.hpp"
+#include "utils.hpp"
+
+#define DEBUG_MSG(str)                                     \
+    if (debugMode) {                                       \
+        std::cout << "[detokenizer] " << str << std::endl; \
+    }
+
+#define INPUT_NAME_LOGITS "logits"
+#define INPUT_NAME_PREVIOUS_TOKENS "input_ids"
+#define INPUT_NAME_PREVIOUS_ATTENTION "attention_mask"
+
+#define OUTPUT_NAME_TEXTS "texts"
+
+// Size of memory allocation on the heap for generated text.
+// If the size of the output is larger than this value, the output is truncated.
+// Consider using memory pool.
+#define DEFAULT_MAX_BUF_LEN 4096
+
+using namespace custom_nodes::tokenizer;
+
+int initialize(void** customNodeLibraryInternalManager, const struct CustomNodeParam* params, int paramsCount) {
+    bool debugMode = get_string_parameter("debug", params, paramsCount) == "true";
+    std::string modelPath = get_string_parameter("model_path", params, paramsCount, "");
+    NODE_ASSERT(!modelPath.empty(), "model_path cannot be empty");
+    try {
+        auto cnlim = std::make_unique<BlingFireModel>(modelPath, debugMode);
+        if (!cnlim->isValid())
+            throw std::exception();
+        *customNodeLibraryInternalManager = cnlim.release();
+    } catch (...) {
+        std::cerr << "[detokenizer] initialize() fail: Cannot load tokenization model from path: " << modelPath << std::endl;
+        return 1;
+    }
+    return 0;
+}
+
+int deinitialize(void* customNodeLibraryInternalManager) {
+    if (customNodeLibraryInternalManager != nullptr) {
+        BlingFireModel* manager = static_cast<BlingFireModel*>(customNodeLibraryInternalManager);
+        delete manager;
+    }
+    return 0;
+}
+
+static int retrieveInputs(
+    // in
+    const struct CustomNodeTensor* inputs,
+    int inputsCount,
+    // out
+    const CustomNodeTensor** logitsTensor,
+    const CustomNodeTensor** inputIdsTensor,
+    const CustomNodeTensor** attentionMaskTensor) {
+    for (int i = 0; i < inputsCount; i++) {
+        if (std::strcmp(inputs[i].name, INPUT_NAME_LOGITS) == 0) {
+            *logitsTensor = &(inputs[i]);
+        } else if (std::strcmp(inputs[i].name, INPUT_NAME_PREVIOUS_TOKENS) == 0) {
+            *inputIdsTensor = &(inputs[i]);
+        } else if (std::strcmp(inputs[i].name, INPUT_NAME_PREVIOUS_ATTENTION) == 0) {
+            *attentionMaskTensor = &(inputs[i]);
+        } else {
+            std::cerr << "Unrecognized input: " << inputs[i].name << std::endl;
+            return 1;
+        }
+    }
+    return 0;
+}
+
+static int validateInputs(
+    const CustomNodeTensor* logitsTensor,
+    const CustomNodeTensor* inputIdsTensor,
+    const CustomNodeTensor* attentionMaskTensor) {
+    NODE_ASSERT(logitsTensor != nullptr, "Missing " INPUT_NAME_LOGITS " input");
+    NODE_ASSERT(logitsTensor->precision == FP32, INPUT_NAME_LOGITS " input is not FP32");
+    NODE_ASSERT(logitsTensor->dimsCount == 3, "input " INPUT_NAME_LOGITS " shape must have 3 dimensions");
+    NODE_ASSERT(logitsTensor->dims[0] > 0, "input " INPUT_NAME_LOGITS " dimension 1 must be larger than 0");
+    NODE_ASSERT(logitsTensor->dims[1] > 0, "input " INPUT_NAME_LOGITS " dimension 2 must be larger than 0");
+    NODE_ASSERT(logitsTensor->dims[2] > 0, "input " INPUT_NAME_LOGITS " text dimension 3 must be larger than 0");
+
+    NODE_ASSERT(inputIdsTensor != nullptr, "Missing " INPUT_NAME_PREVIOUS_TOKENS " input");
+    NODE_ASSERT(inputIdsTensor->precision == I64, INPUT_NAME_PREVIOUS_TOKENS " input is not I64");
+    NODE_ASSERT(inputIdsTensor->dimsCount == 2, INPUT_NAME_PREVIOUS_TOKENS " shape must have 2 dimensions");
+    NODE_ASSERT(inputIdsTensor->dims[0] > 0, INPUT_NAME_PREVIOUS_TOKENS " dimension 1 must be larger than 0");
+    NODE_ASSERT(inputIdsTensor->dims[1] > 0, INPUT_NAME_PREVIOUS_TOKENS " dimension 2 must be larger than 0");
+
+    NODE_ASSERT(attentionMaskTensor != nullptr, "Missing " INPUT_NAME_PREVIOUS_ATTENTION " input");
+    NODE_ASSERT(attentionMaskTensor->precision == I64, INPUT_NAME_PREVIOUS_ATTENTION " input is not I64");
+    NODE_ASSERT(attentionMaskTensor->dimsCount == 2, INPUT_NAME_PREVIOUS_ATTENTION " shape must have 2 dimensions");
+    NODE_ASSERT(attentionMaskTensor->dims[0] > 0, INPUT_NAME_PREVIOUS_ATTENTION " dimension 1 must be larger than 0");
+    NODE_ASSERT(attentionMaskTensor->dims[1] > 0, INPUT_NAME_PREVIOUS_ATTENTION " dimension 2 must be larger than 0");
+
+    NODE_ASSERT(logitsTensor->dims[0] == inputIdsTensor->dims[0], INPUT_NAME_LOGITS " and " INPUT_NAME_PREVIOUS_TOKENS " need to have matching batch dimension");
+    NODE_ASSERT(logitsTensor->dims[0] == attentionMaskTensor->dims[0], INPUT_NAME_LOGITS " and " INPUT_NAME_PREVIOUS_ATTENTION " need to have matching batch dimension");
+
+    NODE_ASSERT(logitsTensor->dims[1] == inputIdsTensor->dims[1], INPUT_NAME_LOGITS " and " INPUT_NAME_PREVIOUS_TOKENS " need to have matching second dimension");
+    NODE_ASSERT(logitsTensor->dims[1] == attentionMaskTensor->dims[1], INPUT_NAME_LOGITS " and " INPUT_NAME_PREVIOUS_ATTENTION " need to have matching second dimension");
+    return 0;
+}
+
+// in:  [-1, -1, 50400]
+// out: [Batch, MaxLength]
+int execute(const struct CustomNodeTensor* inputs, int inputsCount, struct CustomNodeTensor** outputs, int* outputsCount, const struct CustomNodeParam* params, int paramsCount, void* customNodeLibraryInternalManager) {
+    auto start = std::chrono::steady_clock::now();
+    bool debugMode = get_string_parameter("debug", params, paramsCount) == "true";
+    DEBUG_MSG("execute() start");
+    // Parameters reading
+    int maxBufferLength = get_int_parameter("max_buffer_length", params, paramsCount, DEFAULT_MAX_BUF_LEN);
+    NODE_ASSERT(maxBufferLength > 0, "max_buffer_length param must be larger than 0");
+
+    const CustomNodeTensor* logitsTensor = nullptr;
+    const CustomNodeTensor* inputIdsTensor = nullptr;
+    const CustomNodeTensor* attentionMaskTensor = nullptr;
+
+    NODE_ASSERT(retrieveInputs(inputs, inputsCount, &logitsTensor, &inputIdsTensor, &attentionMaskTensor) == 0, "retrieveInputs() failed");
+    NODE_ASSERT(validateInputs(logitsTensor, inputIdsTensor, attentionMaskTensor) == 0, "validateInputs() failed");
+
+    BlingFireModel* model = static_cast<BlingFireModel*>(customNodeLibraryInternalManager);
+
+    std::vector<std::string> results;
+    for (uint64_t batch = 0; batch < logitsTensor->dims[0]; batch++) {
+        // get previous tokens of current batch for context
+        DEBUG_MSG("get previous tokens of batch " << batch);
+        int64_t* inputIds = reinterpret_cast<int64_t*>(
+            inputIdsTensor->data +
+            batch * (inputIdsTensor->dims[1] * sizeof(int64_t)));
+        int64_t* attentionMask = reinterpret_cast<int64_t*>(
+            attentionMaskTensor->data +
+            batch * (attentionMaskTensor->dims[1] * sizeof(int64_t)));
+
+        int64_t* it = std::find(attentionMask, attentionMask + attentionMaskTensor->dims[1], 0);
+        std::ptrdiff_t distance = std::distance(attentionMask, it);
+        std::ptrdiff_t lastNonZeroIndex = distance - 1;
+
+        // case for empty string being in a batch (attention mask all zeros)
+        if (lastNonZeroIndex < 0)
+            lastNonZeroIndex = 0;
+
+        std::vector<int64_t> previousTokens(inputIds, inputIds + distance);
+
+        // slice
+        DEBUG_MSG("slicing batch " << batch);
+        float* logits = reinterpret_cast<float*>(
+            logitsTensor->data +
+            batch * (logitsTensor->dims[1] * logitsTensor->dims[2] * sizeof(float)) +  // offset by batch
+            (lastNonZeroIndex * logitsTensor->dims[2] * sizeof(float)));               // offset to get last element of second dimension
+
+        // argmax
+        DEBUG_MSG("argmax batch " << batch);
+        float* result = std::max_element(logits, logits + logitsTensor->dims[2]);
+        int64_t token = std::distance(logits, result);
+        previousTokens.push_back(token);
+
+        // detokenize
+        DEBUG_MSG("detokenizing token batch " << batch);
+        auto text = model->detokenize(previousTokens, maxBufferLength);
+        DEBUG_MSG("detokenized token: (" << token << ") to: (" << text << ") for batch " << batch);
+        results.emplace_back(std::move(text));
+    }
+
+    DEBUG_MSG("getting max string length");
+    size_t maxStringLength = 0;
+    for (const auto& str : results) {
+        maxStringLength = std::max(maxStringLength, str.size());
+    }
+    size_t width = maxStringLength + 1;
+
+    DEBUG_MSG("prepraing output tensor");
+    *outputsCount = 1;
+    *outputs = (struct CustomNodeTensor*)malloc(*outputsCount * sizeof(CustomNodeTensor));
+    if ((*outputs) == nullptr) {
+        std::cerr << "malloc has failed" << std::endl;
+        return 1;
+    }
+
+    // Outputs allocation
+    CustomNodeTensor& output = (*outputs)[0];
+    output.name = OUTPUT_NAME_TEXTS;
+    output.dataBytes = width * results.size();
+    output.data = (uint8_t*)malloc(output.dataBytes);
+    output.dimsCount = 2;
+    output.dims = (uint64_t*)malloc(output.dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(output.dims != nullptr, "malloc has failed");
+    output.dims[0] = results.size();
+    output.dims[1] = width;
+    output.precision = U8;
+
+    DEBUG_MSG("writing output");
+    for (size_t i = 0; i < results.size(); i++) {
+        std::memcpy(output.data + i * width, results[i].data(), results[i].size());
+        output.data[i * width + results[i].size()] = 0;
+    }
+    DEBUG_MSG("execute() end");
+    auto end = std::chrono::steady_clock::now();
+    DEBUG_MSG("execute() end; took " << std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() / 1000.f << " ms");
+    return 0;
+}
+
+int getInputsInfo(struct CustomNodeTensorInfo** info, int* infoCount, const struct CustomNodeParam* params, int paramsCount, void* customNodeLibraryInternalManager) {
+    *infoCount = 3;
+    *info = (struct CustomNodeTensorInfo*)malloc(*infoCount * sizeof(struct CustomNodeTensorInfo));
+    NODE_ASSERT((*info) != nullptr, "malloc has failed");
+
+    (*info)[0].name = INPUT_NAME_LOGITS;
+    (*info)[0].dimsCount = 3;
+    (*info)[0].dims = (uint64_t*)malloc((*info)[0].dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[0].dims) != nullptr, "malloc has failed");
+    (*info)[0].dims[0] = -1;
+    (*info)[0].dims[1] = -1;
+    (*info)[0].dims[2] = -1;
+    (*info)[0].precision = FP32;
+
+    (*info)[1].name = INPUT_NAME_PREVIOUS_TOKENS;
+    (*info)[1].dimsCount = 2;
+    (*info)[1].dims = (uint64_t*)malloc((*info)[1].dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[1].dims) != nullptr, "malloc has failed");
+    (*info)[1].dims[0] = -1;
+    (*info)[1].dims[1] = -1;
+    (*info)[1].precision = I64;
+
+    (*info)[2].name = INPUT_NAME_PREVIOUS_ATTENTION;
+    (*info)[2].dimsCount = 2;
+    (*info)[2].dims = (uint64_t*)malloc((*info)[2].dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[0].dims) != nullptr, "malloc has failed");
+    (*info)[2].dims[0] = -1;
+    (*info)[2].dims[1] = -1;
+    (*info)[2].precision = I64;
+    return 0;
+}
+
+int getOutputsInfo(struct CustomNodeTensorInfo** info, int* infoCount, const struct CustomNodeParam* params, int paramsCount, void* customNodeLibraryInternalManager) {
+    *infoCount = 1;
+    *info = (struct CustomNodeTensorInfo*)malloc(*infoCount * sizeof(struct CustomNodeTensorInfo));
+    NODE_ASSERT((*info) != nullptr, "malloc has failed");
+
+    (*info)[0].name = OUTPUT_NAME_TEXTS;
+    (*info)[0].dimsCount = 2;
+    (*info)[0].dims = (uint64_t*)malloc((*info)->dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[0].dims) != nullptr, "malloc has failed");
+    (*info)[0].dims[0] = -1;
+    (*info)[0].dims[1] = -1;
+    (*info)[0].precision = U8;
+
+    return 0;
+}
+
+int release(void* ptr, void* customNodeLibraryInternalManager) {
+    free(ptr);
+    return 0;
+}
diff --git a/src/custom_nodes/tokenizer/src/model.cpp b/src/custom_nodes/tokenizer/src/model.cpp
new file mode 100644
index 00000000..3875e428
--- /dev/null
+++ b/src/custom_nodes/tokenizer/src/model.cpp
@@ -0,0 +1,75 @@
+//*****************************************************************************
+// Copyright 2023 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//*****************************************************************************
+#include "model.hpp"
+
+#include <algorithm>
+#include <atomic>
+#include <cstring>
+#include <filesystem>
+#include <iostream>
+#include <memory>
+#include <string>
+#include <utility>
+
+#include "blingfiretokdll.h"  // NOLINT
+
+namespace custom_nodes {
+namespace tokenizer {
+
+static std::atomic<int> maxId{0};
+
+BlingFireModel::BlingFireModel(const std::string& modelPath, bool debug) :
+    id(maxId++),
+    debug(debug) {
+    if (!std::filesystem::exists(modelPath)) {
+        throw std::runtime_error("Model file does not exist: " + modelPath);
+    }
+    handle = BlingFire::LoadModel(modelPath.c_str());
+    if (debug) {
+        std::cout << "[BlingFireModel] [" << id << "] Model loaded from: " << modelPath << std::endl;
+    }
+}
+
+BlingFireModel::~BlingFireModel() {
+    if (handle) {
+        BlingFire::FreeModel(handle);
+        if (debug) {
+            std::cout << "[BlingFireModel] [" << id << "] Model unloaded." << std::endl;
+        }
+    }
+}
+
+std::vector<int64_t> BlingFireModel::tokenize(const std::string& text, int maxIdsArrLength) {
+    auto ids = std::make_unique<int32_t[]>(maxIdsArrLength);
+    const int idsLength = BlingFire::TextToIds(handle, text.c_str(), text.size(), ids.get(), maxIdsArrLength);
+    std::vector<int64_t> vec(idsLength);
+    std::transform(ids.get(), ids.get() + idsLength, vec.begin(),
+        [](int32_t val) { return static_cast<int64_t>(val); });
+    return vec;
+}
+
+std::string BlingFireModel::detokenize(const std::vector<int64_t>& tokens, int maxBufferLength, bool skipSpecialTokens) {
+    auto ids = std::make_unique<int32_t[]>(tokens.size());
+    std::transform(tokens.begin(), tokens.end(), ids.get(),
+        [](int64_t val) { return static_cast<int32_t>(val); });
+    std::string str(maxBufferLength + 1, '\0');  // +1 due to null ending
+    BlingFire::IdsToText(handle, ids.get(), tokens.size(), str.data(), maxBufferLength, skipSpecialTokens);
+    str.resize(std::strlen(str.data()));  // remove all remaining zero bytes
+    return str;
+}
+
+}  // namespace tokenizer
+}  // namespace custom_nodes
diff --git a/src/custom_nodes/tokenizer/src/model.hpp b/src/custom_nodes/tokenizer/src/model.hpp
new file mode 100644
index 00000000..6c24bc33
--- /dev/null
+++ b/src/custom_nodes/tokenizer/src/model.hpp
@@ -0,0 +1,40 @@
+//*****************************************************************************
+// Copyright 2023 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//*****************************************************************************
+#pragma once
+#include <cstdint>
+#include <string>
+#include <vector>
+
+namespace custom_nodes {
+namespace tokenizer {
+
+class BlingFireModel {
+    int id;
+    void* handle = nullptr;
+    bool debug;
+
+public:
+    BlingFireModel(const std::string& modelPath, bool debug = false);
+    ~BlingFireModel();
+
+    bool isValid() const { return handle != nullptr; }
+
+    std::vector<int64_t> tokenize(const std::string& text, int maxIdsArrLength);
+    std::string detokenize(const std::vector<int64_t>& tokens, int maxBufferLength, bool skipSpecialTokens = false);
+};
+
+}  // namespace tokenizer
+}  // namespace custom_nodes
diff --git a/src/custom_nodes/tokenizer/src/tokenizer.cpp b/src/custom_nodes/tokenizer/src/tokenizer.cpp
new file mode 100644
index 00000000..00fddfa0
--- /dev/null
+++ b/src/custom_nodes/tokenizer/src/tokenizer.cpp
@@ -0,0 +1,237 @@
+//*****************************************************************************
+// Copyright 2023 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//*****************************************************************************
+#include <chrono>
+#include <cstring>
+#include <iostream>
+#include <string>
+#include <vector>
+
+#define __STDC_WANT_LIB_EXT1__ 1  // to ensure existence of strnlen
+#include <string.h>
+
+#include "custom_node_interface.h"  // NOLINT
+#include "model.hpp"
+#include "utils.hpp"
+
+#define INPUT_NAME_TEXTS "texts"
+
+#define OUTPUT_NAME_TOKENS "input_ids"
+#define OUTPUT_NAME_ATTENTION "attention_mask"
+#define OUTPUT_NAME_POSITION "position_ids"
+
+// Size of memory allocation on the heap for generated tokens.
+// If the size of the output is larger than this value, the output is truncated.
+// Consider using memory pool.
+#define DEFAULT_MAX_ID_ARR_LEN 1024
+
+using namespace custom_nodes::tokenizer;
+
+#define DEBUG_MSG(str)                                   \
+    if (debugMode) {                                     \
+        std::cout << "[tokenizer] " << str << std::endl; \
+    }
+
+int initialize(void** customNodeLibraryInternalManager, const struct CustomNodeParam* params, int paramsCount) {
+    bool debugMode = get_string_parameter("debug", params, paramsCount) == "true";
+    std::string modelPath = get_string_parameter("model_path", params, paramsCount, "");
+    NODE_ASSERT(!modelPath.empty(), "model_path cannot be empty");
+    try {
+        auto cnlim = std::make_unique<BlingFireModel>(modelPath, debugMode);
+        if (!cnlim->isValid())
+            throw std::exception();
+        *customNodeLibraryInternalManager = cnlim.release();
+    } catch (...) {
+        std::cerr << "[tokenizer] initialize() fail: Cannot load tokenization model from path: " << modelPath << std::endl;
+        return 1;
+    }
+    return 0;
+}
+
+int deinitialize(void* customNodeLibraryInternalManager) {
+    if (customNodeLibraryInternalManager != nullptr) {
+        BlingFireModel* manager = static_cast<BlingFireModel*>(customNodeLibraryInternalManager);
+        delete manager;
+    }
+    return 0;
+}
+
+static int retrieveInputs(
+    // in
+    const struct CustomNodeTensor* inputs,
+    int inputsCount,
+    // out
+    const CustomNodeTensor** textTensor) {
+    for (int i = 0; i < inputsCount; i++) {
+        if (std::strcmp(inputs[i].name, INPUT_NAME_TEXTS) == 0) {
+            *textTensor = &(inputs[i]);
+        } else {
+            std::cerr << "Unrecognized input: " << inputs[i].name << std::endl;
+            return 1;
+        }
+    }
+    return 0;
+}
+
+static int validateInputs(const CustomNodeTensor* textTensor) {
+    NODE_ASSERT(textTensor != nullptr, "Missing " INPUT_NAME_TEXTS " input");
+    NODE_ASSERT(textTensor->precision == U8, INPUT_NAME_TEXTS " input is not U8");
+
+    NODE_ASSERT(textTensor->dimsCount == 2, INPUT_NAME_TEXTS " inout shape must have 2 dimensions");
+    NODE_ASSERT(textTensor->dims[0] > 0, INPUT_NAME_TEXTS " input dimension 1 must be larger than 0 (number of texts)");
+    NODE_ASSERT(textTensor->dims[1] > 0, INPUT_NAME_TEXTS " input dimension 2 must be larger than 0 (max null terminated text length)");
+    return 0;
+}
+
+int execute(const struct CustomNodeTensor* inputs, int inputsCount, struct CustomNodeTensor** outputs, int* outputsCount, const struct CustomNodeParam* params, int paramsCount, void* customNodeLibraryInternalManager) {
+    auto start = std::chrono::steady_clock::now();
+    bool debugMode = get_string_parameter("debug", params, paramsCount) == "true";
+    DEBUG_MSG("execute() start");
+    // Parameters reading
+    int maxIdsArrLength = get_int_parameter("max_ids_arr_length", params, paramsCount, DEFAULT_MAX_ID_ARR_LEN);
+    NODE_ASSERT(maxIdsArrLength > 0, "max_ids_arr_length param must be larger than 0");
+
+    const CustomNodeTensor* textTensor = nullptr;
+
+    NODE_ASSERT(retrieveInputs(inputs, inputsCount, &textTensor) == 0, "retrieveInputs() failed");
+    NODE_ASSERT(validateInputs(textTensor) == 0, "validateInputs() failed");
+
+    BlingFireModel* model = static_cast<BlingFireModel*>(customNodeLibraryInternalManager);
+
+    *outputsCount = 3;
+    *outputs = (struct CustomNodeTensor*)malloc(*outputsCount * sizeof(CustomNodeTensor));
+    if ((*outputs) == nullptr) {
+        std::cerr << "malloc has failed" << std::endl;
+        return 1;
+    }
+
+    std::vector<std::vector<int64_t>> ids(textTensor->dims[0]);
+    // For each batch, sequentially
+    for (uint64_t batch = 0; batch < textTensor->dims[0]; batch++) {
+        DEBUG_MSG("tokenizing batch " << batch);
+        const char* strStart = (const char*)textTensor->data + batch * textTensor->dims[1];
+        std::string text(strStart, strnlen(strStart, textTensor->dims[1]));
+        ids[batch] = model->tokenize(text, maxIdsArrLength);
+        DEBUG_MSG("tokenized batch " << batch << "; of string: " << text);
+    }
+
+    DEBUG_MSG("getting max token size");
+    size_t maxTokenSize = 0;
+    for (const auto& id : ids) {
+        maxTokenSize = std::max(maxTokenSize, id.size());
+    }
+
+    DEBUG_MSG("preparing output tensors");
+    CustomNodeTensor& tokens = (*outputs)[0];
+    tokens.name = OUTPUT_NAME_TOKENS;
+    tokens.dataBytes = sizeof(int64_t) * maxTokenSize * ids.size();
+    tokens.data = (uint8_t*)malloc(tokens.dataBytes);
+    tokens.dimsCount = 2;
+    tokens.dims = (uint64_t*)malloc(tokens.dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(tokens.dims != nullptr, "malloc has failed");
+    tokens.dims[0] = ids.size();
+    tokens.dims[1] = maxTokenSize;
+    tokens.precision = I64;
+
+    CustomNodeTensor& attention = (*outputs)[1];
+    attention.name = OUTPUT_NAME_ATTENTION;
+    attention.dataBytes = sizeof(int64_t) * maxTokenSize * ids.size();
+    attention.data = (uint8_t*)malloc(attention.dataBytes);
+    attention.dimsCount = 2;
+    attention.dims = (uint64_t*)malloc(attention.dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(attention.dims != nullptr, "malloc has failed");
+    attention.dims[0] = ids.size();
+    attention.dims[1] = maxTokenSize;
+    attention.precision = I64;
+
+    CustomNodeTensor& position = (*outputs)[2];
+    position.name = OUTPUT_NAME_POSITION;
+    position.dataBytes = sizeof(int64_t) * maxTokenSize * ids.size();
+    position.data = (uint8_t*)malloc(position.dataBytes);
+    position.dimsCount = 2;
+    position.dims = (uint64_t*)malloc(position.dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(position.dims != nullptr, "malloc has failed");
+    position.dims[0] = ids.size();
+    position.dims[1] = maxTokenSize;
+    position.precision = I64;
+    DEBUG_MSG("writing output");
+
+    for (size_t i = 0; i < ids.size(); i++) {
+        std::memcpy(tokens.data + i * maxTokenSize * sizeof(int64_t), ids[i].data(), ids[i].size() * sizeof(int64_t));
+        for (size_t j = 0; j < ids[i].size(); j++) {
+            ((int64_t*)attention.data)[i * maxTokenSize + j] = 1;
+        }
+        for (size_t j = 0; j < ids[i].size(); j++) {
+            ((int64_t*)position.data)[i * maxTokenSize + j] = j;
+        }
+        for (size_t j = ids[i].size(); j < maxTokenSize; j++) {
+            ((int64_t*)attention.data)[i * maxTokenSize + j] = 0;
+        }
+    }
+    auto end = std::chrono::steady_clock::now();
+    DEBUG_MSG("execute() end; took " << std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() / 1000.f << " ms");
+    return 0;
+}
+
+int getInputsInfo(struct CustomNodeTensorInfo** info, int* infoCount, const struct CustomNodeParam* params, int paramsCount, void* customNodeLibraryInternalManager) {
+    *infoCount = 1;
+    *info = (struct CustomNodeTensorInfo*)malloc(*infoCount * sizeof(struct CustomNodeTensorInfo));
+    NODE_ASSERT((*info) != nullptr, "malloc has failed");
+
+    (*info)[0].name = INPUT_NAME_TEXTS;
+    (*info)[0].dimsCount = 2;
+    (*info)[0].dims = (uint64_t*)malloc((*info)[0].dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[0].dims) != nullptr, "malloc has failed");
+    (*info)[0].dims[0] = -1;
+    (*info)[0].dims[1] = -1;
+    (*info)[0].precision = U8;
+    return 0;
+}
+
+int getOutputsInfo(struct CustomNodeTensorInfo** info, int* infoCount, const struct CustomNodeParam* params, int paramsCount, void* customNodeLibraryInternalManager) {
+    *infoCount = 3;
+    *info = (struct CustomNodeTensorInfo*)malloc(*infoCount * sizeof(struct CustomNodeTensorInfo));
+    NODE_ASSERT((*info) != nullptr, "malloc has failed");
+
+    (*info)[0].name = OUTPUT_NAME_TOKENS;
+    (*info)[0].dimsCount = 2;
+    (*info)[0].dims = (uint64_t*)malloc((*info)->dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[0].dims) != nullptr, "malloc has failed");
+    (*info)[0].dims[0] = -1;
+    (*info)[0].dims[1] = -1;
+    (*info)[0].precision = I64;
+
+    (*info)[1].name = OUTPUT_NAME_ATTENTION;
+    (*info)[1].dimsCount = 2;
+    (*info)[1].dims = (uint64_t*)malloc((*info)->dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[1].dims) != nullptr, "malloc has failed");
+    (*info)[1].dims[0] = -1;
+    (*info)[1].dims[1] = -1;
+    (*info)[1].precision = I64;
+
+    (*info)[2].name = OUTPUT_NAME_POSITION;
+    (*info)[2].dimsCount = 2;
+    (*info)[2].dims = (uint64_t*)malloc((*info)->dimsCount * sizeof(uint64_t));
+    NODE_ASSERT(((*info)[2].dims) != nullptr, "malloc has failed");
+    (*info)[2].dims[0] = -1;
+    (*info)[2].dims[1] = -1;
+    (*info)[2].precision = I64;
+    return 0;
+}
+
+int release(void* ptr, void* customNodeLibraryInternalManager) {
+    free(ptr);
+    return 0;
+}
diff --git a/src/custom_nodes/tokenizer/test/CMakeLists.txt b/src/custom_nodes/tokenizer/test/CMakeLists.txt
new file mode 100644
index 00000000..741bb7f8
--- /dev/null
+++ b/src/custom_nodes/tokenizer/test/CMakeLists.txt
@@ -0,0 +1,48 @@
+#
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+cmake_minimum_required(VERSION 3.10)
+project(test)
+
+set(CMAKE_CXX_STANDARD 17)
+
+# Download and install Google Test
+include(FetchContent)
+FetchContent_Declare(
+  googletest
+  GIT_REPOSITORY https://github.com/google/googletest.git
+  GIT_TAG        release-1.11.0
+)
+FetchContent_MakeAvailable(googletest)
+
+set(UTIL_DIRS
+    ../src/             # for model.hpp
+    ../../../)          # for custom_node_interface.h
+
+file(DOWNLOAD
+    https://github.com/microsoft/BlingFire/raw/${BLINGFIRE_SHA}/ldbsrc/ldb/gpt2.bin
+    ${CMAKE_BINARY_DIR}/gpt2.bin)
+
+file(DOWNLOAD
+    https://github.com/microsoft/BlingFire/raw/${BLINGFIRE_SHA}/ldbsrc/ldb/gpt2.i2w
+    ${CMAKE_BINARY_DIR}/gpt2.i2w)
+
+add_executable(tokenization_test tokenization_test.cpp)
+target_include_directories(tokenization_test PRIVATE ${UTIL_DIRS})
+target_link_libraries(tokenization_test gtest_main tokenizer)
+
+add_executable(detokenization_test detokenization_test.cpp)
+target_include_directories(detokenization_test PRIVATE ${UTIL_DIRS})
+target_link_libraries(detokenization_test gtest_main detokenizer)
diff --git a/src/custom_nodes/tokenizer/test/detokenization_test.cpp b/src/custom_nodes/tokenizer/test/detokenization_test.cpp
new file mode 100644
index 00000000..60f2902b
--- /dev/null
+++ b/src/custom_nodes/tokenizer/test/detokenization_test.cpp
@@ -0,0 +1,280 @@
+//*****************************************************************************
+// Copyright 2023 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//*****************************************************************************
+#include <cstring>
+#include <string>
+#include <vector>
+
+#include <gtest/gtest.h>
+
+#include "custom_node_interface.h"  // NOLINT
+#include "model.hpp"
+
+using namespace custom_nodes::tokenizer;
+
+#define TEST_MODEL_FILE_PATH "./gpt2.i2w"
+
+#define INPUT_NAME_LOGITS "logits"
+#define INPUT_NAME_PREVIOUS_TOKENS "input_ids"
+#define INPUT_NAME_PREVIOUS_ATTENTION "attention_mask"
+
+#define OUTPUT_NAME_TEXTS "texts"
+
+TEST(DetokenizerTest, Run) {
+    BlingFireModel model(TEST_MODEL_FILE_PATH);
+    int maxBufferLen = 1024;
+    auto result = model.detokenize({23294, 241, 22174, 28618, 2515, 94, 31676}, maxBufferLen);
+    ASSERT_EQ(result, "ã“ã‚“ã«ã¡ã¯");
+}
+
+TEST(DetokenizerTest, Run_TooSmallBuffer) {
+    BlingFireModel model(TEST_MODEL_FILE_PATH);
+    int maxBufferLen = 4;
+    auto result = model.detokenize({23294, 241, 22174, 28618, 2515, 94, 31676}, maxBufferLen);
+    ASSERT_EQ(result, "ã“");
+}
+
+TEST(DetokenizerTest, init_deinit) {
+    void* model = nullptr;
+    struct CustomNodeParam params[1];
+    params[0].key = "model_path";
+    params[0].value = TEST_MODEL_FILE_PATH;
+    int ret = initialize(&model, params, 1);
+    ASSERT_EQ(ret, 0);
+    ASSERT_NE(model, nullptr);
+
+    ret = deinitialize(model);
+    ASSERT_EQ(ret, 0);
+
+    model = nullptr;
+    params[0].value = "../invalid.bin";
+    ret = initialize(&model, params, 1);
+    ASSERT_NE(ret, 0);
+    ASSERT_EQ(model, nullptr);
+
+    ret = deinitialize(model);
+    ASSERT_EQ(ret, 0);
+}
+
+TEST(DetokenizerTest, inputs_info) {
+    struct CustomNodeTensorInfo* info = nullptr;
+    int infoCount = 0;
+    struct CustomNodeParam params[1];
+    params[0].key = "model_path";
+    params[0].value = TEST_MODEL_FILE_PATH;
+
+    BlingFireModel model(params[0].value);
+
+    int ret = getInputsInfo(&info, &infoCount, params, 1, (void*)&model);
+    ASSERT_EQ(ret, 0);
+    ASSERT_EQ(infoCount, 3);
+
+    ASSERT_EQ(std::strcmp(info[0].name, INPUT_NAME_LOGITS), 0);
+    ASSERT_EQ(info[0].dimsCount, 3);
+    ASSERT_EQ(info[0].dims[0], -1);
+    ASSERT_EQ(info[0].dims[1], -1);
+    ASSERT_EQ(info[0].dims[2], -1);
+    ASSERT_EQ(info[0].precision, FP32);
+
+    ASSERT_EQ(std::strcmp(info[1].name, INPUT_NAME_PREVIOUS_TOKENS), 0);
+    ASSERT_EQ(info[1].dimsCount, 2);
+    ASSERT_EQ(info[1].dims[0], -1);
+    ASSERT_EQ(info[1].dims[1], -1);
+    ASSERT_EQ(info[1].precision, I64);
+
+    ASSERT_EQ(std::strcmp(info[2].name, INPUT_NAME_PREVIOUS_ATTENTION), 0);
+    ASSERT_EQ(info[2].dimsCount, 2);
+    ASSERT_EQ(info[2].dims[0], -1);
+    ASSERT_EQ(info[2].dims[1], -1);
+    ASSERT_EQ(info[2].precision, I64);
+
+    ret = release(info, (void*)&model);
+    ASSERT_EQ(ret, 0);
+}
+
+TEST(DetokenizerTest, outputs_info) {
+    struct CustomNodeTensorInfo* info = nullptr;
+    int infoCount = 0;
+    struct CustomNodeParam params[1];
+    params[0].key = "model_path";
+    params[0].value = TEST_MODEL_FILE_PATH;
+
+    BlingFireModel model(params[0].value);
+
+    int ret = getOutputsInfo(&info, &infoCount, params, 1, (void*)&model);
+    ASSERT_EQ(ret, 0);
+    ASSERT_EQ(infoCount, 1);
+
+    ASSERT_EQ(std::strcmp(info[0].name, "texts"), 0);
+    ASSERT_EQ(info[0].dimsCount, 2);
+    ASSERT_EQ(info[0].dims[0], -1);
+    ASSERT_EQ(info[0].dims[1], -1);
+    ASSERT_EQ(info[0].precision, U8);
+
+    ret = release(info, (void*)&model);
+    ASSERT_EQ(ret, 0);
+}
+
+static void prepare(std::vector<float> data, std::vector<size_t> shape, std::vector<std::vector<int64_t>> previousTokens, struct CustomNodeTensor* tensors) {
+    // logits
+    struct CustomNodeTensor* tensor = &tensors[0];
+    tensor->dataBytes = data.size() * sizeof(float);
+    tensor->data = (uint8_t*)malloc(tensor->dataBytes);
+    std::memcpy(tensor->data, reinterpret_cast<uint8_t*>(data.data()), tensor->dataBytes);
+
+    tensor->dimsCount = shape.size();
+    tensor->dims = (uint64_t*)malloc(tensor->dimsCount * sizeof(uint64_t));
+    int i = 0;
+    for (size_t dim : shape) {
+        tensor->dims[i] = dim;
+        i++;
+    }
+
+    tensor->precision = FP32;
+    tensor->name = INPUT_NAME_LOGITS;
+    // [2, 8, 50400]
+
+    // input_ids
+    tensor = &tensors[1];
+    tensor->dataBytes = shape[0] * shape[1] * sizeof(int64_t);
+    tensor->data = (uint8_t*)malloc(tensor->dataBytes);
+    for (int i = 0; i < shape[0]; i++) {
+        std::memcpy(
+            tensor->data + i * shape[1] * sizeof(int64_t),
+            reinterpret_cast<uint8_t*>(previousTokens[i].data()), previousTokens[i].size() * sizeof(int64_t));
+    }
+
+    tensor->dimsCount = 2;
+    tensor->dims = (uint64_t*)malloc(tensor->dimsCount * sizeof(uint64_t));
+    tensor->dims[0] = shape[0];
+    tensor->dims[1] = shape[1];
+
+    tensor->precision = I64;
+    tensor->name = INPUT_NAME_PREVIOUS_TOKENS;
+    // [2, 8]
+
+    // attention_mask
+    tensor = &tensors[2];
+    tensor->dataBytes = shape[0] * shape[1] * sizeof(int64_t);
+    tensor->data = (uint8_t*)malloc(tensor->dataBytes);
+    for (int i = 0; i < shape[0]; i++) {
+        for (int j = 0; j < shape[1]; j++) {
+            if (j < previousTokens[i].size()) {
+                reinterpret_cast<int64_t*>(tensor->data)[i * shape[1] + j] = 1;
+            } else {
+                reinterpret_cast<int64_t*>(tensor->data)[i * shape[1] + j] = 0;
+            }
+        }
+    }
+
+    tensor->dimsCount = 2;
+    tensor->dims = (uint64_t*)malloc(tensor->dimsCount * sizeof(uint64_t));
+    tensor->dims[0] = shape[0];
+    tensor->dims[1] = shape[1];
+
+    tensor->precision = I64;
+    tensor->name = INPUT_NAME_PREVIOUS_ATTENTION;
+    // [2, 8]
+}
+
+class DetokenizerFixtureTest : public ::testing::Test {
+protected:
+    void run(std::vector<float> data, std::vector<size_t> shape, std::vector<std::vector<int64_t>> previousTokens, std::vector<std::string>& out) {
+        ASSERT_EQ(shape.size(), 3);
+        struct CustomNodeTensor inputs[3];
+        struct CustomNodeTensor* outputs = nullptr;
+        int outputsCount = 0;
+        prepare(data, shape, previousTokens, inputs);
+        int ret = execute(inputs, 3, &outputs, &outputsCount, params, 3, model);
+        for (int i = 0; i < 3; i++) {
+            free(inputs[i].data);
+            free(inputs[i].dims);
+        }
+        ASSERT_EQ(ret, 0);
+        ASSERT_EQ(outputsCount, 1);
+        std::vector<std::string> results;
+        results.resize(outputs->dims[0]);
+        for (int i = 0; i < outputsCount; i++) {
+            if (std::strcmp(outputs[i].name, "texts") == 0) {
+                for (int j = 0; j < outputs[i].dims[0]; j++) {
+                    char* str = (char*)outputs[i].data + j * outputs[i].dims[1];
+                    results[j] = std::string(str);
+                }
+            } else {
+                FAIL() << "Unknown output name: " << outputs[i].name;
+            }
+        }
+        out = results;
+        ASSERT_EQ(release(outputs, model), 0);
+    }
+    void SetUp() override {
+        params[0].key = "model_path";
+        params[0].value = TEST_MODEL_FILE_PATH;
+        params[1].key = "max_buffer_length";
+        params[1].value = "1024";
+        params[2].key = "debug";
+        params[2].value = "true";
+        int ret = initialize(&model, params, 3);
+        ASSERT_EQ(ret, 0);
+        ASSERT_NE(model, nullptr);
+    }
+    void TearDown() override {
+        int ret = deinitialize(model);
+        ASSERT_EQ(ret, 0);
+    }
+    struct CustomNodeParam params[3];
+    void* model = nullptr;
+};
+
+TEST_F(DetokenizerFixtureTest, execute) {
+    std::vector<std::string> outputs;
+
+    // single batch, single previous token
+    run({1.0, 2.0, 3.0, 1.5}, {1, 1, 4}, {{18435}}, outputs);
+    ASSERT_EQ(outputs.size(), 1);
+    ASSERT_EQ(outputs[0], "Hello#");
+
+    // single batch, 3 previous tokens
+    outputs.clear();
+    run({9.4, 0.2, -0.82, -0.74, 4.2, 1.9, 0.2, 0.95, /**/ 1.0, 2.0, 3.0, 1.5 /**/}, {1, 3, 4}, {{23294, 241, 22174}}, outputs);
+    ASSERT_EQ(outputs.size(), 1);
+    ASSERT_EQ(outputs[0], "ã“ã‚“#");
+
+    // single batch, 3 previous tokens, different token predicted
+    outputs.clear();
+    run({9.4, 0.2, -0.82, -0.74, 4.2, 1.9, 12.2, 0.95, /**/ 0.46, 1.18, 1.16, 1.02 /**/}, {1, 3, 4}, {{23294, 241, 22174}}, outputs);
+    ASSERT_EQ(outputs.size(), 1);
+    ASSERT_EQ(outputs[0], "ã“ã‚“\"");
+
+    // 2 batches, 2 previous tokens
+    outputs.clear();
+    run({9.4, 0.2, -0.82, -0.74, /*start 0*/ 0.46, 1.18, 1.16, 1.02 /*end 0*/, 4.2, 1.9, 0.2, 0.95, /*start 1*/ 1.0, 2.0, 3.0, 1.5 /*end 1*/}, {2, 2, 4}, {{18435, 995}, {18435, 995}}, outputs);
+    ASSERT_EQ(outputs.size(), 2);
+    ASSERT_EQ(outputs[0], "Hello world\"");
+    ASSERT_EQ(outputs[1], "Hello world#");
+
+    // 2 batches, different number of previous tokens (labeled by attention mask under the hood)
+    outputs.clear();
+    run({9.4, 0.2, -0.82, -0.74, /*start 0*/ 0.46, 1.18, 1.16, 1.02 /*end 0*/, /*start 1*/ 4.2, 1.9, 0.2, 0.95, /*end 1*/ 1.0, 2.0, 3.0, 1.5}, {2, 2, 4}, {{18435, 995}, {18435}}, outputs);
+    ASSERT_EQ(outputs.size(), 2);
+    ASSERT_EQ(outputs[0], "Hello world\"");
+    ASSERT_EQ(outputs[1], "Hello!");
+
+    outputs.clear();
+    run({9.4, 0.2, -0.82, -0.74, /*start 0*/ 0.46, 1.18, 1.16, 1.02 /*end 0*/, /*start 1*/ 4.2, 1.9, 0.2, 0.95, /*end 1*/ 1.0, 2.0, 3.0, 1.5}, {2, 2, 4}, {{18435, 995}, {}}, outputs);
+    ASSERT_EQ(outputs.size(), 2);
+    ASSERT_EQ(outputs[0], "Hello world\"");
+    ASSERT_EQ(outputs[1], "!");
+}
diff --git a/src/custom_nodes/tokenizer/test/tokenization_test.cpp b/src/custom_nodes/tokenizer/test/tokenization_test.cpp
new file mode 100644
index 00000000..0a32e938
--- /dev/null
+++ b/src/custom_nodes/tokenizer/test/tokenization_test.cpp
@@ -0,0 +1,246 @@
+//*****************************************************************************
+// Copyright 2023 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//*****************************************************************************
+#include <cstring>
+#include <string>
+#include <vector>
+
+#include <gtest/gtest.h>
+
+#include "custom_node_interface.h"  // NOLINT
+#include "model.hpp"
+
+#define TEST_MODEL_FILE_PATH "./gpt2.bin"
+
+#define INPUT_NAME_TEXTS "texts"
+
+#define OUTPUT_NAME_TOKENS "input_ids"
+#define OUTPUT_NAME_ATTENTION "attention_mask"
+#define OUTPUT_NAME_POSITION "position_ids"
+
+using namespace custom_nodes::tokenizer;
+
+TEST(TokenizerTest, Run) {
+    BlingFireModel model(TEST_MODEL_FILE_PATH);
+    int maxIdArrLen = 1024;
+    auto result = model.tokenize("ã“ã‚“ã«ã¡ã¯", maxIdArrLen);
+    std::vector<int64_t> expected = {23294, 241, 22174, 28618, 2515, 94, 31676};
+    ASSERT_EQ(result.size(), expected.size());
+    for (int i = 0; i < result.size(); i++) {
+        EXPECT_EQ(result[i], expected[i]) << "expected: " << expected[i] << "; actual: " << result[i];
+    }
+}
+
+TEST(TokenizerTest, Run_TooSmallBuffer) {
+    BlingFireModel model(TEST_MODEL_FILE_PATH);
+    int maxIdArrLen = 4;
+    auto result = model.tokenize("ã“ã‚“ã«ã¡ã¯", maxIdArrLen);
+    std::vector<int64_t> expected = {23294, 241, 22174, 28618};
+    ASSERT_EQ(result.size(), expected.size());
+    for (int i = 0; i < result.size(); i++) {
+        EXPECT_EQ(result[i], expected[i]) << "expected: " << expected[i] << "; actual: " << result[i];
+    }
+}
+
+TEST(TokenizerTest, init_deinit) {
+    void* model = nullptr;
+    struct CustomNodeParam params[1];
+    params[0].key = "model_path";
+    params[0].value = TEST_MODEL_FILE_PATH;
+    int ret = initialize(&model, params, 1);
+    ASSERT_EQ(ret, 0);
+    ASSERT_NE(model, nullptr);
+
+    ret = deinitialize(model);
+    ASSERT_EQ(ret, 0);
+
+    model = nullptr;
+    params[0].value = "../invalid.bin";
+    ret = initialize(&model, params, 1);
+    ASSERT_NE(ret, 0);
+    ASSERT_EQ(model, nullptr);
+
+    ret = deinitialize(model);
+    ASSERT_EQ(ret, 0);
+}
+
+TEST(TokenizerTest, inputs_info) {
+    struct CustomNodeTensorInfo* info = nullptr;
+    int infoCount = 0;
+    struct CustomNodeParam params[1];
+    params[0].key = "model_path";
+    params[0].value = TEST_MODEL_FILE_PATH;
+
+    BlingFireModel model(params[0].value);
+
+    int ret = getInputsInfo(&info, &infoCount, params, 1, (void*)&model);
+    ASSERT_EQ(ret, 0);
+    ASSERT_EQ(infoCount, 1);
+    ASSERT_EQ(std::strcmp(info[0].name, INPUT_NAME_TEXTS), 0);
+    ASSERT_EQ(info[0].dimsCount, 2);
+    ASSERT_EQ(info[0].dims[0], -1);
+    ASSERT_EQ(info[0].dims[1], -1);
+    ASSERT_EQ(info[0].precision, U8);
+    ret = release(info, (void*)&model);
+    ASSERT_EQ(ret, 0);
+}
+
+TEST(TokenizerTest, outputs_info) {
+    struct CustomNodeTensorInfo* info = nullptr;
+    int infoCount = 0;
+    struct CustomNodeParam params[1];
+    params[0].key = "model_path";
+    params[0].value = TEST_MODEL_FILE_PATH;
+
+    BlingFireModel model(params[0].value);
+
+    int ret = getOutputsInfo(&info, &infoCount, params, 1, (void*)&model);
+    ASSERT_EQ(ret, 0);
+    ASSERT_EQ(infoCount, 3);
+
+    ASSERT_EQ(std::strcmp(info[0].name, OUTPUT_NAME_TOKENS), 0);
+    ASSERT_EQ(info[0].dimsCount, 2);
+    ASSERT_EQ(info[0].dims[0], -1);
+    ASSERT_EQ(info[0].dims[1], -1);
+    ASSERT_EQ(info[0].precision, I64);
+
+    ASSERT_EQ(std::strcmp(info[1].name, OUTPUT_NAME_ATTENTION), 0);
+    ASSERT_EQ(info[1].dimsCount, 2);
+    ASSERT_EQ(info[1].dims[0], -1);
+    ASSERT_EQ(info[1].dims[1], -1);
+    ASSERT_EQ(info[1].precision, I64);
+
+    ASSERT_EQ(std::strcmp(info[2].name, OUTPUT_NAME_POSITION), 0);
+    ASSERT_EQ(info[1].dimsCount, 2);
+    ASSERT_EQ(info[1].dims[0], -1);
+    ASSERT_EQ(info[1].dims[1], -1);
+    ASSERT_EQ(info[1].precision, I64);
+
+    ret = release(info, (void*)&model);
+    ASSERT_EQ(ret, 0);
+}
+
+static void putStringsToTensor(std::vector<std::string> strings, struct CustomNodeTensor& tensor) {
+    size_t maxStringLength = 0;
+    for (auto& str : strings) {
+        maxStringLength = std::max(str.size(), maxStringLength);
+    }
+    size_t width = maxStringLength + 1;
+
+    tensor.dataBytes = strings.size() * width * sizeof(uint8_t);
+    tensor.data = (uint8_t*)malloc(tensor.dataBytes);
+
+    int i = 0;
+    for (auto& str : strings) {
+        std::memcpy(tensor.data + i * width, str.c_str(), str.size());
+        tensor.data[i * width + str.size()] = 0;
+        i++;
+    }
+
+    tensor.dimsCount = 2;
+    tensor.dims = (uint64_t*)malloc(2 * sizeof(uint64_t));
+    tensor.dims[0] = strings.size();
+    tensor.dims[1] = width;
+
+    tensor.precision = U8;
+    tensor.name = INPUT_NAME_TEXTS;
+}
+
+class TokenizerFixtureTest : public ::testing::Test {
+protected:
+    struct output {
+        std::vector<int64_t> tokens;
+        std::vector<int64_t> attention;
+        std::vector<int64_t> position;
+    };
+    void run(std::vector<std::string> in, std::vector<output>& out) {
+        struct CustomNodeTensor inputs[1];
+        struct CustomNodeTensor* outputs = nullptr;
+        int outputsCount = 0;
+        putStringsToTensor(in, inputs[0]);
+        int ret = execute(inputs, 1, &outputs, &outputsCount, params, 3, model);
+        free(inputs[0].data);
+        free(inputs[0].dims);
+        ASSERT_EQ(ret, 0);
+        ASSERT_EQ(outputsCount, 3);
+        std::vector<output> result;
+        result.resize(outputs->dims[0]);
+        for (int i = 0; i < outputsCount; i++) {
+            if (std::strcmp(outputs[i].name, OUTPUT_NAME_ATTENTION) == 0) {
+                for (int j = 0; j < outputs[i].dims[0]; j++) {
+                    result[j].attention = std::vector<int64_t>(
+                        (int64_t*)outputs[i].data + j * outputs[i].dims[1],
+                        (int64_t*)outputs[i].data + j * outputs[i].dims[1] + outputs[i].dims[1]);
+                }
+            } else if (std::strcmp(outputs[i].name, OUTPUT_NAME_TOKENS) == 0) {
+                for (int j = 0; j < outputs[i].dims[0]; j++) {
+                    result[j].tokens = std::vector<int64_t>(
+                        (int64_t*)outputs[i].data + j * outputs[i].dims[1],
+                        (int64_t*)outputs[i].data + j * outputs[i].dims[1] + outputs[i].dims[1]);
+                }
+            } else if (std::strcmp(outputs[i].name, OUTPUT_NAME_POSITION) == 0) {
+                for (int j = 0; j < outputs[i].dims[0]; j++) {
+                    result[j].position = std::vector<int64_t>(
+                        (int64_t*)outputs[i].data + j * outputs[i].dims[1],
+                        (int64_t*)outputs[i].data + j * outputs[i].dims[1] + outputs[i].dims[1]);
+                }
+            } else {
+                FAIL() << "Unknown output name: " << outputs[i].name;
+            }
+        }
+        out = result;
+        ASSERT_EQ(release(outputs, model), 0);
+    }
+    void SetUp() override {
+        params[0].key = "model_path";
+        params[0].value = TEST_MODEL_FILE_PATH;
+        params[1].key = "max_ids_arr_length";
+        params[1].value = "1024";
+        params[2].key = "debug";
+        params[2].value = "true";
+        int ret = initialize(&model, params, 3);
+        ASSERT_EQ(ret, 0);
+        ASSERT_NE(model, nullptr);
+    }
+    void TearDown() override {
+        int ret = deinitialize(model);
+        ASSERT_EQ(ret, 0);
+    }
+    struct CustomNodeParam params[3];
+    void* model = nullptr;
+};
+
+TEST_F(TokenizerFixtureTest, execute) {
+    std::vector<output> outputs;
+    run({"", "Hello world!", "ã“ã‚“ã«ã¡ã¯"}, outputs);
+    ASSERT_EQ(outputs.size(), 3);
+
+    // ""
+    ASSERT_EQ(outputs[0].tokens.size(), 7);
+    ASSERT_EQ(outputs[0].attention.size(), 7);
+    ASSERT_EQ(std::memcmp(outputs[0].attention.data(), std::vector<int64_t>{0, 0, 0, 0, 0, 0, 0}.data(), 7 * sizeof(int64_t)), 0);
+
+    // "Hello world!"
+    ASSERT_EQ(outputs[1].tokens.size(), 7);
+    ASSERT_EQ(outputs[1].attention.size(), 7);
+    ASSERT_EQ(std::memcmp(outputs[1].tokens.data(), std::vector<int64_t>{18435, 995, 0}.data(), 3 * sizeof(int64_t)), 0);
+    ASSERT_EQ(std::memcmp(outputs[1].attention.data(), std::vector<int64_t>{1, 1, 1, 0, 0, 0, 0}.data(), 7 * sizeof(int64_t)), 0);
+
+    // "ã“ã‚“ã«ã¡ã¯"
+    ASSERT_EQ(outputs[1].tokens.size(), 7);
+    ASSERT_EQ(outputs[1].attention.size(), 7);
+    ASSERT_EQ(std::memcmp(outputs[2].tokens.data(), std::vector<int64_t>{23294, 241, 22174, 28618, 2515, 94, 31676}.data(), 7 * sizeof(int64_t)), 0);
+    ASSERT_EQ(std::memcmp(outputs[2].attention.data(), std::vector<int64_t>{1, 1, 1, 1, 1, 1, 1}.data(), 7 * sizeof(int64_t)), 0);
+}
diff --git a/src/example/SampleCpuExtension/README.md b/src/example/SampleCpuExtension/README.md
index f42abd42..08a4bed8 100644
--- a/src/example/SampleCpuExtension/README.md
+++ b/src/example/SampleCpuExtension/README.md
@@ -8,7 +8,7 @@ custom extension execution.
 
 ## Creating cpu_extension library
 
-Compile the library by running `make cpu_extension BASE_OS=ubuntu` in root directory of [Model Server repository](https://github.com/openvinotoolkit/model_server/tree/main). The implementation of this library slightly differs from the template in OpenVINOâ„¢ repository and can be found in [SampleCpuExtension directory](https://github.com/openvinotoolkit/model_server/tree/main/src/example/SampleCpuExtension).
+Compile the library by running `make cpu_extension BASE_OS=ubuntu` in root directory of [Model Server repository](https://github.com/openvinotoolkit/model_server/tree/main). The implementation of this library slightly differs from the template in OpenVINOâ„¢ repository and can be found in [SampleCpuExtension directory](https://github.com/openvinotoolkit/model_server/tree/releases/2024/1/src/example/SampleCpuExtension).
 
 Shared library will be generated in the `lib` folder. Such library can be used to run Model Server, using `--cpu_extension` argument.
 
diff --git a/src/kfs_frontend/kfs_graph_executor_impl.cpp b/src/kfs_frontend/kfs_graph_executor_impl.cpp
deleted file mode 100644
index 4e574cdf..00000000
--- a/src/kfs_frontend/kfs_graph_executor_impl.cpp
+++ /dev/null
@@ -1,1204 +0,0 @@
-//*****************************************************************************
-// Copyright 2024 Intel Corporation
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-//*****************************************************************************
-#include "kfs_graph_executor_impl.hpp"
-
-#include <sstream>
-#include <string>
-#include <unordered_map>
-#include <utility>
-#include <vector>
-
-#include "../kfs_frontend/kfs_utils.hpp"
-#include "../logging.hpp"
-#include "../mediapipe_internal/mediapipe_utils.hpp"
-#include "../mediapipe_internal/mediapipegraphdefinition.hpp"
-#include "../predict_request_validation_utils.hpp"
-#include "../status.hpp"
-#include "../tfs_frontend/tfs_utils.hpp"
-
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
-#include "mediapipe/framework/calculator_graph.h"
-#include "mediapipe/framework/formats/image_frame.h"
-#include "mediapipe/framework/formats/image_frame_opencv.h"
-#pragma GCC diagnostic pop
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wall"
-#include "mediapipe/framework/formats/tensor.h"
-#include "mediapipe/framework/port/status.h"
-#pragma GCC diagnostic pop
-#include "opencv2/opencv.hpp"
-
-#if (PYTHON_DISABLE == 0)
-#include <pybind11/embed.h>
-#include <pybind11/pybind11.h>
-#include <pybind11/stl.h>
-
-#include "../python/python_backend.hpp"
-#include "../python/pythonnoderesources.hpp"
-#include "src/python/ovms_py_tensor.hpp"
-namespace py = pybind11;
-#endif
-
-namespace ovms {
-
-// Utilities
-
-using namespace request_validation_utils;
-
-const std::string TIMESTAMP_PARAMETER_NAME = "OVMS_MP_TIMESTAMP";
-
-static mediapipe::Tensor::ElementType KFSPrecisionToMPPrecision(const KFSDataType& kfsDatatype) {
-    static std::unordered_map<KFSDataType, mediapipe::Tensor::ElementType> precisionMap{
-        //        {"FP64", mediapipe::Tensor::ElementType::},
-        {"FP32", mediapipe::Tensor::ElementType::kFloat32},
-        {"FP16", mediapipe::Tensor::ElementType::kFloat16},
-        //        {"INT64", mediapipe::Tensor::ElementType::},
-        {"INT32", mediapipe::Tensor::ElementType::kInt32},
-        //        {"INT16", mediapipe::Tensor::ElementType::},
-        {"INT8", mediapipe::Tensor::ElementType::kInt8},
-        //        {"UINT64", mediapipe::Tensor::ElementType::},
-        //        {"UINT32", mediapipe::Tensor::ElementType::},
-        //        {"UINT16", mediapipe::Tensor::ElementType::},
-        {"UINT8", mediapipe::Tensor::ElementType::kUInt8},
-        {"BOOL", mediapipe::Tensor::ElementType::kBool}
-        //        {"", ov::element::Type_t::, mediapipe::Tensor::ElementType::kChar}
-    };
-    auto it = precisionMap.find(kfsDatatype);
-    if (it == precisionMap.end()) {
-        return mediapipe::Tensor::ElementType::kNone;
-    }
-    return it->second;
-}
-
-#define SET_DATA_FROM_MP_TENSOR(TENSOR, VIEW_TYPE)                                                     \
-    switch ((TENSOR)->element_type()) {                                                                \
-    case mediapipe::Tensor::ElementType::kFloat32:                                                     \
-    case mediapipe::Tensor::ElementType::kFloat16:                                                     \
-        data = reinterpret_cast<void*>(const_cast<float*>((TENSOR)->VIEW_TYPE().buffer<float>()));     \
-        break;                                                                                         \
-    case mediapipe::Tensor::ElementType::kUInt8:                                                       \
-        data = reinterpret_cast<void*>(const_cast<uint8_t*>((TENSOR)->VIEW_TYPE().buffer<uint8_t>())); \
-        break;                                                                                         \
-    case mediapipe::Tensor::ElementType::kInt8:                                                        \
-        data = reinterpret_cast<void*>(const_cast<int8_t*>((TENSOR)->VIEW_TYPE().buffer<int8_t>()));   \
-        break;                                                                                         \
-    case mediapipe::Tensor::ElementType::kInt32:                                                       \
-        data = reinterpret_cast<void*>(const_cast<int32_t*>((TENSOR)->VIEW_TYPE().buffer<int32_t>())); \
-        break;                                                                                         \
-    case mediapipe::Tensor::ElementType::kBool:                                                        \
-        data = reinterpret_cast<void*>(const_cast<bool*>((TENSOR)->VIEW_TYPE().buffer<bool>()));       \
-        break;                                                                                         \
-    default:                                                                                           \
-        data = reinterpret_cast<void*>(const_cast<void*>((TENSOR)->VIEW_TYPE().buffer<void>()));       \
-    }
-
-#define HANDLE_PACKET_RECEIVAL_EXCEPTIONS()                           \
-    catch (const std::exception& e) {                                 \
-        std::stringstream ss;                                         \
-        ss << "Failed to get packet"                                  \
-           << outputStreamName                                        \
-           << " with exception: "                                     \
-           << e.what();                                               \
-        std::string details{ss.str()};                                \
-        SPDLOG_DEBUG(details);                                        \
-        return Status(StatusCode::UNKNOWN_ERROR, std::move(details)); \
-    }                                                                 \
-    catch (...) {                                                     \
-        std::stringstream ss;                                         \
-        ss << "Failed to get packet"                                  \
-           << outputStreamName                                        \
-           << " with exception.";                                     \
-        std::string details{ss.str()};                                \
-        SPDLOG_DEBUG(details);                                        \
-        return Status(StatusCode::UNKNOWN_ERROR, std::move(details)); \
-    }
-
-const KFSDataType EMPTY_PREC = "";
-
-static const KFSDataType& MPPrecisionToKFSPrecision(::mediapipe::Tensor::ElementType precision) {
-    static std::unordered_map<mediapipe::Tensor::ElementType, KFSDataType> precisionMap{
-        //        {mediapipe::Tensor::ElementType::, "FP64"},
-        {mediapipe::Tensor::ElementType::kFloat32, "FP32"},
-        {mediapipe::Tensor::ElementType::kFloat16, "FP16"},
-        //        {mediapipe::Tensor::ElementType::, "INT64"},
-        {mediapipe::Tensor::ElementType::kInt32, "INT32"},
-        //        {mediapipe::Tensor::ElementType::, "INT16"},
-        {mediapipe::Tensor::ElementType::kInt8, "INT8"},
-        //        {mediapipe::Tensor::ElementType::, "UINT64"},
-        //        {mediapipe::Tensor::ElementType::, "UINT32"},
-        //        {mediapipe::Tensor::ElementType::, "UINT16"},
-        {mediapipe::Tensor::ElementType::kUInt8, "UINT8"},
-        {mediapipe::Tensor::ElementType::kBool, "BOOL"}
-        //        {"", ov::element::Type_t::, mediapipe::Tensor::ElementType::kChar}
-    };
-    auto it = precisionMap.find(precision);
-    if (it == precisionMap.end()) {
-        SPDLOG_WARN("Unsupported precision passed from Mediapipe graph");
-        return EMPTY_PREC;
-    }
-    return it->second;
-}
-
-template <typename T>
-static Status receiveAndSerializePacket(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName);
-
-template <>
-Status receiveAndSerializePacket<tensorflow::Tensor>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
-    try {
-        auto received = packet.Get<tensorflow::Tensor>();
-        auto* output = response.add_outputs();
-        output->set_name(outputStreamName);
-        output->set_datatype(
-            ovmsPrecisionToKFSPrecision(
-                TFSPrecisionToOvmsPrecision(
-                    received.dtype())));
-        output->clear_shape();
-        for (const auto& dim : received.shape()) {
-            output->add_shape(dim.size);
-        }
-        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(received.data()), received.TotalBytes());
-        return StatusCode::OK;
-    }
-    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
-}
-
-template <>
-Status receiveAndSerializePacket<::mediapipe::Tensor>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
-    try {
-        const ::mediapipe::Tensor& received = packet.Get<::mediapipe::Tensor>();
-        auto* output = response.add_outputs();
-        output->set_name(outputStreamName);
-        output->set_datatype(MPPrecisionToKFSPrecision(received.element_type()));
-        output->clear_shape();
-        for (const auto& dim : received.shape().dims) {
-            output->add_shape(dim);
-        }
-        void* data;
-        SET_DATA_FROM_MP_TENSOR(&received, GetCpuReadView);
-        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(data), received.bytes());
-        return StatusCode::OK;
-    }
-    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
-}
-
-template <>
-Status receiveAndSerializePacket<ov::Tensor>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
-    try {
-        auto received = packet.Get<ov::Tensor>();
-        auto* output = response.add_outputs();
-        output->set_name(outputStreamName);
-        output->set_datatype(
-            ovmsPrecisionToKFSPrecision(
-                ovElementTypeToOvmsPrecision(
-                    received.get_element_type())));
-        output->clear_shape();
-        for (const auto& dim : received.get_shape()) {
-            output->add_shape(dim);
-        }
-        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(received.data()), received.get_byte_size());
-        return StatusCode::OK;
-    }
-    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
-}
-
-template <>
-Status receiveAndSerializePacket<KFSResponse>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
-    try {
-        auto received = packet.Get<KFSResponse>();
-        response = std::move(received);
-        return StatusCode::OK;
-    }
-    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
-}
-
-static KFSDataType convertImageFormatToKFSDataType(const mediapipe::ImageFormat::Format& imageFormat) {
-    static std::unordered_map<mediapipe::ImageFormat::Format, KFSDataType> ImageFormatKFSDatatypeMap{
-        {mediapipe::ImageFormat::GRAY8, "UINT8"},
-        {mediapipe::ImageFormat::SRGB, "UINT8"},
-        {mediapipe::ImageFormat::SRGBA, "UINT8"},
-        {mediapipe::ImageFormat::GRAY16, "UINT16"},
-        {mediapipe::ImageFormat::SRGB48, "UINT16"},
-        {mediapipe::ImageFormat::SRGBA64, "UINT16"},
-        {mediapipe::ImageFormat::VEC32F1, "FP32"},
-        {mediapipe::ImageFormat::VEC32F2, "FP32"}};
-    auto it = ImageFormatKFSDatatypeMap.find(imageFormat);
-    if (it == ImageFormatKFSDatatypeMap.end()) {
-        SPDLOG_DEBUG("Converting Mediapipe::ImageFrame format to KFS datatype failed. Datatype will be set to default - UINT8");
-        return "UINT8";
-    }
-    return it->second;
-}
-
-template <>
-Status receiveAndSerializePacket<mediapipe::ImageFrame>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
-    try {
-        const auto& received = packet.Get<mediapipe::ImageFrame>();
-        auto* output = response.add_outputs();
-        output->set_name(outputStreamName);
-        KFSDataType datatype = convertImageFormatToKFSDataType(received.Format());
-        output->set_datatype(datatype);
-        output->clear_shape();
-        output->add_shape(received.Height());
-        output->add_shape(received.Width());
-        output->add_shape(received.NumberOfChannels());
-        cv::Mat image = mediapipe::formats::MatView(&received);
-
-        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(image.data), image.cols * image.rows * image.channels() * image.elemSize1());
-        return StatusCode::OK;
-    }
-    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
-}
-
-#if (PYTHON_DISABLE == 0)
-template <>
-Status receiveAndSerializePacket<PyObjectWrapper<py::object>>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
-    try {
-        const PyObjectWrapper<py::object>& pyOutput = packet.Get<PyObjectWrapper<py::object>>();
-        auto* output = response.add_outputs();
-        output->set_name(pyOutput.getProperty<std::string>("name"));
-        output->set_datatype(pyOutput.getProperty<std::string>("datatype"));
-        output->clear_shape();
-        for (const auto& dim : pyOutput.getProperty<std::vector<py::ssize_t>>("shape")) {
-            output->add_shape(dim);
-        }
-        void* ptr = pyOutput.getProperty<void*>("ptr");
-        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(ptr), pyOutput.getProperty<py::ssize_t>("size"));
-        return StatusCode::OK;
-    } catch (const pybind11::error_already_set& e) {
-        std::stringstream ss;
-        ss << "Failed to get packet " << outputStreamName << " due to Python object unpacking error: " << e.what();
-        std::string details{ss.str()};
-        SPDLOG_DEBUG(details);
-        return Status(StatusCode::UNKNOWN_ERROR, std::move(details));
-    }
-    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
-}
-#endif
-
-static Status getRequestInput(google::protobuf::internal::RepeatedPtrIterator<const inference::ModelInferRequest_InferInputTensor>& itr, const std::string& requestedName, const KFSRequest& request) {
-    auto requestInputItr = std::find_if(request.inputs().begin(), request.inputs().end(), [&requestedName](const ::KFSRequest::InferInputTensor& tensor) { return tensor.name() == requestedName; });
-    if (requestInputItr == request.inputs().end()) {
-        std::stringstream ss;
-        ss << "Required input: " << requestedName;
-        const std::string details = ss.str();
-        SPDLOG_DEBUG("[servable name: {} version: {}] Missing input with specific name - {}", request.model_name(), request.model_version(), details);
-        return Status(StatusCode::INVALID_MISSING_INPUT, details);
-    }
-    itr = requestInputItr;
-    return StatusCode::OK;
-}
-
-#define HANDLE_DESERIALIZATION_EXCEPTION(TYPE_STRING)                                                       \
-    catch (const std::exception& e) {                                                                       \
-        std::stringstream ss;                                                                               \
-        ss << "Exception:"                                                                                  \
-           << e.what()                                                                                      \
-           << "; caught during " TYPE_STRING " deserialization from KServe request tensor";                 \
-        std::string details = ss.str();                                                                     \
-        SPDLOG_DEBUG(details);                                                                              \
-        return Status(StatusCode::UNKNOWN_ERROR, std::move(details));                                       \
-    }                                                                                                       \
-    catch (...) {                                                                                           \
-        std::stringstream ss;                                                                               \
-        ss << "Unknown exception caught during " TYPE_STRING " deserialization from KServe request tensor"; \
-        std::string details = ss.str();                                                                     \
-        SPDLOG_DEBUG(details);                                                                              \
-        return Status(StatusCode::UNKNOWN_ERROR, std::move(details));                                       \
-    }
-
-#define COPY_INPUT_VALUE_BY_VALUE(TYPE, PROTO_PREFIX)                                                                                       \
-    TYPE* ptr = reinterpret_cast<TYPE*>(data);                                                                                              \
-    const auto& input = request.inputs(inputIndex);                                                                                         \
-    if (!input.has_contents()) {                                                                                                            \
-        return Status(StatusCode::INVALID_CONTENT_SIZE, "Input does not have input tensor contents field");                                 \
-    }                                                                                                                                       \
-    const auto& contents = input.contents();                                                                                                \
-    if (!contents.PROTO_PREFIX##_contents_size()) {                                                                                         \
-        return Status(StatusCode::INVALID_CONTENT_SIZE, "Input does not have proper size of input tensor " #PROTO_PREFIX "contents field"); \
-    }                                                                                                                                       \
-    for (auto& number : contents.PROTO_PREFIX##_contents()) {                                                                               \
-        *(ptr++) = number;                                                                                                                  \
-    }                                                                                                                                       \
-    break;
-
-static Status validateRawInputContent(const size_t expectedBytes, const std::string bufferLocation, const std::string& requestedName, const KFSRequest& request) {
-    if (expectedBytes != bufferLocation.size()) {
-        std::stringstream ss;
-        ss << "Expected: " << expectedBytes << " bytes; Actual: " << bufferLocation.size() << " bytes; input name: " << requestedName;
-        const std::string details = ss.str();
-        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid content size of tensor proto - {}", request.model_name(), request.model_version(), details);
-        return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-    }
-    return StatusCode::OK;
-}
-
-static Status validateInputContent(const KFSTensorInputProto& proto, const size_t expectedBytes, const std::string& requestedName, const KFSRequest& request) {
-    auto precision = KFSPrecisionToOvmsPrecision(proto.datatype());
-    size_t elementsCount = getElementsCount(proto, precision);
-    if (expectedBytes != KFSDataTypeSize(proto.datatype()) * elementsCount) {
-        std::stringstream ss;
-        ss << "Expected: " << expectedBytes << " values; Actual: " << KFSDataTypeSize(proto.datatype()) * elementsCount << " values; input name: " << requestedName;
-        const std::string details = ss.str();
-        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid value size of tensor proto - {}", request.model_name(), request.model_version(), details);
-        return Status(StatusCode::INVALID_VALUE_COUNT, details);
-    }
-    return StatusCode::OK;
-}
-
-static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<mediapipe::Tensor>& outTensor, PythonBackend* pythonBackend) {
-    auto requestInputItr = request.inputs().begin();
-    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
-    auto inputIndex = requestInputItr - request.inputs().begin();
-    try {
-        auto datatype = KFSPrecisionToMPPrecision(requestInputItr->datatype());
-        if (datatype == mediapipe::Tensor::ElementType::kNone) {
-            std::stringstream ss;
-            ss << "Not supported precision for Mediapipe tensor deserialization: " << requestInputItr->datatype();
-            const std::string details = ss.str();
-            SPDLOG_DEBUG(details);
-            return Status(StatusCode::INVALID_PRECISION, std::move(details));
-        }
-        std::vector<int> rawShape;
-        for (int i = 0; i < requestInputItr->shape().size(); i++) {
-            if (requestInputItr->shape()[i] <= 0) {
-                std::stringstream ss;
-                ss << "Negative or zero dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
-                const std::string details = ss.str();
-                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
-                return Status(StatusCode::INVALID_SHAPE, details);
-            }
-            rawShape.emplace_back(requestInputItr->shape()[i]);
-        }
-        mediapipe::Tensor::Shape tensorShape{rawShape};
-        outTensor = std::make_unique<mediapipe::Tensor>(datatype, tensorShape);
-        void* data;
-        SET_DATA_FROM_MP_TENSOR(outTensor, GetCpuWriteView);
-        ov::element::Type precision = ovmsPrecisionToIE2Precision(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
-        size_t expectedBytes = 1;
-        bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<int>(rawShape, precision.size(), expectedBytes);
-        if (!expectedBufferSizeValid) {
-            const std::string details = "Provided shape and datatype declare too large buffer.";
-            SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-            return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-        }
-        if (request.raw_input_contents().size()) {
-            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
-            OVMS_RETURN_ON_FAIL(validateRawInputContent(expectedBytes, bufferLocation, requestedName, request));
-            std::memcpy(data, bufferLocation.data(), bufferLocation.size());
-        } else {  // need to copy each value separately
-            OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
-            switch (datatype) {
-            case mediapipe::Tensor::ElementType::kFloat32: {
-                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
-            }
-            case mediapipe::Tensor::ElementType::kInt32: {
-                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
-            }
-            case mediapipe::Tensor::ElementType::kInt8: {
-                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
-            }
-            case mediapipe::Tensor::ElementType::kUInt8: {
-                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
-            }
-            case mediapipe::Tensor::ElementType::kBool: {
-                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
-            }
-            case mediapipe::Tensor::ElementType::kFloat16:
-            default:
-                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, i32, i8, u8, bool");
-            }
-        }
-    }
-    HANDLE_DESERIALIZATION_EXCEPTION("Mediapipe tensor")
-    return StatusCode::OK;
-}
-
-static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<tensorflow::Tensor>& outTensor, PythonBackend* pythonBackend) {
-    using tensorflow::Tensor;
-    using tensorflow::TensorShape;
-    auto requestInputItr = request.inputs().begin();
-    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
-    auto inputIndex = requestInputItr - request.inputs().begin();
-    try {
-        auto datatype = getPrecisionAsDataType(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
-        if (datatype == TFSDataType::DT_INVALID) {
-            std::stringstream ss;
-            ss << "Not supported precision for Tensorflow tensor deserialization: " << requestInputItr->datatype();
-            const std::string details = ss.str();
-            SPDLOG_DEBUG(details);
-            return Status(StatusCode::INVALID_PRECISION, std::move(details));
-        }
-        TensorShape tensorShape;
-        std::vector<int64_t> rawShape;
-        for (int i = 0; i < requestInputItr->shape().size(); i++) {
-            if (requestInputItr->shape()[i] < 0) {
-                std::stringstream ss;
-                ss << "Negative dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
-                const std::string details = ss.str();
-                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
-                return Status(StatusCode::INVALID_SHAPE, details);
-            }
-            rawShape.emplace_back(requestInputItr->shape()[i]);
-        }
-        int64_t dimsCount = rawShape.size();
-        auto abslStatus = tensorflow::TensorShapeUtils::MakeShape(rawShape.data(), dimsCount, &tensorShape);
-        if (!abslStatus.ok()) {
-            auto stringViewAbslMessage = abslStatus.message();
-            return Status(StatusCode::UNKNOWN_ERROR, std::string{stringViewAbslMessage});
-        }
-        abslStatus = TensorShape::BuildTensorShapeBase(rawShape, static_cast<tensorflow::TensorShapeBase<TensorShape>*>(&tensorShape));
-        if (!abslStatus.ok()) {
-            auto stringViewAbslMessage = abslStatus.message();
-            return Status(StatusCode::UNKNOWN_ERROR, std::string{stringViewAbslMessage});
-        }
-        size_t expectedBytes = 1;
-        bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<int64_t>(rawShape, KFSDataTypeSize(requestInputItr->datatype()), expectedBytes);
-        if (!expectedBufferSizeValid) {
-            const std::string details = "Provided shape and datatype declare too large buffer.";
-            SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-            return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-        }
-        outTensor = std::make_unique<tensorflow::Tensor>(datatype, tensorShape);
-        if (request.raw_input_contents().size()) {
-            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
-            if (outTensor->TotalBytes() != bufferLocation.size()) {
-                std::stringstream ss;
-                ss << "Mediapipe deserialization content size mismatch; allocated TF Tensor: " << outTensor->TotalBytes() << " bytes vs KServe buffer: " << bufferLocation.size() << " bytes";
-                const std::string details = ss.str();
-                SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-                return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-            }
-            void* tfTensordata = outTensor->data();
-            std::memcpy(tfTensordata, bufferLocation.data(), bufferLocation.size());
-        } else {
-            OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
-            void* data = outTensor->data();
-            switch (datatype) {
-            case TFSDataType::DT_FLOAT: {
-                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
-            }
-            case TFSDataType::DT_DOUBLE: {
-                COPY_INPUT_VALUE_BY_VALUE(double, fp64);
-            }
-            case TFSDataType::DT_INT64: {
-                COPY_INPUT_VALUE_BY_VALUE(int64_t, int64);
-            }
-            case TFSDataType::DT_INT32: {
-                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
-            }
-            case TFSDataType::DT_INT16: {
-                COPY_INPUT_VALUE_BY_VALUE(int16_t, int);
-            }
-            case TFSDataType::DT_INT8: {
-                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
-            }
-            case TFSDataType::DT_UINT64: {
-                COPY_INPUT_VALUE_BY_VALUE(uint64_t, uint64);
-            }
-            case TFSDataType::DT_UINT32: {
-                COPY_INPUT_VALUE_BY_VALUE(uint32_t, uint);
-            }
-            case TFSDataType::DT_UINT16: {
-                COPY_INPUT_VALUE_BY_VALUE(uint16_t, uint);
-            }
-            case TFSDataType::DT_UINT8: {
-                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
-            }
-            case TFSDataType::DT_BOOL: {
-                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
-            }
-            case TFSDataType::DT_HALF:
-            default:
-                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, int64, int32, uint32, uint64, int8, uint8, bool");
-            }
-        }
-    }
-    HANDLE_DESERIALIZATION_EXCEPTION("Tensorflow tensor")
-    return StatusCode::OK;
-}
-
-static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<ov::Tensor>& outTensor, PythonBackend* pythonBackend) {
-    auto requestInputItr = request.inputs().begin();
-    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
-    auto inputIndex = requestInputItr - request.inputs().begin();
-    try {
-        ov::Shape shape;
-        for (int i = 0; i < requestInputItr->shape().size(); i++) {
-            if (requestInputItr->shape()[i] < 0) {
-                std::stringstream ss;
-                ss << "Negative dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
-                const std::string details = ss.str();
-                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
-                return Status(StatusCode::INVALID_SHAPE, details);
-            }
-            shape.push_back(requestInputItr->shape()[i]);
-        }
-        ov::element::Type precision = ovmsPrecisionToIE2Precision(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
-        size_t expectedBytes = 1;
-        bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<size_t>(shape, precision.size(), expectedBytes);
-        if (!expectedBufferSizeValid) {
-            const std::string details = "Provided shape and datatype declare too large buffer.";
-            SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-            return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-        }
-        if (request.raw_input_contents().size()) {
-            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
-            OVMS_RETURN_ON_FAIL(validateRawInputContent(expectedBytes, bufferLocation, requestedName, request));
-            if (expectedBytes == 0) {
-                outTensor = std::make_unique<ov::Tensor>(precision, shape);  // OpenVINO does not accept nullptr as data ptr
-            } else {
-                outTensor = std::make_unique<ov::Tensor>(precision, shape, const_cast<void*>((const void*)bufferLocation.data()));
-            }
-        } else {
-            OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
-            if (expectedBytes == 0) {
-                return StatusCode::OK;
-            }
-            outTensor = std::make_unique<ov::Tensor>(precision, shape);
-            void* data = outTensor->data();
-            switch (precision) {
-            case ov::element::Type_t::f32: {
-                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
-            }
-            case ov::element::Type_t::i64: {
-                COPY_INPUT_VALUE_BY_VALUE(int64_t, int64);
-            }
-            case ov::element::Type_t::i32: {
-                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
-            }
-            case ov::element::Type_t::i16: {
-                COPY_INPUT_VALUE_BY_VALUE(int16_t, int);
-            }
-            case ov::element::Type_t::i8: {
-                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
-            }
-            case ov::element::Type_t::u64: {
-                COPY_INPUT_VALUE_BY_VALUE(uint64_t, uint64);
-            }
-            case ov::element::Type_t::u32: {
-                COPY_INPUT_VALUE_BY_VALUE(uint32_t, uint);
-            }
-            case ov::element::Type_t::u16: {
-                COPY_INPUT_VALUE_BY_VALUE(uint16_t, uint);
-            }
-            case ov::element::Type_t::u8: {
-                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
-            }
-            case ov::element::Type_t::boolean: {
-                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
-            }
-            case ov::element::Type_t::f64: {
-                COPY_INPUT_VALUE_BY_VALUE(double, fp64);
-            }
-            // the rest not supported by KFS
-            case ov::element::Type_t::u1:
-            case ov::element::Type_t::u4:
-            case ov::element::Type_t::i4:
-            case ov::element::Type_t::f16:
-            case ov::element::Type_t::bf16:
-            case ov::element::Type_t::undefined:
-            case ov::element::Type_t::dynamic:
-            default:
-                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, i64, i32, i16, i8, u64, u32, u16, u8, bool");
-            }
-        }
-    }
-    HANDLE_DESERIALIZATION_EXCEPTION("OpenVINO tensor")
-    return StatusCode::OK;
-}
-
-static mediapipe::ImageFormat::Format KFSDatatypeToImageFormat(const std::string& datatype, const size_t numberOfChannels) {
-    if (datatype == "FP32") {
-        if (numberOfChannels == 1) {
-            return mediapipe::ImageFormat::VEC32F1;
-        }
-        if (numberOfChannels == 2) {
-            return mediapipe::ImageFormat::VEC32F2;
-        }
-        if (numberOfChannels == 4) {
-            return mediapipe::ImageFormat::VEC32F4;
-        }
-    }
-    if (datatype == "UINT8" || datatype == "INT8") {
-        if (numberOfChannels == 1) {
-            return mediapipe::ImageFormat::GRAY8;
-        }
-        if (numberOfChannels == 3) {
-            return mediapipe::ImageFormat::SRGB;
-        }
-        if (numberOfChannels == 4) {
-            return mediapipe::ImageFormat::SRGBA;
-        }
-    }
-    if (datatype == "UINT16" || datatype == "INT16") {
-        if (numberOfChannels == 1) {
-            return mediapipe::ImageFormat::GRAY16;
-        }
-        if (numberOfChannels == 3) {
-            return mediapipe::ImageFormat::SRGB48;
-        }
-        if (numberOfChannels == 4) {
-            return mediapipe::ImageFormat::SRGBA64;
-        }
-    }
-    if (datatype == "FP16") {
-        if (numberOfChannels == 1) {
-            return mediapipe::ImageFormat::GRAY16;
-        }
-        if (numberOfChannels == 3) {
-            return mediapipe::ImageFormat::SRGB48;
-        }
-        if (numberOfChannels == 4) {
-            return mediapipe::ImageFormat::SRGBA64;
-        }
-    }
-    return mediapipe::ImageFormat::UNKNOWN;
-}
-
-static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<mediapipe::ImageFrame>& outTensor, PythonBackend* pythonBackend) {
-    auto requestInputItr = request.inputs().begin();
-    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
-    auto inputIndex = requestInputItr - request.inputs().begin();
-    if (request.raw_input_contents_size() <= inputIndex) {
-        SPDLOG_DEBUG("Data should be located in raw_input_contents if graph input tag is IMAGE");
-        return StatusCode::MEDIAPIPE_EXECUTION_ERROR;
-    }
-    auto& bufferLocation = request.raw_input_contents().at(inputIndex);
-
-    if (requestInputItr->shape().size() != 3) {
-        std::stringstream ss;
-        ss << "Invalid Mediapipe Image input shape size. Expected: 3; Actual: " << requestInputItr->shape().size();
-        const std::string details = ss.str();
-        SPDLOG_DEBUG(details);
-        return Status(StatusCode::INVALID_SHAPE, details);
-    }
-    int64_t numberOfRows = requestInputItr->shape()[0];
-    if (numberOfRows <= 0) {
-        std::stringstream ss;
-        ss << "Invalid Mediapipe Image input height. Expected greater than 0; Actual: " << numberOfRows << "; Expected layout - HWC.";
-        const std::string details = ss.str();
-        SPDLOG_DEBUG(details);
-        return Status(StatusCode::INVALID_SHAPE, details);
-    }
-    int64_t numberOfCols = requestInputItr->shape()[1];
-    if (numberOfCols <= 0) {
-        std::stringstream ss;
-        ss << "Invalid Mediapipe Image input width. Expected greater than 0; Actual: " << numberOfCols << "; Expected layout - HWC.";
-        const std::string details = ss.str();
-        SPDLOG_DEBUG(details);
-        return Status(StatusCode::INVALID_SHAPE, details);
-    }
-    int64_t numberOfChannels = requestInputItr->shape()[2];
-    if (numberOfChannels <= 0) {
-        std::stringstream ss;
-        ss << "Invalid Mediapipe Image input number of channels. Expected greater than 0; Actual: " << numberOfChannels << "; Expected layout - HWC.";
-        const std::string details = ss.str();
-        SPDLOG_DEBUG(details);
-        return Status(StatusCode::INVALID_SHAPE, details);
-    }
-    size_t elementSize = KFSDataTypeSize(requestInputItr->datatype());
-    size_t expectedSize = numberOfChannels * numberOfCols * numberOfRows * elementSize;
-    if (bufferLocation.size() != expectedSize) {
-        std::stringstream ss;
-        ss << "Invalid Mediapipe Image input buffer size. Actual: " << bufferLocation.size() << "; Expected: " << expectedSize;
-        const std::string details = ss.str();
-        SPDLOG_DEBUG(details);
-        return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-    }
-    auto imageFormat = KFSDatatypeToImageFormat(requestInputItr->datatype(), numberOfChannels);
-    if (imageFormat == mediapipe::ImageFormat::UNKNOWN) {
-        SPDLOG_DEBUG("Invalid KFS request datatype, conversion to Mediapipe ImageFrame format failed.");
-        return Status(StatusCode::INVALID_INPUT_FORMAT, "Invalid KFS request datatype, conversion to Mediapipe ImageFrame format failed.");
-    }
-    try {
-        outTensor = std::make_unique<mediapipe::ImageFrame>(
-            imageFormat,
-            numberOfCols,
-            numberOfRows,
-            numberOfCols * numberOfChannels * elementSize,
-            reinterpret_cast<uint8_t*>((const_cast<char*>(bufferLocation.data()))),
-            mediapipe::ImageFrame::PixelDataDeleter::kNone);
-    }
-    HANDLE_DESERIALIZATION_EXCEPTION("Mediapipe ImageFrame")
-    return StatusCode::OK;
-}
-
-#if (PYTHON_DISABLE == 0)
-static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<PyObjectWrapper<py::object>>& outTensor, PythonBackend* pythonBackend) {
-    auto requestInputItr = request.inputs().begin();
-    auto status = getRequestInput(requestInputItr, requestedName, request);
-    if (!status.ok()) {
-        return status;
-    }
-    auto inputIndex = requestInputItr - request.inputs().begin();
-    try {
-        std::vector<py::ssize_t> shape;
-        for (int i = 0; i < requestInputItr->shape().size(); i++) {
-            if (requestInputItr->shape()[i] < 0) {
-                std::stringstream ss;
-                ss << "Negative dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
-                const std::string details = ss.str();
-                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
-                return Status(StatusCode::INVALID_SHAPE, details);
-            }
-            shape.push_back(requestInputItr->shape()[i]);
-        }
-
-        ov::element::Type precision = ovmsPrecisionToIE2Precision(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
-        auto formatIt = datatypeToBufferFormat.find(requestInputItr->datatype());
-        if (request.raw_input_contents().size()) {
-            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
-            if (formatIt != datatypeToBufferFormat.end()) {
-                // If datatype is known, we check if a valid buffer can be created with provided data
-                size_t itemsize = bufferFormatToItemsize.at(formatIt->second);
-                size_t expectedBufferSize = 1;
-
-                bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<py::ssize_t>(shape, itemsize, expectedBufferSize);
-                if (!expectedBufferSizeValid) {
-                    const std::string details = "Provided shape and datatype declare too large buffer.";
-                    SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-                    return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-                }
-
-                if (bufferLocation.size() != expectedBufferSize) {
-                    std::stringstream ss;
-                    ss << "Invalid Python tensor buffer size. Actual: " << bufferLocation.size() << "; Expected: " << expectedBufferSize;
-                    const std::string details = ss.str();
-                    SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-                    return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-                }
-            }
-
-            auto ok = pythonBackend->createOvmsPyTensor(
-                requestedName,
-                const_cast<void*>((const void*)bufferLocation.data()),
-                shape,
-                requestInputItr->datatype(),
-                bufferLocation.size(),
-                outTensor);
-
-            if (!ok) {
-                SPDLOG_DEBUG("Error creating Python tensor from data");
-                return StatusCode::UNKNOWN_ERROR;
-            }
-        } else {
-            if ((precision != ov::element::Type_t::string) && formatIt == datatypeToBufferFormat.end()) {
-                const std::string details = "Provided datatype is invalid, custom datatypes are allowed only when raw_input_contents is used.";
-                SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-                return Status(StatusCode::INVALID_PRECISION, details);
-            }
-            size_t expectedBytes;
-            if (precision == ov::element::Type_t::string) {
-                expectedBytes = 0;
-                for (const auto& contents : request.inputs(inputIndex).contents().bytes_contents()) {
-                    expectedBytes += contents.size() + sizeof(uint32_t);
-                }
-            } else {
-                expectedBytes = 1;
-                bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<py::ssize_t>(shape, precision.size(), expectedBytes);
-                if (!expectedBufferSizeValid) {
-                    const std::string details = "Provided shape and datatype declare too large buffer.";
-                    SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
-                    return Status(StatusCode::INVALID_CONTENT_SIZE, details);
-                }
-                OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
-            }
-            auto ok = pythonBackend->createEmptyOvmsPyTensor(
-                requestedName,
-                shape,
-                requestInputItr->datatype(),
-                expectedBytes,
-                outTensor);
-            if (!ok) {
-                SPDLOG_DEBUG("Error creating empty Python tensor");
-                return StatusCode::UNKNOWN_ERROR;
-            }
-            void* data;
-            if (!pythonBackend->getOvmsPyTensorData(outTensor, &data)) {
-                return Status(StatusCode::INTERNAL_ERROR);
-            }
-            switch (precision) {
-            case ov::element::Type_t::f32: {
-                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
-            }
-            case ov::element::Type_t::f64: {
-                COPY_INPUT_VALUE_BY_VALUE(double, fp64);
-            }
-            case ov::element::Type_t::i64: {
-                COPY_INPUT_VALUE_BY_VALUE(int64_t, int64);
-            }
-            case ov::element::Type_t::i32: {
-                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
-            }
-            case ov::element::Type_t::i16: {
-                COPY_INPUT_VALUE_BY_VALUE(int16_t, int);
-            }
-            case ov::element::Type_t::i8: {
-                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
-            }
-            case ov::element::Type_t::u64: {
-                COPY_INPUT_VALUE_BY_VALUE(uint64_t, uint64);
-            }
-            case ov::element::Type_t::u32: {
-                COPY_INPUT_VALUE_BY_VALUE(uint32_t, uint);
-            }
-            case ov::element::Type_t::u16: {
-                COPY_INPUT_VALUE_BY_VALUE(uint16_t, uint);
-            }
-            case ov::element::Type_t::u8: {
-                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
-            }
-            case ov::element::Type_t::boolean: {
-                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
-            }
-            case ov::element::Type_t::string: {
-                uint32_t offset = 0;
-                for (const auto& contents : request.inputs(inputIndex).contents().bytes_contents()) {
-                    const uint32_t size = contents.size();
-                    std::memcpy(reinterpret_cast<char*>(data) + offset, &size, sizeof(uint32_t));
-                    offset += sizeof(uint32_t);
-                    std::memcpy(reinterpret_cast<char*>(data) + offset, contents.data(), size);
-                    offset += size;
-                }
-                return StatusCode::OK;
-            }
-
-            // the rest not supported by KFS
-            case ov::element::Type_t::u1:
-            case ov::element::Type_t::u4:
-            case ov::element::Type_t::i4:
-            case ov::element::Type_t::f16:
-            case ov::element::Type_t::bf16:
-            case ov::element::Type_t::undefined:
-            case ov::element::Type_t::dynamic:
-            default:
-                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, i64, i32, i16, i8, u64, u32, u16, u8, bool");
-            }
-
-            if (!ok) {
-                SPDLOG_DEBUG("Error creating Python tensor from data");
-                return StatusCode::UNKNOWN_ERROR;
-            }
-        }
-    }
-    HANDLE_DESERIALIZATION_EXCEPTION("Ovms Python tensor")
-    return StatusCode::OK;
-}
-#endif
-
-template <typename T, template <typename X> typename Holder>
-static Status createPacketAndPushIntoGraph(const std::string& name, std::shared_ptr<const KFSRequest>& request, ::mediapipe::CalculatorGraph& graph, const ::mediapipe::Timestamp& timestamp, PythonBackend* pythonBackend) {
-    if (name.empty()) {
-        SPDLOG_DEBUG("Creating Mediapipe graph inputs name failed for: {}", name);
-        return StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM;
-    }
-    SPDLOG_DEBUG("Tensor to deserialize:\"{}\"", name);
-    OVMS_RETURN_ON_FAIL(validateRequestCoherencyKFS(*request, request->model_name(), MediapipeGraphDefinition::VERSION));
-    if (!request->raw_input_contents().empty() && (request->raw_input_contents().size() != request->inputs().size())) {
-        std::stringstream ss;
-        ss << "Size of raw_input_contents: " << request->raw_input_contents().size() << " is different than number of inputs: " << request->inputs().size();
-        const std::string details = ss.str();
-        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid message structure - {}", request->model_name(), request->model_version(), details);
-        return Status(StatusCode::INVALID_MESSAGE_STRUCTURE, details);
-    }
-    std::unique_ptr<T> inputTensor;
-    OVMS_RETURN_ON_FAIL(deserializeTensor(name, *request, inputTensor, pythonBackend));
-    MP_RETURN_ON_FAIL(graph.AddPacketToInputStream(
-                          name,
-                          ::mediapipe::packet_internal::Create(
-                              new Holder<T>(inputTensor.release(), request))
-                              .At(timestamp)),
-        std::string("failed to add packet to stream: ") + name, StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM);
-    return StatusCode::OK;
-}
-
-template <template <typename X> typename Holder>
-static Status createPacketAndPushIntoGraph(const std::string& name, std::shared_ptr<const KFSRequest>& request, ::mediapipe::CalculatorGraph& graph, const ::mediapipe::Timestamp& timestamp, PythonBackend* pythonBackend) {
-    if (name.empty()) {
-        SPDLOG_DEBUG("Creating Mediapipe graph inputs name failed for: {}", name);
-        return StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM;
-    }
-    SPDLOG_DEBUG("Request to passthrough:\"{}\"", name);
-    const KFSRequest* lvaluePtr = request.get();
-    MP_RETURN_ON_FAIL(graph.AddPacketToInputStream(
-                          name,
-                          ::mediapipe::packet_internal::Create(
-                              new Holder<const KFSRequest*>(lvaluePtr, request))
-                              .At(timestamp)),
-        std::string("failed to add packet to stream: ") + name, StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM);
-    return StatusCode::OK;
-}
-
-// Required for unary/streaming where it is OVMS who creates the request but it is not the packet type and we have to clean up.
-// In case when passing ownership is not required (unary-unary or first request of streaming) it is enough to pass shared_ptr with no-op destructor.
-// Specializations are for special case when the request itsef is the packet and we need to ensure there is no double free.
-template <typename T>
-class HolderWithRequestOwnership : public ::mediapipe::packet_internal::Holder<T> {
-    std::shared_ptr<const KFSRequest> req;
-
-public:
-    explicit HolderWithRequestOwnership(const T* barePtr, const std::shared_ptr<const KFSRequest>& req) :
-        ::mediapipe::packet_internal::Holder<T>(barePtr),
-        req(req) {}
-};
-template <>
-class HolderWithRequestOwnership<const KFSRequest*> : public ::mediapipe::packet_internal::ForeignHolder<const KFSRequest*> {
-    const KFSRequest* hiddenPtr = nullptr;
-    std::shared_ptr<const KFSRequest> req;
-
-public:
-    explicit HolderWithRequestOwnership(const KFSRequest* barePtr, const std::shared_ptr<const KFSRequest>& req) :
-        ::mediapipe::packet_internal::ForeignHolder<const KFSRequest*>(&hiddenPtr),
-        hiddenPtr(barePtr),
-        req(req) {}
-};
-
-template <template <typename X> typename Holder>
-static Status createPacketAndPushIntoGraph(const std::string& inputName, std::shared_ptr<const KFSRequest>& request, ::mediapipe::CalculatorGraph& graph, const ::mediapipe::Timestamp& timestamp, const stream_types_mapping_t& inputTypes, PythonBackend* pythonBackend) {
-    auto it = inputTypes.find(inputName);
-    if (it == inputTypes.end()) {
-        std::stringstream ss;
-        ss << inputName << " is unexpected";
-        const std::string details = ss.str();
-        SPDLOG_DEBUG("[servable name: {} version: {}] Unexpected input name: {}", request->model_name(), request->model_version(), details);
-        return Status(StatusCode::INVALID_UNEXPECTED_INPUT, details);
-    }
-    auto inputPacketType = it->second;
-    ovms::Status status;
-    if (inputPacketType == mediapipe_packet_type_enum::KFS_REQUEST) {
-        SPDLOG_DEBUG("Request processing KFS passthrough: {}", inputName);
-        status = createPacketAndPushIntoGraph<Holder>(inputName, request, graph, timestamp, nullptr);
-    } else if (inputPacketType == mediapipe_packet_type_enum::TFTENSOR) {
-        SPDLOG_DEBUG("Request processing TF tensor: {}", inputName);
-        status = createPacketAndPushIntoGraph<tensorflow::Tensor, Holder>(inputName, request, graph, timestamp, nullptr);
-    } else if (inputPacketType == mediapipe_packet_type_enum::MPTENSOR) {
-        SPDLOG_DEBUG("Request processing MP tensor: {}", inputName);
-        status = createPacketAndPushIntoGraph<mediapipe::Tensor, Holder>(inputName, request, graph, timestamp, nullptr);
-    } else if (inputPacketType == mediapipe_packet_type_enum::MEDIAPIPE_IMAGE) {
-        SPDLOG_DEBUG("Request processing Mediapipe ImageFrame: {}", inputName);
-        status = createPacketAndPushIntoGraph<mediapipe::ImageFrame, Holder>(inputName, request, graph, timestamp, nullptr);
-#if (PYTHON_DISABLE == 0)
-    } else if (inputPacketType == mediapipe_packet_type_enum::OVMS_PY_TENSOR) {
-        SPDLOG_DEBUG("Request processing OVMS Python input: {}", inputName);
-        status = createPacketAndPushIntoGraph<PyObjectWrapper<py::object>, Holder>(inputName, request, graph, timestamp, pythonBackend);
-#endif
-    } else if ((inputPacketType == mediapipe_packet_type_enum::OVTENSOR) ||
-               (inputPacketType == mediapipe_packet_type_enum::UNKNOWN)) {
-        SPDLOG_DEBUG("Request processing OVTensor: {}", inputName);
-        status = createPacketAndPushIntoGraph<ov::Tensor, Holder>(inputName, request, graph, timestamp, nullptr);
-    }
-    return status;
-}
-
-static inline Status checkTimestamp(const KFSRequest& request, const ::mediapipe::Timestamp& timestamp) {
-    if (!timestamp.IsRangeValue()) {
-        SPDLOG_DEBUG("Timestamp not in range: {}; for request to: {};", timestamp.DebugString(), request.model_name());
-        return Status(StatusCode::MEDIAPIPE_INVALID_TIMESTAMP, timestamp.DebugString());
-    }
-    return StatusCode::OK;
-}
-
-static Status deserializeTimestampIfAvailable(
-    const KFSRequest& request,
-    ::mediapipe::Timestamp& timestamp) {
-    auto timestampParamIt = request.parameters().find(TIMESTAMP_PARAMETER_NAME);
-    if (timestampParamIt != request.parameters().end()) {
-        SPDLOG_DEBUG("Found {} timestamp parameter in request for: {}", TIMESTAMP_PARAMETER_NAME, request.model_name());
-        auto& parameterChoice = timestampParamIt->second;
-        if (parameterChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kInt64Param) {
-            // Cannot create with error checking since error check = abseil death test
-            timestamp = ::mediapipe::Timestamp::CreateNoErrorChecking(parameterChoice.int64_param());
-            if (!timestamp.IsRangeValue()) {
-                SPDLOG_DEBUG("Timestamp not in range: {}; for request to: {};", timestamp.DebugString(), request.model_name());
-                return Status(StatusCode::MEDIAPIPE_INVALID_TIMESTAMP, timestamp.DebugString());
-            }
-        } else {
-            auto status = Status(StatusCode::MEDIAPIPE_INVALID_TIMESTAMP, "Invalid timestamp format in request parameter OVMS_MP_TIMESTAMP. Should be int64");
-            SPDLOG_DEBUG(status.string());
-            return status;
-        }
-    }
-    return StatusCode::OK;
-}
-
-// Implementation
-
-Status onPacketReadySerializeAndSendImpl(
-    const std::string& request_id,
-    const std::string& endpointName,
-    const std::string& endpointVersion,
-    const std::string& packetName,
-    const mediapipe_packet_type_enum packetType,
-    const ::mediapipe::Packet& packet,
-    KFSServerReaderWriter& serverReaderWriter) {
-
-    KFSStreamResponse resp;
-    OVMS_RETURN_ON_FAIL(
-        onPacketReadySerializeImpl(
-            request_id,
-            endpointName,
-            endpointVersion,
-            packetName,
-            packetType,
-            packet,
-            *resp.mutable_infer_response()));
-
-    if (!serverReaderWriter.Write(resp)) {
-        return Status(StatusCode::UNKNOWN_ERROR, "client disconnected");
-    }
-
-    return StatusCode::OK;
-}
-
-Status onPacketReadySerializeImpl(
-    const std::string& request_id,
-    const std::string& endpointName,
-    const std::string& endpointVersion,
-    const std::string& packetName,
-    const mediapipe_packet_type_enum packetType,
-    const ::mediapipe::Packet& packet,
-    KFSResponse& response) {
-    Status status;
-    SPDLOG_DEBUG("Received packet from output stream: {}", packetName);
-    if (packetType == mediapipe_packet_type_enum::KFS_RESPONSE) {
-        SPDLOG_DEBUG("Response processing packet type KFSPass name: {}", packetName);
-        status = receiveAndSerializePacket<KFSResponse>(packet, response, packetName);
-    } else if (packetType == mediapipe_packet_type_enum::TFTENSOR) {
-        SPDLOG_DEBUG("Response processing packet type TF Tensor name: {}", packetName);
-        status = receiveAndSerializePacket<tensorflow::Tensor>(packet, response, packetName);
-    } else if (packetType == mediapipe_packet_type_enum::TFLITETENSOR) {
-        SPDLOG_DEBUG("Response processing packet type TFLite Tensor name: {}", packetName);
-        std::string details{"Response processing packet type TFLite Tensor is not supported"};
-        status = Status(StatusCode::NOT_IMPLEMENTED, std::move(details));
-    } else if (packetType == mediapipe_packet_type_enum::MPTENSOR) {
-        SPDLOG_DEBUG("Response processing packet type MP Tensor name: {}", packetName);
-        status = receiveAndSerializePacket<mediapipe::Tensor>(packet, response, packetName);
-    } else if (packetType == mediapipe_packet_type_enum::MEDIAPIPE_IMAGE) {
-        SPDLOG_DEBUG("Response processing Mediapipe Image Frame: {}", packetName);
-        status = receiveAndSerializePacket<mediapipe::ImageFrame>(packet, response, packetName);
-#if (PYTHON_DISABLE == 0)
-    } else if (packetType == mediapipe_packet_type_enum::OVMS_PY_TENSOR) {
-        SPDLOG_DEBUG("Response processing Ovms Python Tensor name: {}", packetName);
-        status = receiveAndSerializePacket<PyObjectWrapper<py::object>>(packet, response, packetName);
-#endif
-    } else if ((packetType == mediapipe_packet_type_enum::OVTENSOR) ||
-               (packetType == mediapipe_packet_type_enum::UNKNOWN)) {
-        SPDLOG_DEBUG("Response processing packet type:  OVTensor name: {}", packetName);
-        status = receiveAndSerializePacket<ov::Tensor>(packet, response, packetName);
-    } else {
-        status = Status(StatusCode::UNKNOWN_ERROR, "Unreachable code");
-    }
-    response.set_model_name(endpointName);
-    response.set_model_version(endpointVersion);
-    response.set_id(request_id);
-    response.mutable_parameters()->operator[](TIMESTAMP_PARAMETER_NAME).set_int64_param(packet.Timestamp().Value());
-    return status;
-}
-
-Status createAndPushPacketsImpl(
-    std::shared_ptr<const KFSRequest> request,
-    stream_types_mapping_t& inputTypes,
-    PythonBackend* pythonBackend,
-    ::mediapipe::CalculatorGraph& graph,
-    ::mediapipe::Timestamp& currentTimestamp,
-    size_t& numberOfPacketsCreated) {
-
-    OVMS_RETURN_ON_FAIL(deserializeTimestampIfAvailable(*request, currentTimestamp));
-    OVMS_RETURN_ON_FAIL(checkTimestamp(*request, currentTimestamp));
-    OVMS_RETURN_ON_FAIL(validateRequestCoherencyKFS(*request, request->model_name(), MediapipeGraphDefinition::VERSION));
-
-    numberOfPacketsCreated = 0;
-    for (const auto& input : request->inputs()) {
-        const auto& inputName = input.name();
-        auto status = createPacketAndPushIntoGraph<HolderWithRequestOwnership>(
-            inputName, request, graph, currentTimestamp, inputTypes, pythonBackend);
-        if (!status.ok()) {
-            return status;
-        }
-        numberOfPacketsCreated++;
-    }
-
-    currentTimestamp = currentTimestamp.NextAllowedInStream();
-
-    return StatusCode::OK;
-}
-
-Status deserializeInputSidePacketsFromFirstRequestImpl(
-    std::map<std::string, mediapipe::Packet>& inputSidePackets,
-    const KFSRequest& request) {
-    static const std::string PYTHON_SESSION_SIDE_PACKET_TAG{"py"};
-    for (const auto& [name, valueChoice] : request.parameters()) {
-        SPDLOG_DEBUG("Found: {}; parameter in request for: {};", name, request.model_name());
-        if (name == TIMESTAMP_PARAMETER_NAME) {
-            SPDLOG_DEBUG("Ignored: {}; parameter in request for: {}; Paremeter is reserved for MediaPipe input packet timestamps", name, request.model_name());
-            continue;
-        }
-        if (name == PYTHON_SESSION_SIDE_PACKET_TAG) {
-            const std::string absMessage = "Incoming input side packet: " + PYTHON_SESSION_SIDE_PACKET_TAG + " is special reserved name and cannot be used";
-            SPDLOG_DEBUG("Failed to insert predefined input side packet: {} with error: {}", PYTHON_SESSION_SIDE_PACKET_TAG, absMessage);
-            return Status(StatusCode::MEDIAPIPE_GRAPH_INITIALIZATION_ERROR, std::move(absMessage));
-        }
-        if (valueChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kStringParam) {
-            inputSidePackets[name] = mediapipe::MakePacket<std::string>(valueChoice.string_param());
-        } else if (valueChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kInt64Param) {
-            inputSidePackets[name] = mediapipe::MakePacket<int64_t>(valueChoice.int64_param());
-        } else if (valueChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kBoolParam) {
-            inputSidePackets[name] = mediapipe::MakePacket<bool>(valueChoice.bool_param());
-        } else {
-            SPDLOG_DEBUG("Handling parameters of other types than: bool, string, int64 is not supported");
-            return Status(StatusCode::NOT_IMPLEMENTED, "Handling parameters of other types than: bool, string, int64 is not supported");
-        }
-    }
-    return StatusCode::OK;
-}
-
-Status validateSubsequentRequestImpl(
-    const KFSRequest& request,
-    const std::string& endpointName,
-    const std::string& endpointVersion,
-    stream_types_mapping_t& inputTypes) {
-    if (request.model_name() != endpointName) {
-        return StatusCode::MEDIAPIPE_INCORRECT_SERVABLE_NAME;
-    }
-    if (request.model_version() != endpointVersion &&
-        request.model_version() != "0" &&    // default version does not matter for user
-        !request.model_version().empty()) {  // empty the same as default
-        return StatusCode::MEDIAPIPE_INCORRECT_SERVABLE_VERSION;
-    }
-    return StatusCode::OK;
-}
-
-Status sendErrorImpl(
-    const std::string& message,
-    KFSServerReaderWriter& serverReaderWriter) {
-    ::inference::ModelStreamInferResponse resp;
-    *resp.mutable_error_message() = message;
-
-    if (serverReaderWriter.Write(resp)) {
-        return StatusCode::OK;
-    }
-
-    return Status(StatusCode::UNKNOWN_ERROR, "error during sending an error response");
-}
-
-bool waitForNewRequest(
-    KFSServerReaderWriter& serverReaderWriter,
-    KFSRequest& newRequest) {
-    return serverReaderWriter.Read(&newRequest);
-}
-
-}  // namespace ovms
diff --git a/src/kfs_frontend/kfs_graph_executor_impl.hpp b/src/kfs_frontend/kfs_graph_executor_impl.hpp
deleted file mode 100644
index 1541f91d..00000000
--- a/src/kfs_frontend/kfs_graph_executor_impl.hpp
+++ /dev/null
@@ -1,121 +0,0 @@
-//*****************************************************************************
-// Copyright 2024 Intel Corporation
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-//*****************************************************************************
-#pragma once
-
-#include <map>
-#include <memory>
-#include <string>
-
-#include "../mediapipe_internal/packettypes.hpp"
-#include "../status.hpp"
-#include "kfs_grpc_inference_service.hpp"
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wall"
-#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
-#include "mediapipe/framework/calculator_graph.h"
-#include "mediapipe/framework/packet.h"
-#pragma GCC diagnostic pop
-
-#if (PYTHON_DISABLE == 0)
-#include "../python/python_backend.hpp"
-#endif
-
-namespace ovms {
-
-// Deserialization of parameters inside KServe gRPC request
-// into mediapipe Packets.
-// To be used by both - infer & inferStream.
-Status deserializeInputSidePacketsFromFirstRequestImpl(
-    std::map<std::string, mediapipe::Packet>& inputSidePackets,  // out
-    const KFSRequest& request);                                  // in
-
-// Used by inferStream only.
-// Whenever MediaPipe graph produces some packet, this function is triggered.
-// Implementation should transform packet into KServe gRPC response and send it.
-// Data race safety:
-// MediaPipe packet available callbacks can be triggered simultanously, from different threads.
-// However, the graph executor synchronizes it with locking mechanism.
-Status onPacketReadySerializeAndSendImpl(
-    const std::string& request_id,
-    const std::string& endpointName,
-    const std::string& endpointVersion,
-    const std::string& packetName,
-    const mediapipe_packet_type_enum packetType,
-    const ::mediapipe::Packet& packet,
-    KFSServerReaderWriter& serverReaderWriter);
-
-// Used by infer only.
-// infer produces single response and lets the caller send the response back on its own.
-// This function is triggered when output poller has packet ready for serialization.
-// Implementation should transform packet into KServe gRPC response and send it.
-// Data race safety:
-// This is always triggered on the same thread.
-Status onPacketReadySerializeImpl(
-    const std::string& request_id,
-    const std::string& endpointName,
-    const std::string& endpointVersion,
-    const std::string& packetName,
-    const mediapipe_packet_type_enum packetType,
-    const ::mediapipe::Packet& packet,
-    KFSResponse& response);
-
-// This is called whenever new request is received.
-// It is responsible for creating packet(s) out of the request.
-// It is also responsive of pushing the packet inside the graph.
-// To be used by both - infer & inferStream.
-Status createAndPushPacketsImpl(
-    // The request wrapped in shared pointer.
-    std::shared_ptr<const KFSRequest> request,
-    // Graph input name => type mapping.
-    // Request can contain multiple packets.
-    // Implementation should validate for existence of such packet type.
-    stream_types_mapping_t& inputTypes,
-    // Context for creating Python bufferprotocol packets.
-    PythonBackend* pythonBackend,
-    // The graph instance.
-    // Implementation is required to push the packet down the graph.
-    ::mediapipe::CalculatorGraph& graph,
-    // Timestamp to be used if request specified no manual timestamp.
-    // Implementation is also expected to leave the timestamp in next
-    // available state for usage in subsequent requests.
-    ::mediapipe::Timestamp& currentTimestamp,
-    // Unary (non-streaming) execution requires information about number of packets
-    // in order to validate if all inputs were fed into the graph.
-    size_t& numberOfPacketsCreated);
-
-// This is called before subsequent createAndPushPacketsImpl in inferStream scenario.
-// At this point we may reject requests with invalid data.
-Status validateSubsequentRequestImpl(
-    const KFSRequest& request,
-    const std::string& endpointName,
-    const std::string& endpointVersion,
-    stream_types_mapping_t& inputTypes);
-
-// Data race safety:
-// This may be called from different threads.
-// However, the caller implements synchronization mechanism
-// which prevents writes at the same time.
-Status sendErrorImpl(
-    const std::string& message,
-    KFSServerReaderWriter& serverReaderWriter);
-
-// Imitation of stream.Read(...) in gRPC stream API
-// Required for inferStream only.
-bool waitForNewRequest(
-    KFSServerReaderWriter& serverReaderWriter,
-    KFSRequest& newRequest);
-
-}  // namespace ovms
diff --git a/src/kfs_frontend/kfs_grpc_inference_service.cpp b/src/kfs_frontend/kfs_grpc_inference_service.cpp
index 9558e312..b49dd78b 100644
--- a/src/kfs_frontend/kfs_grpc_inference_service.cpp
+++ b/src/kfs_frontend/kfs_grpc_inference_service.cpp
@@ -29,15 +29,10 @@
 #include "../deserialization.hpp"
 #include "../execution_context.hpp"
 #include "../grpc_utils.hpp"
-#include "kfs_utils.hpp"
+#include "../kfs_frontend/kfs_utils.hpp"
 #if (MEDIAPIPE_DISABLE == 0)
-// clang-format off
-// kfs_graph_executor_impl needs to be included before mediapipegraphexecutor
-// because it contains functions required by graph execution template
-#include "kfs_graph_executor_impl.hpp"
 #include "../mediapipe_internal/mediapipegraphdefinition.hpp"
 #include "../mediapipe_internal/mediapipegraphexecutor.hpp"
-// clang-format on
 #endif
 #include "../metric.hpp"
 #include "../modelinstance.hpp"
diff --git a/src/kfs_frontend/kfs_grpc_inference_service.hpp b/src/kfs_frontend/kfs_grpc_inference_service.hpp
index 653efcb9..46104c85 100644
--- a/src/kfs_frontend/kfs_grpc_inference_service.hpp
+++ b/src/kfs_frontend/kfs_grpc_inference_service.hpp
@@ -31,8 +31,6 @@ using KFSModelMetadataRequest = inference::ModelMetadataRequest;
 using KFSModelMetadataResponse = inference::ModelMetadataResponse;
 using KFSRequest = inference::ModelInferRequest;
 using KFSResponse = inference::ModelInferResponse;
-using KFSStreamResponse = inference::ModelStreamInferResponse;
-using KFSServerReaderWriter = ::grpc::ServerReaderWriterInterface<KFSStreamResponse, KFSRequest>;
 using KFSTensorInputProto = inference::ModelInferRequest::InferInputTensor;
 using KFSTensorOutputProto = inference::ModelInferResponse::InferOutputTensor;
 using KFSShapeType = google::protobuf::RepeatedField<int64_t>;
diff --git a/src/mediapipe_internal/mediapipe_utils.hpp b/src/mediapipe_internal/mediapipe_utils.hpp
index bb93a053..37ad7ab1 100644
--- a/src/mediapipe_internal/mediapipe_utils.hpp
+++ b/src/mediapipe_internal/mediapipe_utils.hpp
@@ -29,33 +29,6 @@ extern const std::string OV_TENSOR_PREFIX;
 extern const std::string OVMS_PY_TENSOR_PREFIX;
 extern const std::string MP_IMAGE_PREFIX;
 
-#define MP_RETURN_ON_FAIL(code, message, errorCode)              \
-    {                                                            \
-        auto absStatus = code;                                   \
-        if (!absStatus.ok()) {                                   \
-            const std::string absMessage = absStatus.ToString(); \
-            SPDLOG_DEBUG("{} {}", message, absMessage);          \
-            return Status(errorCode, std::move(absMessage));     \
-        }                                                        \
-    }
-
-#define OVMS_RETURN_ON_FAIL(code) \
-    {                             \
-        auto status = code;       \
-        if (!status.ok()) {       \
-            return status;        \
-        }                         \
-    }
-
-#define OVMS_RETURN_MP_ERROR_ON_FAIL(code, message)                     \
-    {                                                                   \
-        auto status = code;                                             \
-        if (!status.ok()) {                                             \
-            SPDLOG_DEBUG("{} {}", message, status.string());            \
-            return absl::Status(absl::StatusCode::kCancelled, message); \
-        }                                                               \
-    }
-
 enum class MediaPipeStreamType { INPUT,
     OUTPUT };
 
diff --git a/src/mediapipe_internal/mediapipegraphexecutor.cpp b/src/mediapipe_internal/mediapipegraphexecutor.cpp
index 76b2975f..ad6bd3de 100644
--- a/src/mediapipe_internal/mediapipegraphexecutor.cpp
+++ b/src/mediapipe_internal/mediapipegraphexecutor.cpp
@@ -15,29 +15,787 @@
 //*****************************************************************************
 #include "mediapipegraphexecutor.hpp"
 
+#include <functional>
+#include <iostream>
+#include <limits>
+#include <map>
+#include <memory>
+#include <set>
+#include <sstream>
 #include <string>
+#include <unordered_map>
 #include <utility>
 #include <vector>
 
+#include "../deserialization.hpp"
+#include "../execution_context.hpp"
+#include "../kfs_frontend/kfs_utils.hpp"
+#include "../metric.hpp"
+#include "../modelmanager.hpp"
+#include "../predict_request_validation_utils.hpp"
+#include "../serialization.hpp"
+#include "../status.hpp"
+#include "../stringutils.hpp"
+#include "../tensorinfo.hpp"
+#include "../tfs_frontend/tfs_utils.hpp"
+#include "../timer.hpp"
+#include "../version.hpp"
 #pragma GCC diagnostic push
 #pragma GCC diagnostic ignored "-Wdeprecated-declarations"
 #include "mediapipe/framework/calculator_graph.h"
+#include "mediapipe/framework/formats/image_frame.h"
+#include "mediapipe/framework/formats/image_frame_opencv.h"
 #pragma GCC diagnostic pop
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wall"
+#include "mediapipe/framework/formats/tensor.h"
+#include "mediapipe/framework/port/status.h"
+#pragma GCC diagnostic pop
+#include "opencv2/opencv.hpp"
 
 #if (PYTHON_DISABLE == 0)
+#include <pybind11/embed.h>
+#include <pybind11/pybind11.h>
+#include <pybind11/stl.h>
+
 #include "../python/python_backend.hpp"
+#include "../python/pythonnoderesources.hpp"
+#include "src/python/ovms_py_tensor.hpp"
+namespace py = pybind11;
 #endif
 
 namespace ovms {
+using namespace request_validation_utils;
+using ::mediapipe::Timestamp;
+const Timestamp DEFAULT_STARTING_STREAM_TIMESTAMP = Timestamp(0);
+
+#define MP_RETURN_ON_FAIL(code, message, errorCode)              \
+    {                                                            \
+        auto absStatus = code;                                   \
+        if (!absStatus.ok()) {                                   \
+            const std::string absMessage = absStatus.ToString(); \
+            SPDLOG_DEBUG("{} {}", message, absMessage);          \
+            return Status(errorCode, std::move(absMessage));     \
+        }                                                        \
+    }
+
+#define OVMS_RETURN_ON_FAIL(code) \
+    {                             \
+        auto status = code;       \
+        if (!status.ok()) {       \
+            return status;        \
+        }                         \
+    }
+
+#define OVMS_RETURN_MP_ERROR_ON_FAIL(code, message)                     \
+    {                                                                   \
+        auto status = code;                                             \
+        if (!status.ok()) {                                             \
+            SPDLOG_DEBUG("{} {}", message, status.string());            \
+            return absl::Status(absl::StatusCode::kCancelled, message); \
+        }                                                               \
+    }
+
+#define OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(code, message)                 \
+    {                                                                        \
+        auto status = code;                                                  \
+        if (!status.ok()) {                                                  \
+            ::inference::ModelStreamInferResponse resp;                      \
+            std::stringstream ss;                                            \
+            ss << status.string() << "; " << message;                        \
+            *resp.mutable_error_message() = ss.str();                        \
+            if (!streamSynchronizedWrite(stream, streamWriterMutex, resp)) { \
+                SPDLOG_DEBUG("Writing error to disconnected client");        \
+            }                                                                \
+        }                                                                    \
+    }
+
+static Status getRequestInput(google::protobuf::internal::RepeatedPtrIterator<const inference::ModelInferRequest_InferInputTensor>& itr, const std::string& requestedName, const KFSRequest& request) {
+    auto requestInputItr = std::find_if(request.inputs().begin(), request.inputs().end(), [&requestedName](const ::KFSRequest::InferInputTensor& tensor) { return tensor.name() == requestedName; });
+    if (requestInputItr == request.inputs().end()) {
+        std::stringstream ss;
+        ss << "Required input: " << requestedName;
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Missing input with specific name - {}", request.model_name(), request.model_version(), details);
+        return Status(StatusCode::INVALID_MISSING_INPUT, details);
+    }
+    itr = requestInputItr;
+    return StatusCode::OK;
+}
+
+static mediapipe::Tensor::ElementType KFSPrecisionToMPPrecision(const KFSDataType& kfsDatatype) {
+    static std::unordered_map<KFSDataType, mediapipe::Tensor::ElementType> precisionMap{
+        //        {"FP64", mediapipe::Tensor::ElementType::},
+        {"FP32", mediapipe::Tensor::ElementType::kFloat32},
+        {"FP16", mediapipe::Tensor::ElementType::kFloat16},
+        //        {"INT64", mediapipe::Tensor::ElementType::},
+        {"INT32", mediapipe::Tensor::ElementType::kInt32},
+        //        {"INT16", mediapipe::Tensor::ElementType::},
+        {"INT8", mediapipe::Tensor::ElementType::kInt8},
+        //        {"UINT64", mediapipe::Tensor::ElementType::},
+        //        {"UINT32", mediapipe::Tensor::ElementType::},
+        //        {"UINT16", mediapipe::Tensor::ElementType::},
+        {"UINT8", mediapipe::Tensor::ElementType::kUInt8},
+        {"BOOL", mediapipe::Tensor::ElementType::kBool}
+        //        {"", ov::element::Type_t::, mediapipe::Tensor::ElementType::kChar}
+    };
+    auto it = precisionMap.find(kfsDatatype);
+    if (it == precisionMap.end()) {
+        return mediapipe::Tensor::ElementType::kNone;
+    }
+    return it->second;
+}
+
+const KFSDataType EMPTY_PREC = "";
+
+static const KFSDataType& MPPrecisionToKFSPrecision(::mediapipe::Tensor::ElementType precision) {
+    static std::unordered_map<mediapipe::Tensor::ElementType, KFSDataType> precisionMap{
+        //        {mediapipe::Tensor::ElementType::, "FP64"},
+        {mediapipe::Tensor::ElementType::kFloat32, "FP32"},
+        {mediapipe::Tensor::ElementType::kFloat16, "FP16"},
+        //        {mediapipe::Tensor::ElementType::, "INT64"},
+        {mediapipe::Tensor::ElementType::kInt32, "INT32"},
+        //        {mediapipe::Tensor::ElementType::, "INT16"},
+        {mediapipe::Tensor::ElementType::kInt8, "INT8"},
+        //        {mediapipe::Tensor::ElementType::, "UINT64"},
+        //        {mediapipe::Tensor::ElementType::, "UINT32"},
+        //        {mediapipe::Tensor::ElementType::, "UINT16"},
+        {mediapipe::Tensor::ElementType::kUInt8, "UINT8"},
+        {mediapipe::Tensor::ElementType::kBool, "BOOL"}
+        //        {"", ov::element::Type_t::, mediapipe::Tensor::ElementType::kChar}
+    };
+    auto it = precisionMap.find(precision);
+    if (it == precisionMap.end()) {
+        SPDLOG_WARN("Unsupported precision passed from Mediapipe graph");
+        return EMPTY_PREC;
+    }
+    return it->second;
+}
+
+#define SET_DATA_FROM_MP_TENSOR(TENSOR, VIEW_TYPE)                                                     \
+    switch ((TENSOR)->element_type()) {                                                                \
+    case mediapipe::Tensor::ElementType::kFloat32:                                                     \
+    case mediapipe::Tensor::ElementType::kFloat16:                                                     \
+        data = reinterpret_cast<void*>(const_cast<float*>((TENSOR)->VIEW_TYPE().buffer<float>()));     \
+        break;                                                                                         \
+    case mediapipe::Tensor::ElementType::kUInt8:                                                       \
+        data = reinterpret_cast<void*>(const_cast<uint8_t*>((TENSOR)->VIEW_TYPE().buffer<uint8_t>())); \
+        break;                                                                                         \
+    case mediapipe::Tensor::ElementType::kInt8:                                                        \
+        data = reinterpret_cast<void*>(const_cast<int8_t*>((TENSOR)->VIEW_TYPE().buffer<int8_t>()));   \
+        break;                                                                                         \
+    case mediapipe::Tensor::ElementType::kInt32:                                                       \
+        data = reinterpret_cast<void*>(const_cast<int32_t*>((TENSOR)->VIEW_TYPE().buffer<int32_t>())); \
+        break;                                                                                         \
+    case mediapipe::Tensor::ElementType::kBool:                                                        \
+        data = reinterpret_cast<void*>(const_cast<bool*>((TENSOR)->VIEW_TYPE().buffer<bool>()));       \
+        break;                                                                                         \
+    default:                                                                                           \
+        data = reinterpret_cast<void*>(const_cast<void*>((TENSOR)->VIEW_TYPE().buffer<void>()));       \
+    }
+
+#define HANDLE_DESERIALIZATION_EXCEPTION(TYPE_STRING)                                                       \
+    catch (const std::exception& e) {                                                                       \
+        std::stringstream ss;                                                                               \
+        ss << "Exception:"                                                                                  \
+           << e.what()                                                                                      \
+           << "; caught during " TYPE_STRING " deserialization from KServe request tensor";                 \
+        std::string details = ss.str();                                                                     \
+        SPDLOG_DEBUG(details);                                                                              \
+        return Status(StatusCode::UNKNOWN_ERROR, std::move(details));                                       \
+    }                                                                                                       \
+    catch (...) {                                                                                           \
+        std::stringstream ss;                                                                               \
+        ss << "Unknown exception caught during " TYPE_STRING " deserialization from KServe request tensor"; \
+        std::string details = ss.str();                                                                     \
+        SPDLOG_DEBUG(details);                                                                              \
+        return Status(StatusCode::UNKNOWN_ERROR, std::move(details));                                       \
+    }
+
+#define COPY_INPUT_VALUE_BY_VALUE(TYPE, PROTO_PREFIX)                                                                                       \
+    TYPE* ptr = reinterpret_cast<TYPE*>(data);                                                                                              \
+    const auto& input = request.inputs(inputIndex);                                                                                         \
+    if (!input.has_contents()) {                                                                                                            \
+        return Status(StatusCode::INVALID_CONTENT_SIZE, "Input does not have input tensor contents field");                                 \
+    }                                                                                                                                       \
+    const auto& contents = input.contents();                                                                                                \
+    if (!contents.PROTO_PREFIX##_contents_size()) {                                                                                         \
+        return Status(StatusCode::INVALID_CONTENT_SIZE, "Input does not have proper size of input tensor " #PROTO_PREFIX "contents field"); \
+    }                                                                                                                                       \
+    for (auto& number : contents.PROTO_PREFIX##_contents()) {                                                                               \
+        *(ptr++) = number;                                                                                                                  \
+    }                                                                                                                                       \
+    break;
+
+static Status validateRawInputContent(const size_t expectedBytes, const std::string bufferLocation, const std::string& requestedName, const KFSRequest& request) {
+    if (expectedBytes != bufferLocation.size()) {
+        std::stringstream ss;
+        ss << "Expected: " << expectedBytes << " bytes; Actual: " << bufferLocation.size() << " bytes; input name: " << requestedName;
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid content size of tensor proto - {}", request.model_name(), request.model_version(), details);
+        return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+    }
+    return StatusCode::OK;
+}
+
+static Status validateInputContent(const KFSTensorInputProto& proto, const size_t expectedBytes, const std::string& requestedName, const KFSRequest& request) {
+    auto precision = KFSPrecisionToOvmsPrecision(proto.datatype());
+    size_t elementsCount = getElementsCount(proto, precision);
+    if (expectedBytes != KFSDataTypeSize(proto.datatype()) * elementsCount) {
+        std::stringstream ss;
+        ss << "Expected: " << expectedBytes << " values; Actual: " << KFSDataTypeSize(proto.datatype()) * elementsCount << " values; input name: " << requestedName;
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid value size of tensor proto - {}", request.model_name(), request.model_version(), details);
+        return Status(StatusCode::INVALID_VALUE_COUNT, details);
+    }
+    return StatusCode::OK;
+}
+
+static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<mediapipe::Tensor>& outTensor, PythonBackend* pythonBackend) {
+    auto requestInputItr = request.inputs().begin();
+    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
+    auto inputIndex = requestInputItr - request.inputs().begin();
+    try {
+        auto datatype = KFSPrecisionToMPPrecision(requestInputItr->datatype());
+        if (datatype == mediapipe::Tensor::ElementType::kNone) {
+            std::stringstream ss;
+            ss << "Not supported precision for Mediapipe tensor deserialization: " << requestInputItr->datatype();
+            const std::string details = ss.str();
+            SPDLOG_DEBUG(details);
+            return Status(StatusCode::INVALID_PRECISION, std::move(details));
+        }
+        std::vector<int> rawShape;
+        for (int i = 0; i < requestInputItr->shape().size(); i++) {
+            if (requestInputItr->shape()[i] <= 0) {
+                std::stringstream ss;
+                ss << "Negative or zero dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
+                const std::string details = ss.str();
+                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
+                return Status(StatusCode::INVALID_SHAPE, details);
+            }
+            rawShape.emplace_back(requestInputItr->shape()[i]);
+        }
+        mediapipe::Tensor::Shape tensorShape{rawShape};
+        outTensor = std::make_unique<mediapipe::Tensor>(datatype, tensorShape);
+        void* data;
+        SET_DATA_FROM_MP_TENSOR(outTensor, GetCpuWriteView);
+        ov::element::Type precision = ovmsPrecisionToIE2Precision(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
+        size_t expectedBytes = 1;
+        bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<int>(rawShape, precision.size(), expectedBytes);
+        if (!expectedBufferSizeValid) {
+            const std::string details = "Provided shape and datatype declare too large buffer.";
+            SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+            return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+        }
+        if (request.raw_input_contents().size()) {
+            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
+            OVMS_RETURN_ON_FAIL(validateRawInputContent(expectedBytes, bufferLocation, requestedName, request));
+            std::memcpy(data, bufferLocation.data(), bufferLocation.size());
+        } else {  // need to copy each value separately
+            OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
+            switch (datatype) {
+            case mediapipe::Tensor::ElementType::kFloat32: {
+                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
+            }
+            case mediapipe::Tensor::ElementType::kInt32: {
+                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
+            }
+            case mediapipe::Tensor::ElementType::kInt8: {
+                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
+            }
+            case mediapipe::Tensor::ElementType::kUInt8: {
+                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
+            }
+            case mediapipe::Tensor::ElementType::kBool: {
+                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
+            }
+            case mediapipe::Tensor::ElementType::kFloat16:
+            default:
+                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, i32, i8, u8, bool");
+            }
+        }
+    }
+    HANDLE_DESERIALIZATION_EXCEPTION("Mediapipe tensor")
+    return StatusCode::OK;
+}
+
+static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<tensorflow::Tensor>& outTensor, PythonBackend* pythonBackend) {
+    using tensorflow::Tensor;
+    using tensorflow::TensorShape;
+    auto requestInputItr = request.inputs().begin();
+    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
+    auto inputIndex = requestInputItr - request.inputs().begin();
+    try {
+        auto datatype = getPrecisionAsDataType(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
+        if (datatype == TFSDataType::DT_INVALID) {
+            std::stringstream ss;
+            ss << "Not supported precision for Tensorflow tensor deserialization: " << requestInputItr->datatype();
+            const std::string details = ss.str();
+            SPDLOG_DEBUG(details);
+            return Status(StatusCode::INVALID_PRECISION, std::move(details));
+        }
+        TensorShape tensorShape;
+        std::vector<int64_t> rawShape;
+        for (int i = 0; i < requestInputItr->shape().size(); i++) {
+            if (requestInputItr->shape()[i] < 0) {
+                std::stringstream ss;
+                ss << "Negative dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
+                const std::string details = ss.str();
+                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
+                return Status(StatusCode::INVALID_SHAPE, details);
+            }
+            rawShape.emplace_back(requestInputItr->shape()[i]);
+        }
+        int64_t dimsCount = rawShape.size();
+        auto abslStatus = tensorflow::TensorShapeUtils::MakeShape(rawShape.data(), dimsCount, &tensorShape);
+        if (!abslStatus.ok()) {
+            auto stringViewAbslMessage = abslStatus.message();
+            return Status(StatusCode::UNKNOWN_ERROR, std::string{stringViewAbslMessage});
+        }
+        abslStatus = TensorShape::BuildTensorShapeBase(rawShape, static_cast<tensorflow::TensorShapeBase<TensorShape>*>(&tensorShape));
+        if (!abslStatus.ok()) {
+            auto stringViewAbslMessage = abslStatus.message();
+            return Status(StatusCode::UNKNOWN_ERROR, std::string{stringViewAbslMessage});
+        }
+        size_t expectedBytes = 1;
+        bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<int64_t>(rawShape, KFSDataTypeSize(requestInputItr->datatype()), expectedBytes);
+        if (!expectedBufferSizeValid) {
+            const std::string details = "Provided shape and datatype declare too large buffer.";
+            SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+            return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+        }
+        outTensor = std::make_unique<tensorflow::Tensor>(datatype, tensorShape);
+        if (request.raw_input_contents().size()) {
+            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
+            if (outTensor->TotalBytes() != bufferLocation.size()) {
+                std::stringstream ss;
+                ss << "Mediapipe deserialization content size mismatch; allocated TF Tensor: " << outTensor->TotalBytes() << " bytes vs KServe buffer: " << bufferLocation.size() << " bytes";
+                const std::string details = ss.str();
+                SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+                return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+            }
+            void* tfTensordata = outTensor->data();
+            std::memcpy(tfTensordata, bufferLocation.data(), bufferLocation.size());
+        } else {
+            OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
+            void* data = outTensor->data();
+            switch (datatype) {
+            case TFSDataType::DT_FLOAT: {
+                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
+            }
+            case TFSDataType::DT_DOUBLE: {
+                COPY_INPUT_VALUE_BY_VALUE(double, fp64);
+            }
+            case TFSDataType::DT_INT64: {
+                COPY_INPUT_VALUE_BY_VALUE(int64_t, int64);
+            }
+            case TFSDataType::DT_INT32: {
+                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
+            }
+            case TFSDataType::DT_INT16: {
+                COPY_INPUT_VALUE_BY_VALUE(int16_t, int);
+            }
+            case TFSDataType::DT_INT8: {
+                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
+            }
+            case TFSDataType::DT_UINT64: {
+                COPY_INPUT_VALUE_BY_VALUE(uint64_t, uint64);
+            }
+            case TFSDataType::DT_UINT32: {
+                COPY_INPUT_VALUE_BY_VALUE(uint32_t, uint);
+            }
+            case TFSDataType::DT_UINT16: {
+                COPY_INPUT_VALUE_BY_VALUE(uint16_t, uint);
+            }
+            case TFSDataType::DT_UINT8: {
+                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
+            }
+            case TFSDataType::DT_BOOL: {
+                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
+            }
+            case TFSDataType::DT_HALF:
+            default:
+                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, int64, int32, uint32, uint64, int8, uint8, bool");
+            }
+        }
+    }
+    HANDLE_DESERIALIZATION_EXCEPTION("Tensorflow tensor")
+    return StatusCode::OK;
+}
+
+static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<ov::Tensor>& outTensor, PythonBackend* pythonBackend) {
+    auto requestInputItr = request.inputs().begin();
+    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
+    auto inputIndex = requestInputItr - request.inputs().begin();
+    try {
+        ov::Shape shape;
+        for (int i = 0; i < requestInputItr->shape().size(); i++) {
+            if (requestInputItr->shape()[i] < 0) {
+                std::stringstream ss;
+                ss << "Negative dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
+                const std::string details = ss.str();
+                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
+                return Status(StatusCode::INVALID_SHAPE, details);
+            }
+            shape.push_back(requestInputItr->shape()[i]);
+        }
+        ov::element::Type precision = ovmsPrecisionToIE2Precision(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
+        size_t expectedBytes = 1;
+        bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<size_t>(shape, precision.size(), expectedBytes);
+        if (!expectedBufferSizeValid) {
+            const std::string details = "Provided shape and datatype declare too large buffer.";
+            SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+            return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+        }
+        if (request.raw_input_contents().size()) {
+            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
+            OVMS_RETURN_ON_FAIL(validateRawInputContent(expectedBytes, bufferLocation, requestedName, request));
+            if (expectedBytes == 0) {
+                outTensor = std::make_unique<ov::Tensor>(precision, shape);  // OpenVINO does not accept nullptr as data ptr
+            } else {
+                outTensor = std::make_unique<ov::Tensor>(precision, shape, const_cast<void*>((const void*)bufferLocation.data()));
+            }
+        } else {
+            OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
+            if (expectedBytes == 0) {
+                return StatusCode::OK;
+            }
+            outTensor = std::make_unique<ov::Tensor>(precision, shape);
+            void* data = outTensor->data();
+            switch (precision) {
+            case ov::element::Type_t::f32: {
+                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
+            }
+            case ov::element::Type_t::i64: {
+                COPY_INPUT_VALUE_BY_VALUE(int64_t, int64);
+            }
+            case ov::element::Type_t::i32: {
+                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
+            }
+            case ov::element::Type_t::i16: {
+                COPY_INPUT_VALUE_BY_VALUE(int16_t, int);
+            }
+            case ov::element::Type_t::i8: {
+                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
+            }
+            case ov::element::Type_t::u64: {
+                COPY_INPUT_VALUE_BY_VALUE(uint64_t, uint64);
+            }
+            case ov::element::Type_t::u32: {
+                COPY_INPUT_VALUE_BY_VALUE(uint32_t, uint);
+            }
+            case ov::element::Type_t::u16: {
+                COPY_INPUT_VALUE_BY_VALUE(uint16_t, uint);
+            }
+            case ov::element::Type_t::u8: {
+                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
+            }
+            case ov::element::Type_t::boolean: {
+                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
+            }
+            case ov::element::Type_t::f64: {
+                COPY_INPUT_VALUE_BY_VALUE(double, fp64);
+            }
+            // the rest not supported by KFS
+            case ov::element::Type_t::u1:
+            case ov::element::Type_t::u4:
+            case ov::element::Type_t::i4:
+            case ov::element::Type_t::f16:
+            case ov::element::Type_t::bf16:
+            case ov::element::Type_t::undefined:
+            case ov::element::Type_t::dynamic:
+            default:
+                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, i64, i32, i16, i8, u64, u32, u16, u8, bool");
+            }
+        }
+    }
+    HANDLE_DESERIALIZATION_EXCEPTION("OpenVINO tensor")
+    return StatusCode::OK;
+}
+
+static mediapipe::ImageFormat::Format KFSDatatypeToImageFormat(const std::string& datatype, const size_t numberOfChannels) {
+    if (datatype == "FP32") {
+        if (numberOfChannels == 1) {
+            return mediapipe::ImageFormat::VEC32F1;
+        }
+        if (numberOfChannels == 2) {
+            return mediapipe::ImageFormat::VEC32F2;
+        }
+        if (numberOfChannels == 4) {
+            return mediapipe::ImageFormat::VEC32F4;
+        }
+    }
+    if (datatype == "UINT8" || datatype == "INT8") {
+        if (numberOfChannels == 1) {
+            return mediapipe::ImageFormat::GRAY8;
+        }
+        if (numberOfChannels == 3) {
+            return mediapipe::ImageFormat::SRGB;
+        }
+        if (numberOfChannels == 4) {
+            return mediapipe::ImageFormat::SRGBA;
+        }
+    }
+    if (datatype == "UINT16" || datatype == "INT16") {
+        if (numberOfChannels == 1) {
+            return mediapipe::ImageFormat::GRAY16;
+        }
+        if (numberOfChannels == 3) {
+            return mediapipe::ImageFormat::SRGB48;
+        }
+        if (numberOfChannels == 4) {
+            return mediapipe::ImageFormat::SRGBA64;
+        }
+    }
+    if (datatype == "FP16") {
+        if (numberOfChannels == 1) {
+            return mediapipe::ImageFormat::GRAY16;
+        }
+        if (numberOfChannels == 3) {
+            return mediapipe::ImageFormat::SRGB48;
+        }
+        if (numberOfChannels == 4) {
+            return mediapipe::ImageFormat::SRGBA64;
+        }
+    }
+    return mediapipe::ImageFormat::UNKNOWN;
+}
 
-MediapipeGraphExecutor::MediapipeGraphExecutor(
-    const std::string& name,
-    const std::string& version,
-    const ::mediapipe::CalculatorGraphConfig& config,
+static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<mediapipe::ImageFrame>& outTensor, PythonBackend* pythonBackend) {
+    auto requestInputItr = request.inputs().begin();
+    OVMS_RETURN_ON_FAIL(getRequestInput(requestInputItr, requestedName, request));
+    auto inputIndex = requestInputItr - request.inputs().begin();
+    if (request.raw_input_contents_size() <= inputIndex) {
+        SPDLOG_DEBUG("Data should be located in raw_input_contents if graph input tag is IMAGE");
+        return StatusCode::MEDIAPIPE_EXECUTION_ERROR;
+    }
+    auto& bufferLocation = request.raw_input_contents().at(inputIndex);
+
+    if (requestInputItr->shape().size() != 3) {
+        std::stringstream ss;
+        ss << "Invalid Mediapipe Image input shape size. Expected: 3; Actual: " << requestInputItr->shape().size();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG(details);
+        return Status(StatusCode::INVALID_SHAPE, details);
+    }
+    int64_t numberOfRows = requestInputItr->shape()[0];
+    if (numberOfRows <= 0) {
+        std::stringstream ss;
+        ss << "Invalid Mediapipe Image input height. Expected greater than 0; Actual: " << numberOfRows << "; Expected layout - HWC.";
+        const std::string details = ss.str();
+        SPDLOG_DEBUG(details);
+        return Status(StatusCode::INVALID_SHAPE, details);
+    }
+    int64_t numberOfCols = requestInputItr->shape()[1];
+    if (numberOfCols <= 0) {
+        std::stringstream ss;
+        ss << "Invalid Mediapipe Image input width. Expected greater than 0; Actual: " << numberOfCols << "; Expected layout - HWC.";
+        const std::string details = ss.str();
+        SPDLOG_DEBUG(details);
+        return Status(StatusCode::INVALID_SHAPE, details);
+    }
+    int64_t numberOfChannels = requestInputItr->shape()[2];
+    if (numberOfChannels <= 0) {
+        std::stringstream ss;
+        ss << "Invalid Mediapipe Image input number of channels. Expected greater than 0; Actual: " << numberOfChannels << "; Expected layout - HWC.";
+        const std::string details = ss.str();
+        SPDLOG_DEBUG(details);
+        return Status(StatusCode::INVALID_SHAPE, details);
+    }
+    size_t elementSize = KFSDataTypeSize(requestInputItr->datatype());
+    size_t expectedSize = numberOfChannels * numberOfCols * numberOfRows * elementSize;
+    if (bufferLocation.size() != expectedSize) {
+        std::stringstream ss;
+        ss << "Invalid Mediapipe Image input buffer size. Actual: " << bufferLocation.size() << "; Expected: " << expectedSize;
+        const std::string details = ss.str();
+        SPDLOG_DEBUG(details);
+        return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+    }
+    auto imageFormat = KFSDatatypeToImageFormat(requestInputItr->datatype(), numberOfChannels);
+    if (imageFormat == mediapipe::ImageFormat::UNKNOWN) {
+        SPDLOG_DEBUG("Invalid KFS request datatype, conversion to Mediapipe ImageFrame format failed.");
+        return Status(StatusCode::INVALID_INPUT_FORMAT, "Invalid KFS request datatype, conversion to Mediapipe ImageFrame format failed.");
+    }
+    try {
+        outTensor = std::make_unique<mediapipe::ImageFrame>(
+            imageFormat,
+            numberOfCols,
+            numberOfRows,
+            numberOfCols * numberOfChannels * elementSize,
+            reinterpret_cast<uint8_t*>((const_cast<char*>(bufferLocation.data()))),
+            mediapipe::ImageFrame::PixelDataDeleter::kNone);
+    }
+    HANDLE_DESERIALIZATION_EXCEPTION("Mediapipe ImageFrame")
+    return StatusCode::OK;
+}
+
+#if (PYTHON_DISABLE == 0)
+static Status deserializeTensor(const std::string& requestedName, const KFSRequest& request, std::unique_ptr<PyObjectWrapper<py::object>>& outTensor, PythonBackend* pythonBackend) {
+    auto requestInputItr = request.inputs().begin();
+    auto status = getRequestInput(requestInputItr, requestedName, request);
+    if (!status.ok()) {
+        return status;
+    }
+    auto inputIndex = requestInputItr - request.inputs().begin();
+    try {
+        std::vector<py::ssize_t> shape;
+        for (int i = 0; i < requestInputItr->shape().size(); i++) {
+            if (requestInputItr->shape()[i] < 0) {
+                std::stringstream ss;
+                ss << "Negative dimension size is not acceptable: " << tensorShapeToString(requestInputItr->shape()) << "; input name: " << requestedName;
+                const std::string details = ss.str();
+                SPDLOG_DEBUG("[servable name: {} version: {}] Invalid shape - {}", request.model_name(), request.model_version(), details);
+                return Status(StatusCode::INVALID_SHAPE, details);
+            }
+            shape.push_back(requestInputItr->shape()[i]);
+        }
+
+        ov::element::Type precision = ovmsPrecisionToIE2Precision(KFSPrecisionToOvmsPrecision(requestInputItr->datatype()));
+        auto formatIt = datatypeToBufferFormat.find(requestInputItr->datatype());
+        if (request.raw_input_contents().size()) {
+            auto& bufferLocation = request.raw_input_contents().at(inputIndex);
+            if (formatIt != datatypeToBufferFormat.end()) {
+                // If datatype is known, we check if a valid buffer can be created with provided data
+                size_t itemsize = bufferFormatToItemsize.at(formatIt->second);
+                size_t expectedBufferSize = 1;
+
+                bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<py::ssize_t>(shape, itemsize, expectedBufferSize);
+                if (!expectedBufferSizeValid) {
+                    const std::string details = "Provided shape and datatype declare too large buffer.";
+                    SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+                    return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+                }
+
+                if (bufferLocation.size() != expectedBufferSize) {
+                    std::stringstream ss;
+                    ss << "Invalid Python tensor buffer size. Actual: " << bufferLocation.size() << "; Expected: " << expectedBufferSize;
+                    const std::string details = ss.str();
+                    SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+                    return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+                }
+            }
+
+            auto ok = pythonBackend->createOvmsPyTensor(
+                requestedName,
+                const_cast<void*>((const void*)bufferLocation.data()),
+                shape,
+                requestInputItr->datatype(),
+                bufferLocation.size(),
+                outTensor);
+
+            if (!ok) {
+                SPDLOG_DEBUG("Error creating Python tensor from data");
+                return StatusCode::UNKNOWN_ERROR;
+            }
+        } else {
+            if ((precision != ov::element::Type_t::string) && formatIt == datatypeToBufferFormat.end()) {
+                const std::string details = "Provided datatype is invalid, custom datatypes are allowed only when raw_input_contents is used.";
+                SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+                return Status(StatusCode::INVALID_PRECISION, details);
+            }
+            size_t expectedBytes;
+            if (precision == ov::element::Type_t::string) {
+                expectedBytes = 0;
+                for (auto contents : request.inputs(inputIndex).contents().bytes_contents()) {
+                    expectedBytes += contents.size() + sizeof(uint32_t);
+                }
+            } else {
+                expectedBytes = 1;
+                bool expectedBufferSizeValid = computeExpectedBufferSizeReturnFalseIfOverflow<py::ssize_t>(shape, precision.size(), expectedBytes);
+                if (!expectedBufferSizeValid) {
+                    const std::string details = "Provided shape and datatype declare too large buffer.";
+                    SPDLOG_DEBUG("[servable name: {} version: {}] {}", request.model_name(), request.model_version(), details);
+                    return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+                }
+                OVMS_RETURN_ON_FAIL(validateInputContent(*requestInputItr, expectedBytes, requestedName, request));
+            }
+            auto ok = pythonBackend->createEmptyOvmsPyTensor(
+                requestedName,
+                shape,
+                requestInputItr->datatype(),
+                expectedBytes,
+                outTensor);
+            if (!ok) {
+                SPDLOG_DEBUG("Error creating empty Python tensor");
+                return StatusCode::UNKNOWN_ERROR;
+            }
+            void* data;
+            if (!pythonBackend->getOvmsPyTensorData(outTensor, &data)) {
+                return Status(StatusCode::INTERNAL_ERROR);
+            }
+            switch (precision) {
+            case ov::element::Type_t::f32: {
+                COPY_INPUT_VALUE_BY_VALUE(float, fp32);
+            }
+            case ov::element::Type_t::f64: {
+                COPY_INPUT_VALUE_BY_VALUE(double, fp64);
+            }
+            case ov::element::Type_t::i64: {
+                COPY_INPUT_VALUE_BY_VALUE(int64_t, int64);
+            }
+            case ov::element::Type_t::i32: {
+                COPY_INPUT_VALUE_BY_VALUE(int32_t, int);
+            }
+            case ov::element::Type_t::i16: {
+                COPY_INPUT_VALUE_BY_VALUE(int16_t, int);
+            }
+            case ov::element::Type_t::i8: {
+                COPY_INPUT_VALUE_BY_VALUE(int8_t, int);
+            }
+            case ov::element::Type_t::u64: {
+                COPY_INPUT_VALUE_BY_VALUE(uint64_t, uint64);
+            }
+            case ov::element::Type_t::u32: {
+                COPY_INPUT_VALUE_BY_VALUE(uint32_t, uint);
+            }
+            case ov::element::Type_t::u16: {
+                COPY_INPUT_VALUE_BY_VALUE(uint16_t, uint);
+            }
+            case ov::element::Type_t::u8: {
+                COPY_INPUT_VALUE_BY_VALUE(uint8_t, uint);
+            }
+            case ov::element::Type_t::boolean: {
+                COPY_INPUT_VALUE_BY_VALUE(bool, bool);
+            }
+            case ov::element::Type_t::string: {
+                uint32_t offset = 0;
+                for (auto contents : request.inputs(inputIndex).contents().bytes_contents()) {
+                    const uint32_t size = contents.size();
+                    std::memcpy(reinterpret_cast<char*>(data) + offset, &size, sizeof(uint32_t));
+                    offset += sizeof(uint32_t);
+                    std::memcpy(reinterpret_cast<char*>(data) + offset, contents.data(), size);
+                    offset += size;
+                }
+                return StatusCode::OK;
+            }
+
+            // the rest not supported by KFS
+            case ov::element::Type_t::u1:
+            case ov::element::Type_t::u4:
+            case ov::element::Type_t::i4:
+            case ov::element::Type_t::f16:
+            case ov::element::Type_t::bf16:
+            case ov::element::Type_t::undefined:
+            case ov::element::Type_t::dynamic:
+            default:
+                return ovms::Status(ovms::StatusCode::NOT_IMPLEMENTED, "There is no support for types different than fp32, i64, i32, i16, i8, u64, u32, u16, u8, bool");
+            }
+
+            if (!ok) {
+                SPDLOG_DEBUG("Error creating Python tensor from data");
+                return StatusCode::UNKNOWN_ERROR;
+            }
+        }
+    }
+    HANDLE_DESERIALIZATION_EXCEPTION("Ovms Python tensor")
+    return StatusCode::OK;
+}
+#endif
+
+MediapipeGraphExecutor::MediapipeGraphExecutor(const std::string& name, const std::string& version, const ::mediapipe::CalculatorGraphConfig& config,
     stream_types_mapping_t inputTypes,
     stream_types_mapping_t outputTypes,
-    std::vector<std::string> inputNames,
-    std::vector<std::string> outputNames,
+    std::vector<std::string> inputNames, std::vector<std::string> outputNames,
     const PythonNodeResourcesMap& pythonNodeResourcesMap,
     PythonBackend* pythonBackend) :
     name(name),
@@ -49,9 +807,566 @@ MediapipeGraphExecutor::MediapipeGraphExecutor(
     outputNames(std::move(outputNames)),
     pythonNodeResourcesMap(pythonNodeResourcesMap),
     pythonBackend(pythonBackend),
-    currentStreamTimestamp(STARTING_TIMESTAMP) {}
+    currentStreamTimestamp(DEFAULT_STARTING_STREAM_TIMESTAMP) {}
+
+namespace {
+enum : unsigned int {
+    INITIALIZE_GRAPH,
+    RUN_GRAPH,
+    ADD_INPUT_PACKET,
+    FETCH_OUTPUT,
+    ALL_FETCH,
+    TOTAL,
+    TIMER_END
+};
+}  // namespace
+
+constexpr size_t STARTING_TIMESTAMP = 0;
+const std::string MediapipeGraphExecutor::TIMESTAMP_PARAMETER_NAME = "OVMS_MP_TIMESTAMP";
+
+const std::string PYTHON_SESSION_SIDE_PACKET_TAG = "py";
+
+static Status createInputSidePackets(std::map<std::string, mediapipe::Packet>& inputSidePackets, const KFSRequest* request) {
+    for (const auto& [name, valueChoice] : request->parameters()) {
+        SPDLOG_DEBUG("Found: {}; parameter in request for: {};", name, request->model_name());
+        if (name == MediapipeGraphExecutor::TIMESTAMP_PARAMETER_NAME) {
+            SPDLOG_DEBUG("Ignored: {}; parameter in request for: {}; Paremeter is reserved for MediaPipe input packet timestamps", name, request->model_name());
+            continue;
+        }
+        if (name == PYTHON_SESSION_SIDE_PACKET_TAG) {
+            const std::string absMessage = "Incoming input side packet: " + PYTHON_SESSION_SIDE_PACKET_TAG + " is special reserved name and cannot be used";
+            SPDLOG_DEBUG("Failed to insert predefined input side packet: {} with error: {}", PYTHON_SESSION_SIDE_PACKET_TAG, absMessage);
+            return Status(StatusCode::MEDIAPIPE_GRAPH_INITIALIZATION_ERROR, std::move(absMessage));
+        }
+        if (valueChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kStringParam) {
+            inputSidePackets[name] = mediapipe::MakePacket<std::string>(valueChoice.string_param());
+        } else if (valueChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kInt64Param) {
+            inputSidePackets[name] = mediapipe::MakePacket<int64_t>(valueChoice.int64_param());
+        } else if (valueChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kBoolParam) {
+            inputSidePackets[name] = mediapipe::MakePacket<bool>(valueChoice.bool_param());
+        } else {
+            SPDLOG_DEBUG("Handling parameters of other types than: bool, string, int64 is not supported");
+            return Status(StatusCode::NOT_IMPLEMENTED, "Handling parameters of other types than: bool, string, int64 is not supported");
+        }
+    }
+    return StatusCode::OK;
+}
+
+template <typename T, template <typename X> typename Holder>
+static Status createPacketAndPushIntoGraph(const std::string& name, std::shared_ptr<const KFSRequest>& request, ::mediapipe::CalculatorGraph& graph, const Timestamp& timestamp, PythonBackend* pythonBackend) {
+    if (name.empty()) {
+        SPDLOG_DEBUG("Creating Mediapipe graph inputs name failed for: {}", name);
+        return StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM;
+    }
+    SPDLOG_DEBUG("Tensor to deserialize:\"{}\"", name);
+    OVMS_RETURN_ON_FAIL(validateRequestCoherencyKFS(*request, request->model_name(), MediapipeGraphDefinition::VERSION));
+    if (!request->raw_input_contents().empty() && (request->raw_input_contents().size() != request->inputs().size())) {
+        std::stringstream ss;
+        ss << "Size of raw_input_contents: " << request->raw_input_contents().size() << " is different than number of inputs: " << request->inputs().size();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid message structure - {}", request->model_name(), request->model_version(), details);
+        return Status(StatusCode::INVALID_MESSAGE_STRUCTURE, details);
+    }
+    std::unique_ptr<T> inputTensor;
+    OVMS_RETURN_ON_FAIL(deserializeTensor(name, *request, inputTensor, pythonBackend));
+    MP_RETURN_ON_FAIL(graph.AddPacketToInputStream(
+                          name,
+                          ::mediapipe::packet_internal::Create(
+                              new Holder<T>(inputTensor.release(), request))
+                              .At(timestamp)),
+        std::string("failed to add packet to stream: ") + name, StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM);
+    return StatusCode::OK;
+}
+
+template <template <typename X> typename Holder>
+static Status createPacketAndPushIntoGraph(const std::string& name, std::shared_ptr<const KFSRequest>& request, ::mediapipe::CalculatorGraph& graph, const Timestamp& timestamp, PythonBackend* pythonBackend) {
+    if (name.empty()) {
+        SPDLOG_DEBUG("Creating Mediapipe graph inputs name failed for: {}", name);
+        return StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM;
+    }
+    SPDLOG_DEBUG("Request to passthrough:\"{}\"", name);
+    const KFSRequest* lvaluePtr = request.get();
+    MP_RETURN_ON_FAIL(graph.AddPacketToInputStream(
+                          name,
+                          ::mediapipe::packet_internal::Create(
+                              new Holder<const KFSRequest*>(lvaluePtr, request))
+                              .At(timestamp)),
+        std::string("failed to add packet to stream: ") + name, StatusCode::MEDIAPIPE_GRAPH_ADD_PACKET_INPUT_STREAM);
+    return StatusCode::OK;
+}
+
+#define HANDLE_PACKET_RECEIVAL_EXCEPTIONS()                           \
+    catch (const std::exception& e) {                                 \
+        std::stringstream ss;                                         \
+        ss << "Failed to get packet"                                  \
+           << outputStreamName                                        \
+           << " with exception: "                                     \
+           << e.what();                                               \
+        std::string details{ss.str()};                                \
+        SPDLOG_DEBUG(details);                                        \
+        return Status(StatusCode::UNKNOWN_ERROR, std::move(details)); \
+    }                                                                 \
+    catch (...) {                                                     \
+        std::stringstream ss;                                         \
+        ss << "Failed to get packet"                                  \
+           << outputStreamName                                        \
+           << " with exception.";                                     \
+        std::string details{ss.str()};                                \
+        SPDLOG_DEBUG(details);                                        \
+        return Status(StatusCode::UNKNOWN_ERROR, std::move(details)); \
+    }
 
-const std::string MediapipeGraphExecutor::PYTHON_SESSION_SIDE_PACKET_TAG = "py";
-const ::mediapipe::Timestamp MediapipeGraphExecutor::STARTING_TIMESTAMP = ::mediapipe::Timestamp(0);
+template <typename T>
+static Status receiveAndSerializePacket(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName);
 
+template <>
+Status receiveAndSerializePacket<tensorflow::Tensor>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
+    try {
+        auto received = packet.Get<tensorflow::Tensor>();
+        auto* output = response.add_outputs();
+        output->set_name(outputStreamName);
+        output->set_datatype(
+            ovmsPrecisionToKFSPrecision(
+                TFSPrecisionToOvmsPrecision(
+                    received.dtype())));
+        output->clear_shape();
+        for (const auto& dim : received.shape()) {
+            output->add_shape(dim.size);
+        }
+        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(received.data()), received.TotalBytes());
+        return StatusCode::OK;
+    }
+    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
+}
+
+template <>
+Status receiveAndSerializePacket<::mediapipe::Tensor>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
+    try {
+        const ::mediapipe::Tensor& received = packet.Get<::mediapipe::Tensor>();
+        auto* output = response.add_outputs();
+        output->set_name(outputStreamName);
+        output->set_datatype(MPPrecisionToKFSPrecision(received.element_type()));
+        output->clear_shape();
+        for (const auto& dim : received.shape().dims) {
+            output->add_shape(dim);
+        }
+        void* data;
+        SET_DATA_FROM_MP_TENSOR(&received, GetCpuReadView);
+        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(data), received.bytes());
+        return StatusCode::OK;
+    }
+    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
+}
+
+template <>
+Status receiveAndSerializePacket<ov::Tensor>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
+    try {
+        auto received = packet.Get<ov::Tensor>();
+        auto* output = response.add_outputs();
+        output->set_name(outputStreamName);
+        output->set_datatype(
+            ovmsPrecisionToKFSPrecision(
+                ovElementTypeToOvmsPrecision(
+                    received.get_element_type())));
+        output->clear_shape();
+        for (const auto& dim : received.get_shape()) {
+            output->add_shape(dim);
+        }
+        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(received.data()), received.get_byte_size());
+        return StatusCode::OK;
+    }
+    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
+}
+
+template <>
+Status receiveAndSerializePacket<KFSResponse>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
+    try {
+        auto received = packet.Get<KFSResponse>();
+        response = std::move(received);
+        return StatusCode::OK;
+    }
+    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
+}
+
+static KFSDataType convertImageFormatToKFSDataType(const mediapipe::ImageFormat::Format& imageFormat) {
+    static std::unordered_map<mediapipe::ImageFormat::Format, KFSDataType> ImageFormatKFSDatatypeMap{
+        {mediapipe::ImageFormat::GRAY8, "UINT8"},
+        {mediapipe::ImageFormat::SRGB, "UINT8"},
+        {mediapipe::ImageFormat::SRGBA, "UINT8"},
+        {mediapipe::ImageFormat::GRAY16, "UINT16"},
+        {mediapipe::ImageFormat::SRGB48, "UINT16"},
+        {mediapipe::ImageFormat::SRGBA64, "UINT16"},
+        {mediapipe::ImageFormat::VEC32F1, "FP32"},
+        {mediapipe::ImageFormat::VEC32F2, "FP32"}};
+    auto it = ImageFormatKFSDatatypeMap.find(imageFormat);
+    if (it == ImageFormatKFSDatatypeMap.end()) {
+        SPDLOG_DEBUG("Converting Mediapipe::ImageFrame format to KFS datatype failed. Datatype will be set to default - UINT8");
+        return "UINT8";
+    }
+    return it->second;
+}
+
+template <>
+Status receiveAndSerializePacket<mediapipe::ImageFrame>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
+    try {
+        const auto& received = packet.Get<mediapipe::ImageFrame>();
+        auto* output = response.add_outputs();
+        output->set_name(outputStreamName);
+        KFSDataType datatype = convertImageFormatToKFSDataType(received.Format());
+        output->set_datatype(datatype);
+        output->clear_shape();
+        output->add_shape(received.Height());
+        output->add_shape(received.Width());
+        output->add_shape(received.NumberOfChannels());
+        cv::Mat image = mediapipe::formats::MatView(&received);
+
+        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(image.data), image.cols * image.rows * image.channels() * image.elemSize1());
+        return StatusCode::OK;
+    }
+    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
+}
+
+#if (PYTHON_DISABLE == 0)
+template <>
+Status receiveAndSerializePacket<PyObjectWrapper<py::object>>(const ::mediapipe::Packet& packet, KFSResponse& response, const std::string& outputStreamName) {
+    try {
+        const PyObjectWrapper<py::object>& pyOutput = packet.Get<PyObjectWrapper<py::object>>();
+        auto* output = response.add_outputs();
+        output->set_name(pyOutput.getProperty<std::string>("name"));
+        output->set_datatype(pyOutput.getProperty<std::string>("datatype"));
+        output->clear_shape();
+        for (const auto& dim : pyOutput.getProperty<std::vector<py::ssize_t>>("shape")) {
+            output->add_shape(dim);
+        }
+        void* ptr = pyOutput.getProperty<void*>("ptr");
+        response.add_raw_output_contents()->assign(reinterpret_cast<char*>(ptr), pyOutput.getProperty<py::ssize_t>("size"));
+        return StatusCode::OK;
+    } catch (const pybind11::error_already_set& e) {
+        std::stringstream ss;
+        ss << "Failed to get packet " << outputStreamName << " due to Python object unpacking error: " << e.what();
+        std::string details{ss.str()};
+        SPDLOG_DEBUG(details);
+        return Status(StatusCode::UNKNOWN_ERROR, std::move(details));
+    }
+    HANDLE_PACKET_RECEIVAL_EXCEPTIONS();
+}
+#endif
+
+// Two types of holders
+// One (HolderWithRequestOwnership) is required for streaming where it is OVMS who creates the request but it is not the packet type and we have to clean up
+// Second (HolderWithNoRequestOwnership) is required for unary-unary where it is gRPC who creates the request and musn't clean up
+// Specializations are for special case when the request itsef is the packet and we need to ensure there is no double free
+template <typename T>
+class HolderWithRequestOwnership : public ::mediapipe::packet_internal::Holder<T> {
+    std::shared_ptr<const KFSRequest> req;
+
+public:
+    explicit HolderWithRequestOwnership(const T* barePtr, const std::shared_ptr<const KFSRequest>& req) :
+        ::mediapipe::packet_internal::Holder<T>(barePtr),
+        req(req) {}
+};
+template <>
+class HolderWithRequestOwnership<const KFSRequest*> : public ::mediapipe::packet_internal::ForeignHolder<const KFSRequest*> {
+    const KFSRequest* hiddenPtr = nullptr;
+    std::shared_ptr<const KFSRequest> req;
+
+public:
+    explicit HolderWithRequestOwnership(const KFSRequest* barePtr, const std::shared_ptr<const KFSRequest>& req) :
+        ::mediapipe::packet_internal::ForeignHolder<const KFSRequest*>(&hiddenPtr),
+        hiddenPtr(barePtr),
+        req(req) {}
+};
+
+template <typename T>
+class HolderWithNoRequestOwnership : public ::mediapipe::packet_internal::Holder<T> {
+public:
+    explicit HolderWithNoRequestOwnership(const T* barePtr, const std::shared_ptr<const KFSRequest>& req) :
+        ::mediapipe::packet_internal::Holder<T>(barePtr) {}
+};
+template <>
+class HolderWithNoRequestOwnership<const KFSRequest*> : public ::mediapipe::packet_internal::ForeignHolder<const KFSRequest*> {
+public:
+    const KFSRequest* hiddenPtr = nullptr;
+    explicit HolderWithNoRequestOwnership(const KFSRequest* barePtr, const std::shared_ptr<const KFSRequest>& req) :
+        ::mediapipe::packet_internal::ForeignHolder<const KFSRequest*>(&hiddenPtr),
+        hiddenPtr(barePtr) {}
+};
+
+template <template <typename X> typename Holder>
+static Status createPacketAndPushIntoGraph(const std::string& inputName, std::shared_ptr<const KFSRequest>& request, ::mediapipe::CalculatorGraph& graph, const Timestamp& timestamp, const stream_types_mapping_t& inputTypes, PythonBackend* pythonBackend) {
+    auto inputPacketType = inputTypes.at(inputName);
+    ovms::Status status;
+    if (inputPacketType == mediapipe_packet_type_enum::KFS_REQUEST) {
+        SPDLOG_DEBUG("Request processing KFS passthrough: {}", inputName);
+        status = createPacketAndPushIntoGraph<Holder>(inputName, request, graph, timestamp, nullptr);
+    } else if (inputPacketType == mediapipe_packet_type_enum::TFTENSOR) {
+        SPDLOG_DEBUG("Request processing TF tensor: {}", inputName);
+        status = createPacketAndPushIntoGraph<tensorflow::Tensor, Holder>(inputName, request, graph, timestamp, nullptr);
+    } else if (inputPacketType == mediapipe_packet_type_enum::MPTENSOR) {
+        SPDLOG_DEBUG("Request processing MP tensor: {}", inputName);
+        status = createPacketAndPushIntoGraph<mediapipe::Tensor, Holder>(inputName, request, graph, timestamp, nullptr);
+    } else if (inputPacketType == mediapipe_packet_type_enum::MEDIAPIPE_IMAGE) {
+        SPDLOG_DEBUG("Request processing Mediapipe ImageFrame: {}", inputName);
+        status = createPacketAndPushIntoGraph<mediapipe::ImageFrame, Holder>(inputName, request, graph, timestamp, nullptr);
+#if (PYTHON_DISABLE == 0)
+    } else if (inputPacketType == mediapipe_packet_type_enum::OVMS_PY_TENSOR) {
+        SPDLOG_DEBUG("Request processing OVMS Python input: {}", inputName);
+        status = createPacketAndPushIntoGraph<PyObjectWrapper<py::object>, Holder>(inputName, request, graph, timestamp, pythonBackend);
+#endif
+    } else if ((inputPacketType == mediapipe_packet_type_enum::OVTENSOR) ||
+               (inputPacketType == mediapipe_packet_type_enum::UNKNOWN)) {
+        SPDLOG_DEBUG("Request processing OVTensor: {}", inputName);
+        status = createPacketAndPushIntoGraph<ov::Tensor, Holder>(inputName, request, graph, timestamp, nullptr);
+    }
+    return status;
+}
+
+Status MediapipeGraphExecutor::infer(const KFSRequest* request, KFSResponse* response, ExecutionContext executionContext, ServableMetricReporter*& reporterOut) const {
+    Timer<TIMER_END> timer;
+    SPDLOG_DEBUG("Start unary KServe request mediapipe graph: {} execution", request->model_name());
+    ::mediapipe::CalculatorGraph graph;
+    MP_RETURN_ON_FAIL(graph.Initialize(this->config), std::string("failed initialization of MediaPipe graph: ") + request->model_name(), StatusCode::MEDIAPIPE_GRAPH_INITIALIZATION_ERROR);
+    std::unordered_map<std::string, ::mediapipe::OutputStreamPoller> outputPollers;
+    for (auto& name : this->outputNames) {
+        if (name.empty()) {
+            SPDLOG_DEBUG("Creating Mediapipe graph outputs name failed for: {}", name);
+            return StatusCode::MEDIAPIPE_GRAPH_ADD_OUTPUT_STREAM_ERROR;
+        }
+        auto absStatusOrPoller = graph.AddOutputStreamPoller(name);
+        if (!absStatusOrPoller.ok()) {
+            const std::string absMessage = absStatusOrPoller.status().ToString();
+            SPDLOG_DEBUG("Failed to add mediapipe graph output stream poller: {} with error: {}", request->model_name(), absMessage);
+            return Status(StatusCode::MEDIAPIPE_GRAPH_ADD_OUTPUT_STREAM_ERROR, std::move(absMessage));
+        }
+        outputPollers.emplace(name, std::move(absStatusOrPoller).value());
+    }
+    std::map<std::string, mediapipe::Packet> sideInputPackets;
+    OVMS_RETURN_ON_FAIL(createInputSidePackets(sideInputPackets, request));
+#if (PYTHON_DISABLE == 0)
+    sideInputPackets[PYTHON_SESSION_SIDE_PACKET_TAG] = mediapipe::MakePacket<PythonNodeResourcesMap>(this->pythonNodeResourcesMap).At(mediapipe::Timestamp(STARTING_TIMESTAMP));
+#endif
+    MP_RETURN_ON_FAIL(graph.StartRun(sideInputPackets), std::string("start MediaPipe graph: ") + request->model_name(), StatusCode::MEDIAPIPE_GRAPH_START_ERROR);
+    if (static_cast<int>(this->inputNames.size()) != request->inputs().size()) {
+        std::stringstream ss;
+        ss << "Expected: " << this->inputNames.size() << "; Actual: " << request->inputs().size();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid number of inputs - {}", request->model_name(), version, details);
+        return Status(StatusCode::INVALID_NO_OF_INPUTS, details);
+    }
+
+    ::mediapipe::Packet packet;
+    std::set<std::string> outputPollersWithReceivedPacket;
+
+    ovms::Status status;
+    size_t insertedStreamPackets = 0;
+    std::shared_ptr<const KFSRequest> requestWithNoOwnership(request, [](const KFSRequest* r) {});
+    for (auto& inputName : this->inputNames) {
+        OVMS_RETURN_ON_FAIL(createPacketAndPushIntoGraph<HolderWithNoRequestOwnership>(inputName, requestWithNoOwnership, graph, this->currentStreamTimestamp, this->inputTypes, pythonBackend));
+        ++insertedStreamPackets;
+    }
+    if (this->inputNames.size() > insertedStreamPackets) {
+        SPDLOG_DEBUG("Not all input packets created. Expected: {}, Actual: {}. Aborting execution of mediapipe graph: {}",
+            this->inputNames.size(),
+            insertedStreamPackets,
+            this->name);
+        return Status(StatusCode::INTERNAL_ERROR, "Not all input packets created");
+    }
+    // we wait idle since some calculators could hold ownership on packet content while nodes further down the graph
+    // can be still processing those. Closing packet sources triggers Calculator::Close() on nodes that do not expect
+    // new packets
+    MP_RETURN_ON_FAIL(graph.WaitUntilIdle(), "graph wait until idle", StatusCode::MEDIAPIPE_EXECUTION_ERROR);
+    MP_RETURN_ON_FAIL(graph.CloseAllPacketSources(), "graph close all packet sources", StatusCode::MEDIAPIPE_GRAPH_CLOSE_INPUT_STREAM_ERROR);
+    for (auto& [outputStreamName, poller] : outputPollers) {
+        size_t receivedOutputs = 0;
+        SPDLOG_DEBUG("Will wait for output stream: {} packet", outputStreamName);
+        if (poller.Next(&packet)) {
+            SPDLOG_DEBUG("Received packet from output stream: {}", outputStreamName);
+            OVMS_RETURN_ON_FAIL(serializePacket(outputStreamName, *response, packet));
+            outputPollersWithReceivedPacket.insert(outputStreamName);
+            ++receivedOutputs;
+        }
+        SPDLOG_TRACE("Received all: {} packets for: {}", receivedOutputs, outputStreamName);
+    }
+    MP_RETURN_ON_FAIL(graph.WaitUntilDone(), "grap wait until done", StatusCode::MEDIAPIPE_EXECUTION_ERROR);
+    if (outputPollers.size() != outputPollersWithReceivedPacket.size()) {
+        SPDLOG_DEBUG("Mediapipe failed to execute. Failed to receive all output packets");
+        return Status(StatusCode::MEDIAPIPE_EXECUTION_ERROR, "Unknown error during mediapipe execution");
+    }
+    SPDLOG_DEBUG("Received all output stream packets for graph: {}", request->model_name());
+    response->set_model_name(request->model_name());
+    response->set_id(request->id());
+    response->set_model_version(request->model_version());
+    return StatusCode::OK;
+}
+
+Status MediapipeGraphExecutor::deserializeTimestampIfAvailable(const KFSRequest& request, Timestamp& timestamp) {
+    auto timestampParamIt = request.parameters().find(TIMESTAMP_PARAMETER_NAME);
+    if (timestampParamIt != request.parameters().end()) {
+        SPDLOG_DEBUG("Found {} timestamp parameter in request for: {}", TIMESTAMP_PARAMETER_NAME, request.model_name());
+        auto& parameterChoice = timestampParamIt->second;
+        if (parameterChoice.parameter_choice_case() == inference::InferParameter::ParameterChoiceCase::kInt64Param) {
+            // Cannot create with error checking since error check = abseil death test
+            timestamp = Timestamp::CreateNoErrorChecking(parameterChoice.int64_param());
+        } else {
+            auto status = Status(StatusCode::MEDIAPIPE_INVALID_TIMESTAMP, "Invalid timestamp format in request parameter OVMS_MP_TIMESTAMP. Should be int64");
+            SPDLOG_DEBUG(status.string());
+            return status;
+        }
+    }
+    return StatusCode::OK;
+}
+
+static inline Status checkTimestamp(const KFSRequest& request, const Timestamp& timestamp) {
+    if (!timestamp.IsRangeValue()) {
+        SPDLOG_DEBUG("Timestamp not in range: {}; for request to: {};", timestamp.DebugString(), request.model_name());
+        return Status(StatusCode::MEDIAPIPE_INVALID_TIMESTAMP, timestamp.DebugString());
+    }
+    return StatusCode::OK;
+}
+
+Status MediapipeGraphExecutor::partialDeserialize(std::shared_ptr<const KFSRequest> request, ::mediapipe::CalculatorGraph& graph) {
+    OVMS_RETURN_ON_FAIL(deserializeTimestampIfAvailable(*request, this->currentStreamTimestamp));
+    OVMS_RETURN_ON_FAIL(checkTimestamp(*request, this->currentStreamTimestamp));
+    OVMS_RETURN_ON_FAIL(validateRequestCoherencyKFS(*request, request->model_name(), MediapipeGraphDefinition::VERSION));
+    for (const auto& input : request->inputs()) {
+        const auto& inputName = input.name();
+        if (std::find_if(this->inputNames.begin(), this->inputNames.end(), [&inputName](auto streamName) { return streamName == inputName; }) == this->inputNames.end()) {
+            SPDLOG_DEBUG("Request for {}, contains not expected input name: {}", request->model_name(), inputName);
+            return Status(StatusCode::INVALID_UNEXPECTED_INPUT, std::string(inputName) + " is unexpected");
+        }
+        OVMS_RETURN_ON_FAIL(createPacketAndPushIntoGraph<HolderWithRequestOwnership>(inputName, request, graph, this->currentStreamTimestamp, this->inputTypes, pythonBackend));
+    }
+    currentStreamTimestamp = currentStreamTimestamp.NextAllowedInStream();
+    return StatusCode::OK;
+}
+
+Status MediapipeGraphExecutor::validateSubsequentRequest(const ::inference::ModelInferRequest& request) const {
+    if (request.model_name() != this->name) {
+        return StatusCode::MEDIAPIPE_INCORRECT_SERVABLE_NAME;
+    }
+    if (request.model_version() != this->version &&
+        request.model_version() != "0" &&    // default version does not matter for user
+        !request.model_version().empty()) {  // empty the same as default
+        return StatusCode::MEDIAPIPE_INCORRECT_SERVABLE_VERSION;
+    }
+    return StatusCode::OK;
+}
+
+static bool streamSynchronizedWrite(::grpc::ServerReaderWriterInterface<::inference::ModelStreamInferResponse, KFSRequest>& stream,
+    std::mutex& mtx, ::inference::ModelStreamInferResponse& resp) {
+    const std::lock_guard<std::mutex> lock(mtx);
+    return stream.Write(resp);
+}
+
+Status MediapipeGraphExecutor::inferStream(const KFSRequest& firstRequest, ::grpc::ServerReaderWriterInterface<::inference::ModelStreamInferResponse, KFSRequest>& stream) {
+    SPDLOG_DEBUG("Start streaming KServe request mediapipe graph: {} execution", this->name);
+    std::mutex streamWriterMutex;
+    try {
+        // Init
+        ::mediapipe::CalculatorGraph graph;
+        MP_RETURN_ON_FAIL(graph.Initialize(this->config), "graph initialization", StatusCode::MEDIAPIPE_GRAPH_INITIALIZATION_ERROR);
+
+        // Installing observers
+        for (const auto& outputName : this->outputNames) {
+            MP_RETURN_ON_FAIL(graph.ObserveOutputStream(outputName, [&stream, &streamWriterMutex, &outputName, this](const ::mediapipe::Packet& packet) -> absl::Status {
+                try {
+                    ::inference::ModelStreamInferResponse resp;
+                    OVMS_RETURN_MP_ERROR_ON_FAIL(serializePacket(outputName, *resp.mutable_infer_response(), packet), "error in serialization");
+                    *resp.mutable_infer_response()->mutable_model_name() = this->name;
+                    *resp.mutable_infer_response()->mutable_model_version() = this->version;
+                    resp.mutable_infer_response()->mutable_parameters()->operator[](MediapipeGraphExecutor::TIMESTAMP_PARAMETER_NAME).set_int64_param(packet.Timestamp().Value());
+                    if (!streamSynchronizedWrite(stream, streamWriterMutex, resp)) {
+                        return absl::Status(absl::StatusCode::kCancelled, "client disconnected");
+                    }
+                    return absl::OkStatus();
+                } catch (...) {
+                    return absl::Status(absl::StatusCode::kCancelled, "error in serialization");
+                }
+            }),
+                "output stream observer installation", StatusCode::INTERNAL_ERROR);  // Should never happen for validated graphs
+        }
+
+        // Launch
+        std::map<std::string, mediapipe::Packet> inputSidePackets;
+        OVMS_RETURN_ON_FAIL(createInputSidePackets(inputSidePackets, &firstRequest));
+#if (PYTHON_DISABLE == 0)
+        inputSidePackets[PYTHON_SESSION_SIDE_PACKET_TAG] = mediapipe::MakePacket<PythonNodeResourcesMap>(this->pythonNodeResourcesMap).At(mediapipe::Timestamp(STARTING_TIMESTAMP));
+#endif
+        MP_RETURN_ON_FAIL(graph.StartRun(inputSidePackets), "graph start", StatusCode::MEDIAPIPE_GRAPH_START_ERROR);
+
+        // Deserialize first request
+        OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(this->partialDeserialize(
+                                                  std::shared_ptr<const KFSRequest>(&firstRequest,
+                                                      // Custom deleter to avoid deallocation by custom holder
+                                                      // Conversion to shared_ptr is required for unified deserialization method
+                                                      // for first and subsequent requests
+                                                      [](const KFSRequest*) {}),
+                                                  graph),
+            "partial deserialization of first request");
+
+        // Read loop
+        // Here we create ModelInferRequest with shared ownership,
+        // and move it down to custom packet holder to ensure
+        // lifetime is extended to lifetime of deserialized Packets.
+        auto req = std::make_shared<::inference::ModelInferRequest>();
+        while (stream.Read(req.get())) {
+            auto pstatus = this->validateSubsequentRequest(*req);
+            if (pstatus.ok()) {
+                OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(this->partialDeserialize(req, graph), "partial deserialization of subsequent requests");
+            } else {
+                OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(pstatus, "validate subsequent requests");
+            }
+            if (graph.HasError()) {
+                SPDLOG_DEBUG("Graph {}: encountered an error, stopping the execution", this->name);
+                break;
+            }
+            req = std::make_shared<::inference::ModelInferRequest>();
+        }
+
+        SPDLOG_DEBUG("Graph {}: Closing packet sources...", this->name);
+        // Close input streams
+        MP_RETURN_ON_FAIL(graph.CloseAllPacketSources(), "closing all packet sources", StatusCode::MEDIAPIPE_GRAPH_CLOSE_INPUT_STREAM_ERROR);
+
+        SPDLOG_DEBUG("Graph {}: Closed all packet sources. Waiting untill done...", this->name);
+        MP_RETURN_ON_FAIL(graph.WaitUntilDone(), "waiting until done", StatusCode::MEDIAPIPE_EXECUTION_ERROR);
+        SPDLOG_DEBUG("Graph {}: Done execution", this->name);
+        return StatusCode::OK;
+    } catch (...) {
+        return Status(StatusCode::UNKNOWN_ERROR, "Exception while processing MediaPipe graph");  // To be displayed in method level above
+    }
+}
+
+Status MediapipeGraphExecutor::serializePacket(const std::string& name, ::inference::ModelInferResponse& response, const ::mediapipe::Packet& packet) const {
+    Status status;
+    SPDLOG_DEBUG("Received packet from output stream: {}", name);
+    if (this->outputTypes.at(name) == mediapipe_packet_type_enum::KFS_RESPONSE) {
+        SPDLOG_DEBUG("Response processing packet type KFSPass name: {}", name);
+        status = receiveAndSerializePacket<KFSResponse>(packet, response, name);
+    } else if (this->outputTypes.at(name) == mediapipe_packet_type_enum::TFTENSOR) {
+        SPDLOG_DEBUG("Response processing packet type TF Tensor name: {}", name);
+        status = receiveAndSerializePacket<tensorflow::Tensor>(packet, response, name);
+    } else if (this->outputTypes.at(name) == mediapipe_packet_type_enum::TFLITETENSOR) {
+        SPDLOG_DEBUG("Response processing packet type TFLite Tensor name: {}", name);
+        std::string details{"Response processing packet type TFLite Tensor is not supported"};
+        status = Status(StatusCode::NOT_IMPLEMENTED, std::move(details));
+    } else if (this->outputTypes.at(name) == mediapipe_packet_type_enum::MPTENSOR) {
+        SPDLOG_DEBUG("Response processing packet type MP Tensor name: {}", name);
+        status = receiveAndSerializePacket<mediapipe::Tensor>(packet, response, name);
+    } else if (this->outputTypes.at(name) == mediapipe_packet_type_enum::MEDIAPIPE_IMAGE) {
+        SPDLOG_DEBUG("Response processing Mediapipe Image Frame: {}", name);
+        status = receiveAndSerializePacket<mediapipe::ImageFrame>(packet, response, name);
+#if (PYTHON_DISABLE == 0)
+    } else if (this->outputTypes.at(name) == mediapipe_packet_type_enum::OVMS_PY_TENSOR) {
+        SPDLOG_DEBUG("Response processing Ovms Python Tensor name: {}", name);
+        status = receiveAndSerializePacket<PyObjectWrapper<py::object>>(packet, response, name);
+#endif
+    } else if ((this->outputTypes.at(name) == mediapipe_packet_type_enum::OVTENSOR) ||
+               (this->outputTypes.at(name) == mediapipe_packet_type_enum::UNKNOWN)) {
+        SPDLOG_DEBUG("Response processing packet type:  OVTensor name: {}", name);
+        status = receiveAndSerializePacket<ov::Tensor>(packet, response, name);
+    } else {
+        status = Status(StatusCode::UNKNOWN_ERROR, "Unreachable code");
+    }
+    return status;
+}
 }  // namespace ovms
diff --git a/src/mediapipe_internal/mediapipegraphexecutor.hpp b/src/mediapipe_internal/mediapipegraphexecutor.hpp
index ee5a70af..332fad45 100644
--- a/src/mediapipe_internal/mediapipegraphexecutor.hpp
+++ b/src/mediapipe_internal/mediapipegraphexecutor.hpp
@@ -14,45 +14,29 @@
 // limitations under the License.
 //*****************************************************************************
 #pragma once
+#include <iostream>
 #include <map>
 #include <memory>
-#include <set>
 #include <sstream>
 #include <string>
 #include <unordered_map>
-#include <utility>
 #include <vector>
 
-#include "../execution_context.hpp"
+#include "../kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "../metric.hpp"
-#include "../status.hpp"
 #pragma GCC diagnostic push
 #pragma GCC diagnostic ignored "-Wdeprecated-declarations"
 #include "mediapipe/framework/calculator_graph.h"
 #include "mediapipe/framework/port/status.h"
 #pragma GCC diagnostic pop
-#include "mediapipe_utils.hpp"
 #include "mediapipegraphdefinition.hpp"  // for version in response and PythonNodeResourceMap
 #include "packettypes.hpp"
 
 namespace ovms {
+class Status;
+class PythonNodeResources;
 class PythonBackend;
 
-#define OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(code, message)             \
-    {                                                                    \
-        auto status = code;                                              \
-        if (!status.ok()) {                                              \
-            std::stringstream ss;                                        \
-            ss << status.string() << "; " << message;                    \
-            std::lock_guard<std::mutex> lock(sendMutex);                 \
-            auto status = sendErrorImpl(ss.str(), res);                  \
-            if (!status.ok()) {                                          \
-                SPDLOG_DEBUG("Writing error to disconnected client: {}", \
-                    status.string());                                    \
-            }                                                            \
-        }                                                                \
-    }
-
 class MediapipeGraphExecutor {
     const std::string name;
     const std::string version;
@@ -67,205 +51,23 @@ class MediapipeGraphExecutor {
 
     ::mediapipe::Timestamp currentStreamTimestamp;
 
-public:
-    static const std::string PYTHON_SESSION_SIDE_PACKET_TAG;
-    static const ::mediapipe::Timestamp STARTING_TIMESTAMP;
+    static Status deserializeTimestampIfAvailable(const KFSRequest& request, ::mediapipe::Timestamp& timestamp);
+    Status partialDeserialize(std::shared_ptr<const ::inference::ModelInferRequest> request, ::mediapipe::CalculatorGraph& graph);
+    Status validateSubsequentRequest(const ::inference::ModelInferRequest& request) const;
+
+protected:
+    Status serializePacket(const std::string& name, ::inference::ModelInferResponse& response, const ::mediapipe::Packet& packet) const;
 
+public:
+    static const std::string TIMESTAMP_PARAMETER_NAME;
     MediapipeGraphExecutor(const std::string& name, const std::string& version, const ::mediapipe::CalculatorGraphConfig& config,
         stream_types_mapping_t inputTypes,
         stream_types_mapping_t outputTypes,
         std::vector<std::string> inputNames, std::vector<std::string> outputNames,
         const PythonNodeResourcesMap& pythonNodeResourcesMap,
         PythonBackend* pythonBackend);
+    Status infer(const KFSRequest* request, KFSResponse* response, ExecutionContext executionContext, ServableMetricReporter*& reporterOut) const;
 
-    template <typename RequestType, typename ResponseType>
-    Status infer(const RequestType* request, ResponseType* response, ExecutionContext executionContext, ServableMetricReporter*& reporterOut) {
-        SPDLOG_DEBUG("Start unary KServe request mediapipe graph: {} execution", this->name);
-        ::mediapipe::CalculatorGraph graph;
-        MP_RETURN_ON_FAIL(graph.Initialize(this->config), std::string("failed initialization of MediaPipe graph: ") + this->name, StatusCode::MEDIAPIPE_GRAPH_INITIALIZATION_ERROR);
-        std::unordered_map<std::string, ::mediapipe::OutputStreamPoller> outputPollers;
-        for (auto& name : this->outputNames) {
-            if (name.empty()) {
-                SPDLOG_DEBUG("Creating Mediapipe graph outputs name failed for: {}", name);
-                return StatusCode::MEDIAPIPE_GRAPH_ADD_OUTPUT_STREAM_ERROR;
-            }
-            auto absStatusOrPoller = graph.AddOutputStreamPoller(name);
-            if (!absStatusOrPoller.ok()) {
-                const std::string absMessage = absStatusOrPoller.status().ToString();
-                SPDLOG_DEBUG("Failed to add mediapipe graph output stream poller: {} with error: {}", this->name, absMessage);
-                return Status(StatusCode::MEDIAPIPE_GRAPH_ADD_OUTPUT_STREAM_ERROR, std::move(absMessage));
-            }
-            outputPollers.emplace(name, std::move(absStatusOrPoller).value());
-        }
-        std::map<std::string, mediapipe::Packet> sideInputPackets;
-        OVMS_RETURN_ON_FAIL(deserializeInputSidePacketsFromFirstRequestImpl(sideInputPackets, *request));
-#if (PYTHON_DISABLE == 0)
-        sideInputPackets[PYTHON_SESSION_SIDE_PACKET_TAG] = mediapipe::MakePacket<PythonNodeResourcesMap>(this->pythonNodeResourcesMap).At(STARTING_TIMESTAMP);
-#endif
-        MP_RETURN_ON_FAIL(graph.StartRun(sideInputPackets), std::string("start MediaPipe graph: ") + this->name, StatusCode::MEDIAPIPE_GRAPH_START_ERROR);
-
-        ::mediapipe::Packet packet;
-        std::set<std::string> outputPollersWithReceivedPacket;
-
-        size_t numberOfPacketsCreated = 0;
-        OVMS_RETURN_ON_FAIL(
-            createAndPushPacketsImpl(
-                std::shared_ptr<const RequestType>(request,
-                    // Custom deleter to avoid deallocation by custom holder
-                    // Conversion to shared_ptr is required for unified deserialization method
-                    // for first and subsequent requests
-                    [](const RequestType*) {}),
-                this->inputTypes,
-                this->pythonBackend,
-                graph,
-                this->currentStreamTimestamp,
-                numberOfPacketsCreated));
-
-        // This differs from inferStream - we require user to feed all streams
-        if (this->inputNames.size() > numberOfPacketsCreated) {
-            SPDLOG_DEBUG("Not all input packets created. Expected: {}, Actual: {}. Aborting execution of mediapipe graph: {}",
-                this->inputNames.size(),
-                numberOfPacketsCreated,
-                this->name);
-            return Status(StatusCode::INVALID_NO_OF_INPUTS, "Not all input packets created");
-        }
-
-        // we wait idle since some calculators could hold ownership on packet content while nodes further down the graph
-        // can be still processing those. Closing packet sources triggers Calculator::Close() on nodes that do not expect
-        // new packets
-        MP_RETURN_ON_FAIL(graph.WaitUntilIdle(), "graph wait until idle", StatusCode::MEDIAPIPE_EXECUTION_ERROR);
-        MP_RETURN_ON_FAIL(graph.CloseAllPacketSources(), "graph close all packet sources", StatusCode::MEDIAPIPE_GRAPH_CLOSE_INPUT_STREAM_ERROR);
-        for (auto& [outputStreamName, poller] : outputPollers) {
-            size_t receivedOutputs = 0;
-            SPDLOG_DEBUG("Will wait for output stream: {} packet", outputStreamName);
-            if (poller.Next(&packet)) {
-                SPDLOG_DEBUG("Received packet from output stream: {}", outputStreamName);
-                try {
-                    OVMS_RETURN_ON_FAIL(
-                        onPacketReadySerializeImpl(
-                            request->id(),
-                            this->name,
-                            this->version,
-                            outputStreamName,
-                            this->outputTypes.at(outputStreamName),
-                            packet,
-                            *response));
-                } catch (...) {
-                    return Status(StatusCode::MEDIAPIPE_EXECUTION_ERROR, "Exception during packet serialization");
-                }
-                outputPollersWithReceivedPacket.insert(outputStreamName);
-                ++receivedOutputs;
-            }
-            SPDLOG_TRACE("Received all: {} packets for: {}", receivedOutputs, outputStreamName);
-        }
-        MP_RETURN_ON_FAIL(graph.WaitUntilDone(), "grap wait until done", StatusCode::MEDIAPIPE_EXECUTION_ERROR);
-        if (outputPollers.size() != outputPollersWithReceivedPacket.size()) {
-            SPDLOG_DEBUG("Mediapipe failed to execute. Failed to receive all output packets");
-            return Status(StatusCode::MEDIAPIPE_EXECUTION_ERROR, "Unknown error during mediapipe execution");
-        }
-        SPDLOG_DEBUG("Received all output stream packets for graph: {}", this->name);
-        return StatusCode::OK;
-    }
-
-    template <typename RequestType, typename ResponseType>
-    Status inferStream(const RequestType& req, ResponseType& res) {
-        SPDLOG_DEBUG("Start MediapipeGraphExecutor::inferEx mediapipe graph: {} execution", this->name);
-        std::mutex sendMutex;
-        try {
-            // Init
-            ::mediapipe::CalculatorGraph graph;
-            MP_RETURN_ON_FAIL(graph.Initialize(this->config), "graph initialization", StatusCode::MEDIAPIPE_GRAPH_INITIALIZATION_ERROR);
-
-            // Installing observers
-            for (const auto& outputName : this->outputNames) {
-                MP_RETURN_ON_FAIL(graph.ObserveOutputStream(outputName, [&res, &sendMutex, &outputName, this](const ::mediapipe::Packet& packet) -> absl::Status {
-                    try {
-                        std::lock_guard<std::mutex> lock(sendMutex);
-                        OVMS_RETURN_MP_ERROR_ON_FAIL(onPacketReadySerializeAndSendImpl(
-                                                         "" /*no ids for streaming*/,
-                                                         this->name,
-                                                         this->version,
-                                                         outputName,
-                                                         this->outputTypes.at(outputName),
-                                                         packet,
-                                                         res),
-                            "error in send packet routine");
-                        return absl::OkStatus();
-                    } catch (...) {
-                        return absl::Status(absl::StatusCode::kCancelled, "error in serialization");
-                    }
-                }),
-                    "output stream observer installation", StatusCode::INTERNAL_ERROR);  // Should never happen for validated graphs
-            }
-
-            std::map<std::string, mediapipe::Packet> inputSidePackets;
-            OVMS_RETURN_ON_FAIL(deserializeInputSidePacketsFromFirstRequestImpl(inputSidePackets, req));
-#if (PYTHON_DISABLE == 0)
-            inputSidePackets[PYTHON_SESSION_SIDE_PACKET_TAG] = mediapipe::MakePacket<PythonNodeResourcesMap>(this->pythonNodeResourcesMap)
-                                                                   .At(STARTING_TIMESTAMP);
-#endif
-            MP_RETURN_ON_FAIL(graph.StartRun(inputSidePackets), "graph start", StatusCode::MEDIAPIPE_GRAPH_START_ERROR);
-
-            // Deserialize first request
-            size_t numberOfPacketsCreated = 0;
-            OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(
-                createAndPushPacketsImpl(
-                    std::shared_ptr<const RequestType>(&req,
-                        // Custom deleter to avoid deallocation by custom holder
-                        // Conversion to shared_ptr is required for unified deserialization method
-                        // for first and subsequent requests
-                        [](const RequestType*) {}),
-                    this->inputTypes,
-                    this->pythonBackend,
-                    graph,
-                    this->currentStreamTimestamp,
-                    numberOfPacketsCreated),
-                "partial deserialization of first request");
-
-            // Read loop
-            // Here we create ModelInferRequest with shared ownership,
-            // and move it down to custom packet holder to ensure
-            // lifetime is extended to lifetime of deserialized Packets.
-            auto newReq = std::make_shared<RequestType>();
-            while (waitForNewRequest(res, *newReq)) {
-                auto pstatus = validateSubsequentRequestImpl(
-                    *newReq,
-                    this->name,
-                    this->version,
-                    this->inputTypes);
-                if (pstatus.ok()) {
-                    OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(
-                        createAndPushPacketsImpl(
-                            newReq,
-                            this->inputTypes,
-                            this->pythonBackend,
-                            graph,
-                            this->currentStreamTimestamp,
-                            numberOfPacketsCreated),
-                        "partial deserialization of subsequent requests");
-                } else {
-                    OVMS_WRITE_ERROR_ON_FAIL_AND_CONTINUE(pstatus, "validate subsequent requests");
-                }
-
-                if (graph.HasError()) {
-                    SPDLOG_DEBUG("Graph {}: encountered an error, stopping the execution", this->name);
-                    break;
-                }
-
-                newReq = std::make_shared<RequestType>();
-            }
-
-            SPDLOG_DEBUG("Graph {}: Closing packet sources...", this->name);
-            // Close input streams
-            MP_RETURN_ON_FAIL(graph.CloseAllPacketSources(), "closing all packet sources", StatusCode::MEDIAPIPE_GRAPH_CLOSE_INPUT_STREAM_ERROR);
-
-            SPDLOG_DEBUG("Graph {}: Closed all packet sources. Waiting untill done...", this->name);
-            MP_RETURN_ON_FAIL(graph.WaitUntilDone(), "waiting until done", StatusCode::MEDIAPIPE_EXECUTION_ERROR);
-            SPDLOG_DEBUG("Graph {}: Done execution", this->name);
-            return StatusCode::OK;
-        } catch (...) {
-            return Status(StatusCode::UNKNOWN_ERROR, "Exception while processing MediaPipe graph");  // To be displayed in method level above
-        }
-    }
+    Status inferStream(const ::inference::ModelInferRequest& firstRequest, ::grpc::ServerReaderWriterInterface<::inference::ModelStreamInferResponse, ::inference::ModelInferRequest>& stream);
 };
 }  // namespace ovms
diff --git a/src/test/ensemble_config_change_stress.cpp b/src/test/ensemble_config_change_stress.cpp
index 349b836b..6550ac67 100644
--- a/src/test/ensemble_config_change_stress.cpp
+++ b/src/test/ensemble_config_change_stress.cpp
@@ -23,9 +23,6 @@
 #include "../dags/pipeline_factory.hpp"
 #include "../dags/pipelinedefinition.hpp"
 #include "../get_model_metadata_impl.hpp"
-#if (MEDIAPIPE_DISABLE == 0)
-#include "../kfs_frontend/kfs_graph_executor_impl.hpp"
-#endif
 #include "../kfs_frontend/kfs_utils.hpp"
 #include "../localfilesystem.hpp"
 #include "../logging.hpp"
diff --git a/src/test/mediapipeflow_test.cpp b/src/test/mediapipeflow_test.cpp
index b8eb6609..7923c568 100644
--- a/src/test/mediapipeflow_test.cpp
+++ b/src/test/mediapipeflow_test.cpp
@@ -38,7 +38,6 @@
 #include "../dags/pipelinedefinition.hpp"
 #include "../grpcservermodule.hpp"
 #include "../http_rest_api_handler.hpp"
-#include "../kfs_frontend/kfs_graph_executor_impl.hpp"
 #include "../kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "../mediapipe_internal/mediapipe_utils.hpp"
 #include "../mediapipe_internal/mediapipefactory.hpp"
@@ -2395,6 +2394,10 @@ TEST_F(MediapipeConfigChanges, ConfigWithEmptyBasePath) {
 class MediapipeSerialization : public ::testing::Test {
     class MockedMediapipeGraphExecutor : public ovms::MediapipeGraphExecutor {
     public:
+        Status serializePacket(const std::string& name, ::inference::ModelInferResponse& response, const ::mediapipe::Packet& packet) const {
+            return ovms::MediapipeGraphExecutor::serializePacket(name, response, packet);
+        }
+
         MockedMediapipeGraphExecutor(const std::string& name, const std::string& version, const ::mediapipe::CalculatorGraphConfig& config,
             stream_types_mapping_t inputTypes,
             stream_types_mapping_t outputTypes,
@@ -2431,7 +2434,7 @@ TEST_F(MediapipeSerialization, KFSResponse) {
     std::vector<float> data = {1.0f};
     response.add_raw_output_contents()->assign(reinterpret_cast<char*>(data.data()), data.size() * sizeof(float));
     ::mediapipe::Packet packet = ::mediapipe::MakePacket<KFSResponse>(response);
-    ASSERT_EQ(onPacketReadySerializeImpl("1", "name", "1", "name", mediapipe_packet_type_enum::KFS_RESPONSE, packet, mp_response), StatusCode::OK);
+    ASSERT_EQ(executor->serializePacket("kfs_response", mp_response, packet), StatusCode::OK);
     ASSERT_EQ(mp_response.id(), "1");
     ASSERT_EQ(mp_response.outputs_size(), 1);
     auto mp_output = mp_response.outputs(0);
@@ -2447,7 +2450,7 @@ TEST_F(MediapipeSerialization, TFTensor) {
     tensorflow::Tensor response(TFSDataType::DT_FLOAT, {1});
     response.flat<float>()(0) = 1.0f;
     ::mediapipe::Packet packet = ::mediapipe::MakePacket<tensorflow::Tensor>(response);
-    ASSERT_EQ(onPacketReadySerializeImpl("1", "tf_response", "1", "tf_response", mediapipe_packet_type_enum::TFTENSOR, packet, mp_response), StatusCode::OK);
+    ASSERT_EQ(executor->serializePacket("tf_response", mp_response, packet), StatusCode::OK);
     ASSERT_EQ(mp_response.outputs(0).datatype(), "FP32");
     ASSERT_EQ(mp_response.outputs_size(), 1);
     auto mp_output = mp_response.outputs(0);
@@ -2463,7 +2466,7 @@ TEST_F(MediapipeSerialization, OVTensor) {
     ov::element::Type type(ov::element::Type_t::f32);
     ov::Tensor response(type, {1}, data.data());
     ::mediapipe::Packet packet = ::mediapipe::MakePacket<ov::Tensor>(response);
-    ASSERT_EQ(onPacketReadySerializeImpl("1", "ov_response", "1", "ov_response", mediapipe_packet_type_enum::OVTENSOR, packet, mp_response), StatusCode::OK);
+    ASSERT_EQ(executor->serializePacket("ov_response", mp_response, packet), StatusCode::OK);
     ASSERT_EQ(mp_response.outputs(0).datatype(), "FP32");
     ASSERT_EQ(mp_response.outputs_size(), 1);
     auto mp_output = mp_response.outputs(0);
@@ -2478,7 +2481,7 @@ TEST_F(MediapipeSerialization, MPTensor) {
     mediapipe::Tensor response(mediapipe::Tensor::ElementType::kFloat32, {1});
     response.GetCpuWriteView().buffer<float>()[0] = 1.0f;
     ::mediapipe::Packet packet = ::mediapipe::MakePacket<mediapipe::Tensor>(std::move(response));
-    ASSERT_EQ(onPacketReadySerializeImpl("1", "mp_response", "1", "mp_response", mediapipe_packet_type_enum::MPTENSOR, packet, mp_response), StatusCode::OK);
+    ASSERT_EQ(executor->serializePacket("mp_response", mp_response, packet), StatusCode::OK);
     ASSERT_EQ(mp_response.outputs(0).datatype(), "FP32");
     ASSERT_EQ(mp_response.outputs_size(), 1);
     auto mp_output = mp_response.outputs(0);
@@ -2495,7 +2498,7 @@ TEST_F(MediapipeSerialization, MPImageTensor) {
     response.MutablePixelData()[1] = (char)1;
     response.MutablePixelData()[2] = (char)1;
     ::mediapipe::Packet packet = ::mediapipe::MakePacket<mediapipe::ImageFrame>(std::move(response));
-    ASSERT_EQ(onPacketReadySerializeImpl("1", "mp_img_response", "1", "mp_img_response", mediapipe_packet_type_enum::MEDIAPIPE_IMAGE, packet, mp_response), StatusCode::OK);
+    ASSERT_EQ(executor->serializePacket("mp_img_response", mp_response, packet), StatusCode::OK);
     ASSERT_EQ(mp_response.outputs(0).datatype(), "UINT8");
     ASSERT_EQ(mp_response.outputs_size(), 1);
     auto mp_output = mp_response.outputs(0);
diff --git a/src/test/pythonnode_test.cpp b/src/test/pythonnode_test.cpp
index 5a71728b..4f3fd686 100644
--- a/src/test/pythonnode_test.cpp
+++ b/src/test/pythonnode_test.cpp
@@ -29,7 +29,6 @@
 #include "../dags/pipelinedefinition.hpp"
 #include "../grpcservermodule.hpp"
 #include "../http_rest_api_handler.hpp"
-#include "../kfs_frontend/kfs_graph_executor_impl.hpp"
 #include "../kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "../mediapipe_internal/mediapipefactory.hpp"
 #include "../mediapipe_internal/mediapipegraphdefinition.hpp"
@@ -952,6 +951,10 @@ public:
 
 class MockedMediapipeGraphExecutorPy : public ovms::MediapipeGraphExecutor {
 public:
+    Status serializePacket(const std::string& name, ::inference::ModelInferResponse& response, const ::mediapipe::Packet& packet) const {
+        return ovms::MediapipeGraphExecutor::serializePacket(name, response, packet);
+    }
+
     MockedMediapipeGraphExecutorPy(const std::string& name, const std::string& version, const ::mediapipe::CalculatorGraphConfig& config,
         stream_types_mapping_t inputTypes,
         stream_types_mapping_t outputTypes,
@@ -979,7 +982,7 @@ TEST_F(PythonFlowTest, SerializePyObjectWrapperToKServeResponse) {
     ::inference::ModelInferResponse response;
 
     ::mediapipe::Packet packet = ::mediapipe::Adopt<PyObjectWrapper<py::object>>(tensor.pyTensor.release());
-    ASSERT_EQ(onPacketReadySerializeImpl("id", name, "1", name, mediapipe_packet_type_enum::OVMS_PY_TENSOR, packet, response), StatusCode::OK);
+    ASSERT_EQ(executor.serializePacket(name, response, packet), StatusCode::OK);
     ASSERT_EQ(response.outputs_size(), 1);
     auto output = response.outputs(0);
     ASSERT_EQ(output.datatype(), "FP32");
diff --git a/src/test/streaming_test.cpp b/src/test/streaming_test.cpp
index 4989ecc7..0311163b 100644
--- a/src/test/streaming_test.cpp
+++ b/src/test/streaming_test.cpp
@@ -19,7 +19,6 @@
 #include <gmock/gmock.h>
 #include <gtest/gtest.h>
 
-#include "../kfs_frontend/kfs_graph_executor_impl.hpp"
 #include "../kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "../mediapipe_internal/mediapipegraphdefinition.hpp"
 #include "../mediapipe_internal/mediapipegraphexecutor.hpp"
@@ -85,20 +84,18 @@ public:
 };
 #endif
 
-static const std::string TIMESTAMP_PARAMETER_NAME{"OVMS_MP_TIMESTAMP"};
-
 static void setRequestTimestamp(KFSRequest& request, const std::string& value) {
     request.clear_parameters();
     auto intOpt = ovms::stoi64(value);
     if (intOpt.has_value()) {
-        request.mutable_parameters()->operator[](TIMESTAMP_PARAMETER_NAME).set_int64_param(intOpt.value());
+        request.mutable_parameters()->operator[](MediapipeGraphExecutor::TIMESTAMP_PARAMETER_NAME).set_int64_param(intOpt.value());
     } else {
-        request.mutable_parameters()->operator[](TIMESTAMP_PARAMETER_NAME).set_string_param(value);
+        request.mutable_parameters()->operator[](MediapipeGraphExecutor::TIMESTAMP_PARAMETER_NAME).set_string_param(value);
     }
 }
 // TODO what to do if several inputs have different timestamp
 static int64_t getResponseTimestamp(const KFSResponse& response) {
-    return response.parameters().at(TIMESTAMP_PARAMETER_NAME).int64_param();
+    return response.parameters().at(MediapipeGraphExecutor::TIMESTAMP_PARAMETER_NAME).int64_param();
 }
 
 static void prepareRequest(::inference::ModelInferRequest& request, const std::vector<std::tuple<std::string, float>>& content, std::optional<int64_t> timestamp = std::nullopt, const std::string& servableName = "", const std::string& servableVersion = "") {
